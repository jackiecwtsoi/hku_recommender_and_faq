{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Kackie\\anaconda3\\envs\\DataScience\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForQuestionAnswering, AutoTokenizer, pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "roberta = 'deepset/roberta-base-squad2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model and tokenizer\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(roberta)\n",
    "tokenizer = AutoTokenizer.from_pretrained(roberta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get predictions\n",
    "nlp = pipeline('question-answering', model=model, tokenizer=tokenizer)\n",
    "\n",
    "qa_input = {\n",
    "    'question': 'How many pretrained models are available in Transformers?',\n",
    "    'context': r\"\"\"Transformers (formerly known as pytorch-transformers and pytorch-pretrained-bert) provides general-purpose\n",
    "architectures (BERT, GPT-2, RoBERTa, XLM, DistilBert, XLNet…) for Natural Language Understanding (NLU) and Natural\n",
    "Language Generation (NLG) with over 32+ pretrained models in 100+ languages and deep interoperability between\n",
    "TensorFlow 2.0 and PyTorch.\"\"\"\n",
    "}\n",
    "\n",
    "res = nlp(qa_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'score': 0.4159207046031952, 'start': 253, 'end': 261, 'answer': 'over 32+'}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_faq = pd.read_csv('faq-data/df_faq.csv', index_col=0)\n",
    "df_faq['Question'] = '(' + df_faq['Type'] + ') ' + df_faq['Question']\n",
    "df_faq['Answer'] = '(' + df_faq['Type'] + ') ' + df_faq['Answer']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentence Transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Kackie\\anaconda3\\envs\\DataScience\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'df_faq' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_21580\\784003115.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msentence_transformers\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mSentenceTransformer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mquestions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf_faq\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Question'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0manswers\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf_faq\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Answer'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'df_faq' is not defined"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "questions = df_faq['Question']\n",
    "answers = df_faq['Answer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(175, 768)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_transformer = SentenceTransformer('bert-base-nli-mean-tokens')\n",
    "question_embeddings = sentence_transformer.encode(questions)\n",
    "question_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def get_qQ_similarities(query):\n",
    "    query_embedding = sentence_transformer.encode([query])\n",
    "\n",
    "    similarities = cosine_similarity(\n",
    "        question_embeddings, query_embedding\n",
    "    )\n",
    "\n",
    "    df = pd.DataFrame(similarities, columns = ['Similarity'])\n",
    "    df['Question'] = questions\n",
    "    df = df.sort_values(by='Similarity', ascending=False)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Similarity</th>\n",
       "      <th>Question</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.774812</td>\n",
       "      <td>(International) How can I apply for a scholars...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.771021</td>\n",
       "      <td>(International) What is the University’s appli...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>0.757958</td>\n",
       "      <td>(AAO) Who will consider my course enrolment an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168</th>\n",
       "      <td>0.748967</td>\n",
       "      <td>(AAO) Are there any bursaries or scholarships ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>0.718374</td>\n",
       "      <td>(AAO) When the course selection status reads “...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Similarity                                           Question\n",
       "22     0.774812  (International) How can I apply for a scholars...\n",
       "18     0.771021  (International) What is the University’s appli...\n",
       "124    0.757958  (AAO) Who will consider my course enrolment an...\n",
       "168    0.748967  (AAO) Are there any bursaries or scholarships ...\n",
       "125    0.718374  (AAO) When the course selection status reads “..."
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_qQ_similarities('When is the application deadline for scholarships?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(175, 768)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer_embeddings = sentence_transformer.encode(answers)\n",
    "answer_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_qA_similarities(query):\n",
    "    query_embedding = sentence_transformer.encode([query])\n",
    "\n",
    "    similarities = cosine_similarity(\n",
    "        answer_embeddings, query_embedding\n",
    "    )\n",
    "\n",
    "    df = pd.DataFrame(similarities, columns = ['Similarity'])\n",
    "    df['Answer'] = answers\n",
    "    df = df.sort_values(by='Similarity', ascending=False)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Similarity</th>\n",
       "      <th>Answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.484508</td>\n",
       "      <td>(HKDSE (Non-local)) You may refer to our Fees ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>0.483337</td>\n",
       "      <td>(AAO) Under their SIS menu -&gt; Enrollment -&gt; En...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150</th>\n",
       "      <td>0.482735</td>\n",
       "      <td>(AAO) GPA is the abbreviation of Grade Point A...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>0.479445</td>\n",
       "      <td>(AAO) It means it is still pending for approva...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.475474</td>\n",
       "      <td>(International) You may refer to our Fees and ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>0.141538</td>\n",
       "      <td>(BASc) There is no particular subject requirem...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>0.134559</td>\n",
       "      <td>(BASc) At HKU, applicants will be competing wi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>0.116894</td>\n",
       "      <td>(BSc 6901) No.  Students are free to choose an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>0.048570</td>\n",
       "      <td>(BSc 6901) one programme code with a choice of...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>0.018936</td>\n",
       "      <td>(BSc 6901) HKU Science is the first university...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>175 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Similarity                                             Answer\n",
       "17     0.484508  (HKDSE (Non-local)) You may refer to our Fees ...\n",
       "126    0.483337  (AAO) Under their SIS menu -> Enrollment -> En...\n",
       "150    0.482735  (AAO) GPA is the abbreviation of Grade Point A...\n",
       "125    0.479445  (AAO) It means it is still pending for approva...\n",
       "24     0.475474  (International) You may refer to our Fees and ...\n",
       "..          ...                                                ...\n",
       "74     0.141538  (BASc) There is no particular subject requirem...\n",
       "78     0.134559  (BASc) At HKU, applicants will be competing wi...\n",
       "36     0.116894  (BSc 6901) No.  Students are free to choose an...\n",
       "31     0.048570  (BSc 6901) one programme code with a choice of...\n",
       "60     0.018936  (BSc 6901) HKU Science is the first university...\n",
       "\n",
       "[175 rows x 2 columns]"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_qA_similarities('How much does the programme cost?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create new column representing relevance between each QA pair\n",
    "df_faq['QA Relevance'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate 24 negative samples for each QA pair\n",
    "import random\n",
    "\n",
    "def generate_negative_samples(df):\n",
    "    questions = df['Question']\n",
    "    answers = df['Answer']\n",
    "\n",
    "    df_negative = pd.DataFrame()\n",
    "    for i, q in enumerate(questions):\n",
    "        remaining_answers = answers.drop(index=i)\n",
    "        df_negative_samples = pd.DataFrame(random.sample(list(remaining_answers), 2), columns=['Answer'])\n",
    "        df_negative_samples['Question'] = q\n",
    "        df_negative_samples['QA Relevance'] = 0\n",
    "        df_negative = df_negative.append(df_negative_samples)\n",
    "    \n",
    "    return df_negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate the final dataset with negative samples included\n",
    "\n",
    "df_negative = generate_negative_samples(df_faq)\n",
    "\n",
    "# df_faq = df_faq.drop(columns=['Type'])\n",
    "df_final = df_faq.append(df_negative).reset_index(drop = True)\n",
    "\n",
    "# rename the columns to the required 'text_a', 'text_b', 'labels'\n",
    "df_final = df_final.rename(columns={'Question': 'text_a', 'Answer': 'text_b', 'QA Relevance': 'labels'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final.to_csv('faq-data/df_final.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final = pd.read_csv('data/df_final.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "df_train, df_test = train_test_split(df_final, test_size=0.2, random_state=7600, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Kackie\\anaconda3\\envs\\DataScience\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from simpletransformers.classification import ClassificationModel, ClassificationArgs\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(level=logging.INFO)\n",
    "transformers_logger = logging.getLogger('transformers')\n",
    "transformers_logger.setLevel(logging.WARNING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "cuda_available = torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Trial 1: ROBERTA using 24 negative samples per positive sample (same as in the paper)**\n",
    "- N_EPOCHS = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define model configuration parameters\n",
    "N_EPOCHS = 5\n",
    "\n",
    "# configure the classification model\n",
    "bert_qA_args = ClassificationArgs(num_train_epochs=N_EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'roberta.pooler.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'roberta.pooler.dense.bias', 'lm_head.dense.bias', 'lm_head.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# define the model\n",
    "bert_qA = ClassificationModel('roberta', 'roberta-base', use_cuda=cuda_available, args=bert_qA_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:simpletransformers.classification.classification_utils: Converting to features started. Cache is not used.\n",
      "  0%|          | 7/3500 [00:06<50:31,  1.15it/s]  \n",
      "INFO:simpletransformers.classification.classification_utils: Saving features into cached file cache_dir/cached_train_roberta_128_2_3\n",
      "Epochs 0/5. Running Loss:    0.0232: 100%|██████████| 438/438 [00:26<00:00, 16.49it/s]\n",
      "Epochs 1/5. Running Loss:    0.0157: 100%|██████████| 438/438 [00:25<00:00, 17.41it/s]\n",
      "Epochs 2/5. Running Loss:    2.2101: 100%|██████████| 438/438 [00:24<00:00, 17.61it/s]\n",
      "Epochs 3/5. Running Loss:    0.0162: 100%|██████████| 438/438 [00:25<00:00, 17.19it/s]\n",
      "Epochs 4/5. Running Loss:    0.0152: 100%|██████████| 438/438 [00:27<00:00, 15.89it/s]\n",
      "Epoch 5 of 5: 100%|██████████| 5/5 [02:18<00:00, 27.75s/it]\n",
      "INFO:simpletransformers.classification.classification_model: Training of roberta model complete. Saved to outputs/.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(2190, 0.1949672647807152)"
      ]
     },
     "execution_count": 243,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_qA.train_model(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:simpletransformers.classification.classification_utils: Converting to features started. Cache is not used.\n",
      "  0%|          | 2/875 [00:04<29:06,  2.00s/it]\n",
      "INFO:simpletransformers.classification.classification_utils: Saving features into cached file cache_dir/cached_dev_roberta_128_2_3\n",
      "Running Evaluation: 100%|██████████| 110/110 [00:01<00:00, 83.64it/s]\n",
      "INFO:simpletransformers.classification.classification_model:{'mcc': 0.0, 'tp': 0, 'tn': 841, 'fp': 0, 'fn': 34, 'auroc': 0.5, 'auprc': 0.038857142857142854, 'eval_loss': 0.18288354006680577}\n"
     ]
    }
   ],
   "source": [
    "t1_result, t1_model_outputs, t1_wrong_predictions = bert_qA.eval_model(\n",
    "    df_test\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "qA_test_samples = [\n",
    "    [\n",
    "        'What is my application deadline?', \n",
    "        'You may wish to go through the Important Dates when applying to the University. Applications submitted after the first round application deadline will be considered on a rolling basis subject to programme availability.'\n",
    "    ],\n",
    "\n",
    "    [\n",
    "        'When will I know the result of my application?',\n",
    "        'You may wish to go through the Important Dates when applying to the University. Applications submitted after the first round application deadline will be considered on a rolling basis subject to programme availability.'\n",
    "    ],\n",
    "\n",
    "    [\n",
    "        'Hi',\n",
    "        'You may wish to go through the Important Dates when applying to the University. Applications submitted after the first round application deadline will be considered on a rolling basis subject to programme availability.'\n",
    "    ]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:simpletransformers.classification.classification_utils: Converting to features started. Cache is not used.\n",
      " 33%|███▎      | 1/3 [00:03<00:07,  3.62s/it]\n",
      "100%|██████████| 1/1 [00:00<00:00,  3.43it/s]\n"
     ]
    }
   ],
   "source": [
    "t1_predictions, t1_raw_outputs = bert_qA.predict(qA_test_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0], dtype=int64)"
      ]
     },
     "execution_count": 310,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t1_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Trial 2: RoBERTA with 2 negative samples per positive sample**\n",
    "- N_EPOCHS = 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [],
   "source": [
    "# configure the classification model\n",
    "N_EPOCHS = 7\n",
    "roberta_qA_args = ClassificationArgs(num_train_epochs=N_EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'roberta.pooler.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'roberta.pooler.dense.bias', 'lm_head.dense.bias', 'lm_head.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "roberta_qA = ClassificationModel('roberta', 'roberta-base', use_cuda=cuda_available, args=roberta_qA_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:simpletransformers.classification.classification_utils: Converting to features started. Cache is not used.\n",
      "  0%|          | 1/420 [00:03<25:34,  3.66s/it]\n",
      "INFO:simpletransformers.classification.classification_utils: Saving features into cached file cache_dir/cached_train_roberta_128_2_3\n",
      "Epochs 0/7. Running Loss:    1.0736: 100%|██████████| 53/53 [00:03<00:00, 16.00it/s]\n",
      "Epochs 1/7. Running Loss:    0.7789: 100%|██████████| 53/53 [00:03<00:00, 16.48it/s]\n",
      "Epochs 2/7. Running Loss:    1.2007: 100%|██████████| 53/53 [00:03<00:00, 16.74it/s]\n",
      "Epochs 3/7. Running Loss:    0.0070: 100%|██████████| 53/53 [00:03<00:00, 16.12it/s]\n",
      "Epochs 4/7. Running Loss:    0.0018: 100%|██████████| 53/53 [00:03<00:00, 16.16it/s]\n",
      "Epochs 5/7. Running Loss:    0.0013: 100%|██████████| 53/53 [00:03<00:00, 16.29it/s]\n",
      "Epochs 6/7. Running Loss:    0.0012: 100%|██████████| 53/53 [00:03<00:00, 16.06it/s]\n",
      "Epoch 7 of 7: 100%|██████████| 7/7 [00:35<00:00,  5.10s/it]\n",
      "INFO:simpletransformers.classification.classification_model: Training of roberta model complete. Saved to outputs/roberta-t2.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(371, 0.27318945784131793)"
      ]
     },
     "execution_count": 294,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roberta_qA.train_model(df_train, output_dir='outputs/roberta-t2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:simpletransformers.classification.classification_utils: Converting to features started. Cache is not used.\n",
      "  1%|          | 1/105 [00:03<06:25,  3.71s/it]\n",
      "INFO:simpletransformers.classification.classification_utils: Saving features into cached file cache_dir/cached_dev_roberta_128_2_3\n",
      "Running Evaluation: 100%|██████████| 14/14 [00:00<00:00, 61.35it/s]\n",
      "INFO:simpletransformers.classification.classification_model:{'mcc': 0.7880977119810487, 'tp': 25, 'tn': 71, 'fp': 5, 'fn': 4, 'auroc': 0.9686932849364791, 'auprc': 0.9395412408405989, 'eval_loss': 0.42512241857392447}\n"
     ]
    }
   ],
   "source": [
    "t2_result, t2_model_outputs, t2_wrong_predictions = roberta_qA.eval_model(\n",
    "    df_test\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:simpletransformers.classification.classification_utils: Converting to features started. Cache is not used.\n",
      " 33%|███▎      | 1/3 [00:03<00:07,  3.61s/it]\n",
      "100%|██████████| 1/1 [00:00<00:00,  3.12it/s]\n"
     ]
    }
   ],
   "source": [
    "t2_predictions, t2_raw_outputs = roberta_qA.predict(qA_test_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:simpletransformers.classification.classification_utils: Converting to features started. Cache is not used.\n",
      "  1%|          | 1/175 [00:03<10:33,  3.64s/it]\n",
      "100%|██████████| 22/22 [00:00<00:00, 35.57it/s]\n"
     ]
    }
   ],
   "source": [
    "# test some tricky sentences\n",
    "query = 'I wanna know what the application deadline is.'\n",
    "available_answers = df_faq['Answer']\n",
    "\n",
    "qa_test_samples_tricky = []\n",
    "for a in available_answers:\n",
    "    qa_test_samples_tricky.append([query, a])\n",
    "\n",
    "# get the predictions\n",
    "t2_predictions_tricky, t2_raw_outputs_tricky = roberta_qA.predict(qa_test_samples_tricky)\n",
    "\n",
    "# format the predictions into an easy-to-read dataframe\n",
    "df_t2_predictions_tricky = pd.DataFrame(qa_test_samples_tricky, columns=['User Query', 'Answer in Database'])\n",
    "df_t2_predictions_tricky['Question in Database'] = df_faq['Question']\n",
    "df_t2_predictions_tricky['Predicted QA Relevance'] = t2_predictions_tricky"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_t2_predictions_tricky' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_21580\\2673849913.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdf_t2_predictions_tricky\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'outputs/roberta-t2/df_t2_predictions_tricky.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'df_t2_predictions_tricky' is not defined"
     ]
    }
   ],
   "source": [
    "df_t2_predictions_tricky.to_csv('outputs/roberta-t2/df_t2_predictions_tricky.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Trial 3: ROBERTA initially trained on SQuAD (but not for classification purposes)**\n",
    "- n_epochs = 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# configure the classification model\n",
    "N_EPOCHS = 12\n",
    "roberta_qA_squad2_args = ClassificationArgs(num_train_epochs=N_EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/roberta-base-squad2 were not used when initializing RobertaForSequenceClassification: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at deepset/roberta-base-squad2 and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "roberta_qA_squad2 = ClassificationModel('roberta', 'deepset/roberta-base-squad2', use_cuda=cuda_available, args=roberta_qA_squad2_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:simpletransformers.classification.classification_utils: Converting to features started. Cache is not used.\n",
      "  0%|          | 1/420 [00:02<17:44,  2.54s/it]\n",
      "INFO:simpletransformers.classification.classification_utils: Saving features into cached file cache_dir/cached_train_roberta_128_2_3\n",
      "c:\\Users\\Kackie\\anaconda3\\envs\\DataScience\\lib\\site-packages\\transformers\\optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  FutureWarning,\n",
      "Epochs 0/12. Running Loss:    1.0514: 100%|██████████| 53/53 [00:03<00:00, 14.58it/s]\n",
      "Epochs 1/12. Running Loss:    0.0050: 100%|██████████| 53/53 [00:03<00:00, 16.10it/s]\n",
      "Epochs 2/12. Running Loss:    0.0099: 100%|██████████| 53/53 [00:03<00:00, 16.11it/s]\n",
      "Epochs 3/12. Running Loss:    0.6107: 100%|██████████| 53/53 [00:03<00:00, 16.67it/s]\n",
      "Epochs 4/12. Running Loss:    0.0019: 100%|██████████| 53/53 [00:03<00:00, 16.34it/s]\n",
      "Epochs 5/12. Running Loss:    0.0063: 100%|██████████| 53/53 [00:03<00:00, 16.25it/s]\n",
      "Epochs 6/12. Running Loss:    0.0002: 100%|██████████| 53/53 [00:03<00:00, 16.37it/s]\n",
      "Epochs 7/12. Running Loss:    0.0002: 100%|██████████| 53/53 [00:03<00:00, 16.24it/s]\n",
      "Epochs 8/12. Running Loss:    0.0002: 100%|██████████| 53/53 [00:03<00:00, 16.60it/s]\n",
      "Epochs 9/12. Running Loss:    0.0001: 100%|██████████| 53/53 [00:03<00:00, 16.01it/s]\n",
      "Epochs 10/12. Running Loss:    0.0002: 100%|██████████| 53/53 [00:03<00:00, 16.17it/s]\n",
      "Epochs 11/12. Running Loss:    0.0001: 100%|██████████| 53/53 [00:03<00:00, 16.12it/s]\n",
      "Epoch 12 of 12: 100%|██████████| 12/12 [01:01<00:00,  5.13s/it]\n",
      "INFO:simpletransformers.classification.classification_model: Training of roberta model complete. Saved to outputs/roberta-t3-squad2.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(636, 0.09954484901244535)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roberta_qA_squad2.train_model(df_train, output_dir='outputs/roberta-t3-squad2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:simpletransformers.classification.classification_utils: Converting to features started. Cache is not used.\n",
      "  1%|          | 1/105 [00:03<05:39,  3.26s/it]\n",
      "INFO:simpletransformers.classification.classification_utils: Saving features into cached file cache_dir/cached_dev_roberta_128_2_3\n",
      "Running Evaluation: 100%|██████████| 14/14 [00:00<00:00, 34.54it/s]\n",
      "INFO:simpletransformers.classification.classification_model:{'mcc': 0.78353372286637, 'tp': 24, 'tn': 72, 'fp': 4, 'fn': 5, 'auroc': 0.9516787658802179, 'auprc': 0.9372552124290898, 'eval_loss': 0.6542652781520572}\n"
     ]
    }
   ],
   "source": [
    "t3_result, t3_model_outputs, t3_wrong_predictions = roberta_qA_squad2.eval_model(\n",
    "    df_test\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:simpletransformers.classification.classification_utils: Converting to features started. Cache is not used.\n",
      " 33%|███▎      | 1/3 [00:02<00:04,  2.49s/it]\n",
      "100%|██████████| 1/1 [00:00<00:00, 10.52it/s]\n"
     ]
    }
   ],
   "source": [
    "t3_predictions, t3_raw_outputs = roberta_qA_squad2.predict(qA_test_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 0], dtype=int64)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t3_predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Trial 4**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# configure the classification model\n",
    "N_EPOCHS = 20\n",
    "mpnet_qA_args = ClassificationArgs(num_train_epochs=N_EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at sentence-transformers/all-mpnet-base-v2 were not used when initializing MPNetForSequenceClassification: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "- This IS expected if you are initializing MPNetForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing MPNetForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of MPNetForSequenceClassification were not initialized from the model checkpoint at sentence-transformers/all-mpnet-base-v2 and are newly initialized: ['classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.dense.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "mpnet_qA = ClassificationModel('mpnet', 'sentence-transformers/all-mpnet-base-v2', use_cuda=cuda_available, args=mpnet_qA_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:simpletransformers.classification.classification_utils: Converting to features started. Cache is not used.\n",
      "  0%|          | 1/420 [00:02<17:57,  2.57s/it]\n",
      "INFO:simpletransformers.classification.classification_utils: Saving features into cached file cache_dir/cached_train_mpnet_128_2_3\n",
      "c:\\Users\\Kackie\\anaconda3\\envs\\DataScience\\lib\\site-packages\\transformers\\optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  FutureWarning,\n",
      "Epochs 0/20. Running Loss:    0.9338: 100%|██████████| 53/53 [00:03<00:00, 15.32it/s]\n",
      "Epochs 1/20. Running Loss:    0.3824: 100%|██████████| 53/53 [00:03<00:00, 16.71it/s]\n",
      "Epochs 2/20. Running Loss:    0.0786: 100%|██████████| 53/53 [00:03<00:00, 16.37it/s]\n",
      "Epochs 3/20. Running Loss:    0.0244: 100%|██████████| 53/53 [00:03<00:00, 16.52it/s]\n",
      "Epochs 4/20. Running Loss:    1.1250: 100%|██████████| 53/53 [00:03<00:00, 16.93it/s]\n",
      "Epochs 5/20. Running Loss:    0.0063: 100%|██████████| 53/53 [00:03<00:00, 16.76it/s]\n",
      "Epochs 6/20. Running Loss:    0.0050: 100%|██████████| 53/53 [00:03<00:00, 16.63it/s]\n",
      "Epochs 7/20. Running Loss:    0.0035: 100%|██████████| 53/53 [00:03<00:00, 16.72it/s]\n",
      "Epochs 8/20. Running Loss:    0.0026: 100%|██████████| 53/53 [00:03<00:00, 16.71it/s]\n",
      "Epochs 9/20. Running Loss:    0.0020: 100%|██████████| 53/53 [00:03<00:00, 16.69it/s]\n",
      "Epochs 10/20. Running Loss:    0.0018: 100%|██████████| 53/53 [00:03<00:00, 16.64it/s]\n",
      "Epochs 11/20. Running Loss:    0.0017: 100%|██████████| 53/53 [00:03<00:00, 16.58it/s]\n",
      "Epochs 12/20. Running Loss:    0.0012: 100%|██████████| 53/53 [00:03<00:00, 16.62it/s]\n",
      "Epochs 13/20. Running Loss:    0.0013: 100%|██████████| 53/53 [00:03<00:00, 16.55it/s]\n",
      "Epochs 14/20. Running Loss:    0.0011: 100%|██████████| 53/53 [00:03<00:00, 16.27it/s]\n",
      "Epochs 15/20. Running Loss:    0.0010: 100%|██████████| 53/53 [00:03<00:00, 16.77it/s]\n",
      "Epochs 16/20. Running Loss:    0.0009: 100%|██████████| 53/53 [00:03<00:00, 16.79it/s]\n",
      "Epochs 17/20. Running Loss:    0.0010: 100%|██████████| 53/53 [00:03<00:00, 16.57it/s]\n",
      "Epochs 18/20. Running Loss:    0.0009: 100%|██████████| 53/53 [00:03<00:00, 16.71it/s]\n",
      "Epochs 19/20. Running Loss:    0.0011: 100%|██████████| 53/53 [00:03<00:00, 16.75it/s]\n",
      "Epoch 20 of 20: 100%|██████████| 20/20 [01:35<00:00,  4.76s/it]\n",
      "INFO:simpletransformers.classification.classification_model: Training of mpnet model complete. Saved to outputs/mpnet-t4.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1060, 0.10138939568456613)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mpnet_qA.train_model(df_train, output_dir='outputs/mpnet-t4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:simpletransformers.classification.classification_utils: Converting to features started. Cache is not used.\n",
      "  1%|          | 1/105 [00:03<05:47,  3.34s/it]\n",
      "INFO:simpletransformers.classification.classification_utils: Saving features into cached file cache_dir/cached_dev_mpnet_128_2_3\n",
      "Running Evaluation: 100%|██████████| 14/14 [00:00<00:00, 42.07it/s]\n",
      "INFO:simpletransformers.classification.classification_model:{'mcc': 0.5994931313360115, 'tp': 21, 'tn': 67, 'fp': 9, 'fn': 8, 'auroc': 0.8888384754990926, 'auprc': 0.8193617097508505, 'eval_loss': 0.9931466409138271}\n"
     ]
    }
   ],
   "source": [
    "t4_result, t4_model_outputs, t4_wrong_predictions = mpnet_qA.eval_model(\n",
    "    df_test\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:simpletransformers.classification.classification_utils: Converting to features started. Cache is not used.\n",
      " 33%|███▎      | 1/3 [00:02<00:05,  2.57s/it]\n",
      "100%|██████████| 1/1 [00:00<00:00,  3.35it/s]\n"
     ]
    }
   ],
   "source": [
    "t4_predictions, t4_raw_outputs = mpnet_qA.predict(qA_test_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1], dtype=int64)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t4_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>User Query</th>\n",
       "      <th>Answer in Database</th>\n",
       "      <th>Question in Database</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I wanna know what the application deadline is.</td>\n",
       "      <td>We welcome your application to HKU through the...</td>\n",
       "      <td>How do I apply to HKU through JUPAS scheme?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I wanna know what the application deadline is.</td>\n",
       "      <td>Students should not forget that in addition to...</td>\n",
       "      <td>What are the common mistakes as a JUPAS applic...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I wanna know what the application deadline is.</td>\n",
       "      <td>All students who apply to HKU on the basis of ...</td>\n",
       "      <td>How can I apply to HKU as a HKDSE repeater?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I wanna know what the application deadline is.</td>\n",
       "      <td>To have your application considered for admiss...</td>\n",
       "      <td>What are the minimum university entrance requi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I wanna know what the application deadline is.</td>\n",
       "      <td>Starting from the academic year 2020/2021, HKU...</td>\n",
       "      <td>How is the admission score calculated?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170</th>\n",
       "      <td>I wanna know what the application deadline is.</td>\n",
       "      <td>You might try to look at the FAQ compiled by t...</td>\n",
       "      <td>I still have other questions regarding the set...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>171</th>\n",
       "      <td>I wanna know what the application deadline is.</td>\n",
       "      <td>Please visit the website of the Scholarships O...</td>\n",
       "      <td>Are there scholarships that accept application...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>172</th>\n",
       "      <td>I wanna know what the application deadline is.</td>\n",
       "      <td>You need to apply for leave of absence if you ...</td>\n",
       "      <td>When and how do I apply for leave of absence?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173</th>\n",
       "      <td>I wanna know what the application deadline is.</td>\n",
       "      <td>To put it simply, plagiarism is defined as the...</td>\n",
       "      <td>What is plagiarism and what happens if I am fo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174</th>\n",
       "      <td>I wanna know what the application deadline is.</td>\n",
       "      <td>If you need further advice on some other study...</td>\n",
       "      <td>I have a question that isn’t answered here! Wh...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>175 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         User Query  \\\n",
       "0    I wanna know what the application deadline is.   \n",
       "1    I wanna know what the application deadline is.   \n",
       "2    I wanna know what the application deadline is.   \n",
       "3    I wanna know what the application deadline is.   \n",
       "4    I wanna know what the application deadline is.   \n",
       "..                                              ...   \n",
       "170  I wanna know what the application deadline is.   \n",
       "171  I wanna know what the application deadline is.   \n",
       "172  I wanna know what the application deadline is.   \n",
       "173  I wanna know what the application deadline is.   \n",
       "174  I wanna know what the application deadline is.   \n",
       "\n",
       "                                    Answer in Database  \\\n",
       "0    We welcome your application to HKU through the...   \n",
       "1    Students should not forget that in addition to...   \n",
       "2    All students who apply to HKU on the basis of ...   \n",
       "3    To have your application considered for admiss...   \n",
       "4    Starting from the academic year 2020/2021, HKU...   \n",
       "..                                                 ...   \n",
       "170  You might try to look at the FAQ compiled by t...   \n",
       "171  Please visit the website of the Scholarships O...   \n",
       "172  You need to apply for leave of absence if you ...   \n",
       "173  To put it simply, plagiarism is defined as the...   \n",
       "174  If you need further advice on some other study...   \n",
       "\n",
       "                                  Question in Database  \n",
       "0          How do I apply to HKU through JUPAS scheme?  \n",
       "1    What are the common mistakes as a JUPAS applic...  \n",
       "2          How can I apply to HKU as a HKDSE repeater?  \n",
       "3    What are the minimum university entrance requi...  \n",
       "4               How is the admission score calculated?  \n",
       "..                                                 ...  \n",
       "170  I still have other questions regarding the set...  \n",
       "171  Are there scholarships that accept application...  \n",
       "172      When and how do I apply for leave of absence?  \n",
       "173  What is plagiarism and what happens if I am fo...  \n",
       "174  I have a question that isn’t answered here! Wh...  \n",
       "\n",
       "[175 rows x 3 columns]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions_tricky"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:simpletransformers.classification.classification_utils: Converting to features started. Cache is not used.\n",
      "  1%|          | 1/175 [00:02<07:24,  2.56s/it]\n",
      "100%|██████████| 22/22 [00:00<00:00, 34.61it/s]\n"
     ]
    }
   ],
   "source": [
    "# test some tricky sentences\n",
    "query = 'I wanna know what the application deadline is.'\n",
    "available_answers = df_faq['Answer']\n",
    "\n",
    "qa_test_samples_tricky = []\n",
    "for a in available_answers:\n",
    "    qa_test_samples_tricky.append([query, a])\n",
    "\n",
    "# get the predictions\n",
    "t4_predictions_tricky, t4_raw_outputs_tricky = mpnet_qA.predict(qa_test_samples_tricky)\n",
    "\n",
    "# format the predictions into an easy-to-read dataframe\n",
    "predictions_tricky = pd.DataFrame(qa_test_samples_tricky, columns=['User Query', 'Answer in Database'])\n",
    "predictions_tricky['Question in Database'] = df_faq['Question']\n",
    "predictions_tricky['Predicted QA Relevance'] = t4_predictions_tricky"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_tricky.sort_values(by='Predicted QA Relevance', ascending=False).to_csv('outputs/mpnet-t4/df_t4_predictions_tricky.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Trial 5: BERT**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# configure the classification model\n",
    "N_EPOCHS = 10\n",
    "bert_qA_args = ClassificationArgs(num_train_epochs=N_EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: 100%|██████████| 570/570 [00:00<00:00, 569kB/s]\n",
      "Downloading: 100%|██████████| 420M/420M [00:21<00:00, 20.8MB/s] \n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Downloading: 100%|██████████| 28.0/28.0 [00:00<00:00, 14.0kB/s]\n",
      "Downloading: 100%|██████████| 226k/226k [00:00<00:00, 249kB/s]  \n",
      "Downloading: 100%|██████████| 455k/455k [00:01<00:00, 415kB/s] \n"
     ]
    }
   ],
   "source": [
    "bert_qA = ClassificationModel('bert', 'bert-base-uncased', use_cuda=cuda_available, args=bert_qA_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:simpletransformers.classification.classification_utils: Converting to features started. Cache is not used.\n",
      "  0%|          | 1/420 [00:02<18:08,  2.60s/it]\n",
      "INFO:simpletransformers.classification.classification_utils: Saving features into cached file cache_dir/cached_train_bert_128_2_3\n",
      "c:\\Users\\Kackie\\anaconda3\\envs\\DataScience\\lib\\site-packages\\transformers\\optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  FutureWarning,\n",
      "Epochs 0/10. Running Loss:    0.9420: 100%|██████████| 53/53 [00:03<00:00, 16.32it/s]\n",
      "Epochs 1/10. Running Loss:    0.5595: 100%|██████████| 53/53 [00:02<00:00, 17.94it/s]\n",
      "Epochs 2/10. Running Loss:    0.0569: 100%|██████████| 53/53 [00:02<00:00, 18.32it/s]\n",
      "Epochs 3/10. Running Loss:    0.8850: 100%|██████████| 53/53 [00:02<00:00, 17.96it/s]\n",
      "Epochs 4/10. Running Loss:    0.8934: 100%|██████████| 53/53 [00:02<00:00, 17.94it/s]\n",
      "Epochs 5/10. Running Loss:    0.0277: 100%|██████████| 53/53 [00:02<00:00, 17.98it/s]\n",
      "Epochs 6/10. Running Loss:    0.0058: 100%|██████████| 53/53 [00:03<00:00, 17.25it/s]\n",
      "Epochs 7/10. Running Loss:    0.0051: 100%|██████████| 53/53 [00:02<00:00, 17.84it/s]\n",
      "Epochs 8/10. Running Loss:    0.0013: 100%|██████████| 53/53 [00:02<00:00, 17.69it/s]\n",
      "Epochs 9/10. Running Loss:    0.0024: 100%|██████████| 53/53 [00:02<00:00, 17.86it/s]\n",
      "Epoch 10 of 10: 100%|██████████| 10/10 [00:45<00:00,  4.56s/it]\n",
      "INFO:simpletransformers.classification.classification_model: Training of bert model complete. Saved to outputs/bert-t5.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(530, 0.2881794899702072)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_qA.train_model(df_train, output_dir='outputs/bert-t5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:simpletransformers.classification.classification_utils: Converting to features started. Cache is not used.\n",
      "  1%|          | 1/105 [00:03<05:43,  3.30s/it]\n",
      "INFO:simpletransformers.classification.classification_utils: Saving features into cached file cache_dir/cached_dev_bert_128_2_3\n",
      "Running Evaluation: 100%|██████████| 14/14 [00:00<00:00, 62.44it/s]\n",
      "INFO:simpletransformers.classification.classification_model:{'mcc': 0.5808188131873345, 'tp': 21, 'tn': 66, 'fp': 10, 'fn': 8, 'auroc': 0.8584392014519056, 'auprc': 0.7138490267997438, 'eval_loss': 0.7921056428125927}\n"
     ]
    }
   ],
   "source": [
    "t5_result, t5_model_outputs, t5_wrong_predictions = bert_qA.eval_model(\n",
    "    df_test\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:simpletransformers.classification.classification_utils: Converting to features started. Cache is not used.\n",
      " 33%|███▎      | 1/3 [00:02<00:05,  2.54s/it]\n",
      "100%|██████████| 1/1 [00:00<00:00,  4.29it/s]\n"
     ]
    }
   ],
   "source": [
    "t5_predictions, t5_raw_outputs = bert_qA.predict(qA_test_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1], dtype=int64)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t5_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# configure the classification model\n",
    "N_EPOCHS = 7\n",
    "bert_qA_pretrained_args = ClassificationArgs(num_train_epochs=N_EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "Can't load config for 'data/uncased_L-12_H-768_A-12'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'data/uncased_L-12_H-768_A-12' is the correct path to a directory containing a config.json file",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Kackie\\anaconda3\\envs\\DataScience\\lib\\site-packages\\transformers\\configuration_utils.py\u001b[0m in \u001b[0;36m_get_config_dict\u001b[1;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[0;32m    603\u001b[0m                 \u001b[0muse_auth_token\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0muse_auth_token\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 604\u001b[1;33m                 \u001b[0muser_agent\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0muser_agent\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    605\u001b[0m             )\n",
      "\u001b[1;32mc:\\Users\\Kackie\\anaconda3\\envs\\DataScience\\lib\\site-packages\\transformers\\utils\\hub.py\u001b[0m in \u001b[0;36mcached_path\u001b[1;34m(url_or_filename, cache_dir, force_download, proxies, resume_download, user_agent, extract_compressed_file, force_extract, use_auth_token, local_files_only)\u001b[0m\n\u001b[0;32m    296\u001b[0m         \u001b[1;31m# File, but it doesn't exist.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 297\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mEnvironmentError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"file {url_or_filename} not found\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    298\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mOSError\u001b[0m: file data/uncased_L-12_H-768_A-12\\config.json not found",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_21580\\4283148446.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mbert_qA_pretrained\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mClassificationModel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'roberta'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'data/uncased_L-12_H-768_A-12'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0muse_cuda\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcuda_available\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbert_qA_pretrained_args\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\Users\\Kackie\\anaconda3\\envs\\DataScience\\lib\\site-packages\\simpletransformers\\classification\\classification_model.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, model_type, model_name, tokenizer_type, tokenizer_name, num_labels, weight, args, use_cuda, cuda_device, onnx_execution_provider, **kwargs)\u001b[0m\n\u001b[0;32m    358\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnum_labels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnum_labels\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    359\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 360\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconfig\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconfig_class\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    361\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnum_labels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnum_labels\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    362\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Kackie\\anaconda3\\envs\\DataScience\\lib\\site-packages\\transformers\\configuration_utils.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[0;32m    519\u001b[0m         \u001b[1;32massert\u001b[0m \u001b[0munused_kwargs\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;34m\"foo\"\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    520\u001b[0m         ```\"\"\"\n\u001b[1;32m--> 521\u001b[1;33m         \u001b[0mconfig_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_config_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    522\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;34m\"model_type\"\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mconfig_dict\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"model_type\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mconfig_dict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"model_type\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mcls\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel_type\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    523\u001b[0m             logger.warning(\n",
      "\u001b[1;32mc:\\Users\\Kackie\\anaconda3\\envs\\DataScience\\lib\\site-packages\\transformers\\configuration_utils.py\u001b[0m in \u001b[0;36mget_config_dict\u001b[1;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[0;32m    546\u001b[0m         \u001b[0moriginal_kwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdeepcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    547\u001b[0m         \u001b[1;31m# Get config dict associated with the base config file\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 548\u001b[1;33m         \u001b[0mconfig_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_config_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    549\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    550\u001b[0m         \u001b[1;31m# That config file may point us toward another config file to use.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Kackie\\anaconda3\\envs\\DataScience\\lib\\site-packages\\transformers\\configuration_utils.py\u001b[0m in \u001b[0;36m_get_config_dict\u001b[1;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[0;32m    635\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mEnvironmentError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    636\u001b[0m             raise EnvironmentError(\n\u001b[1;32m--> 637\u001b[1;33m                 \u001b[1;34mf\"Can't load config for '{pretrained_model_name_or_path}'. If you were trying to load it from \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    638\u001b[0m                 \u001b[1;34m\"'https://huggingface.co/models', make sure you don't have a local directory with the same name. \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    639\u001b[0m                 \u001b[1;34mf\"Otherwise, make sure '{pretrained_model_name_or_path}' is the correct path to a directory \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mOSError\u001b[0m: Can't load config for 'data/uncased_L-12_H-768_A-12'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'data/uncased_L-12_H-768_A-12' is the correct path to a directory containing a config.json file"
     ]
    }
   ],
   "source": [
    "bert_qA_pretrained = ClassificationModel('roberta', 'data/uncased_L-12_H-768_A-12', use_cuda=cuda_available, args=bert_qA_pretrained_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:simpletransformers.classification.classification_utils: Converting to features started. Cache is not used.\n",
      "  0%|          | 1/420 [00:02<17:44,  2.54s/it]\n",
      "INFO:simpletransformers.classification.classification_utils: Saving features into cached file cache_dir/cached_train_roberta_128_2_3\n",
      "c:\\Users\\Kackie\\anaconda3\\envs\\DataScience\\lib\\site-packages\\transformers\\optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  FutureWarning,\n",
      "Epochs 0/12. Running Loss:    1.0514: 100%|██████████| 53/53 [00:03<00:00, 14.58it/s]\n",
      "Epochs 1/12. Running Loss:    0.0050: 100%|██████████| 53/53 [00:03<00:00, 16.10it/s]\n",
      "Epochs 2/12. Running Loss:    0.0099: 100%|██████████| 53/53 [00:03<00:00, 16.11it/s]\n",
      "Epochs 3/12. Running Loss:    0.6107: 100%|██████████| 53/53 [00:03<00:00, 16.67it/s]\n",
      "Epochs 4/12. Running Loss:    0.0019: 100%|██████████| 53/53 [00:03<00:00, 16.34it/s]\n",
      "Epochs 5/12. Running Loss:    0.0063: 100%|██████████| 53/53 [00:03<00:00, 16.25it/s]\n",
      "Epochs 6/12. Running Loss:    0.0002: 100%|██████████| 53/53 [00:03<00:00, 16.37it/s]\n",
      "Epochs 7/12. Running Loss:    0.0002: 100%|██████████| 53/53 [00:03<00:00, 16.24it/s]\n",
      "Epochs 8/12. Running Loss:    0.0002: 100%|██████████| 53/53 [00:03<00:00, 16.60it/s]\n",
      "Epochs 9/12. Running Loss:    0.0001: 100%|██████████| 53/53 [00:03<00:00, 16.01it/s]\n",
      "Epochs 10/12. Running Loss:    0.0002: 100%|██████████| 53/53 [00:03<00:00, 16.17it/s]\n",
      "Epochs 11/12. Running Loss:    0.0001: 100%|██████████| 53/53 [00:03<00:00, 16.12it/s]\n",
      "Epoch 12 of 12: 100%|██████████| 12/12 [01:01<00:00,  5.13s/it]\n",
      "INFO:simpletransformers.classification.classification_model: Training of roberta model complete. Saved to outputs/roberta-t3-squad2.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(636, 0.09954484901244535)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "roberta_qA_squad2.train_model(df_train, output_dir='outputs/roberta-t3-squad2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:simpletransformers.classification.classification_utils: Converting to features started. Cache is not used.\n",
      "  1%|          | 1/105 [00:03<05:39,  3.26s/it]\n",
      "INFO:simpletransformers.classification.classification_utils: Saving features into cached file cache_dir/cached_dev_roberta_128_2_3\n",
      "Running Evaluation: 100%|██████████| 14/14 [00:00<00:00, 34.54it/s]\n",
      "INFO:simpletransformers.classification.classification_model:{'mcc': 0.78353372286637, 'tp': 24, 'tn': 72, 'fp': 4, 'fn': 5, 'auroc': 0.9516787658802179, 'auprc': 0.9372552124290898, 'eval_loss': 0.6542652781520572}\n"
     ]
    }
   ],
   "source": [
    "t3_result, t3_model_outputs, t3_wrong_predictions = roberta_qA_squad2.eval_model(\n",
    "    df_test\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:simpletransformers.classification.classification_utils: Converting to features started. Cache is not used.\n",
      " 33%|███▎      | 1/3 [00:02<00:04,  2.49s/it]\n",
      "100%|██████████| 1/1 [00:00<00:00, 10.52it/s]\n"
     ]
    }
   ],
   "source": [
    "t3_predictions, t3_raw_outputs = roberta_qA_squad2.predict(qA_test_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 0], dtype=int64)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "t3_predictions"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.13 ('DataScience')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "32d60d92059b3e36b6bd9986edfc808c4ae526b74500509951e82855bb1b814d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
