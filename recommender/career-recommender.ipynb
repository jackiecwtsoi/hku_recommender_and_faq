{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Kackie\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Kackie\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Kackie\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\Kackie\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "from nltk import word_tokenize\n",
    "from nltk import sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem import PorterStemmer\n",
    "import string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load 'Online Job Postings' Kaggle dataset\n",
    "df_job_postings_original = pd.read_csv('data/online-job-postings-dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract useful columns for the career profile\n",
    "df_job_postings = df_job_postings_original[[\n",
    "    'jobpost', # the original job post is included just in case other columns do not contain enough information\n",
    "    'Title', 'JobDescription', 'JobRequirment',\n",
    "    'RequiredQual',\n",
    "    'ApplicationP'\n",
    "]]\n",
    "\n",
    "# drop any row that does not have a proper 'Title' or 'JobDescription'\n",
    "df_job_postings = df_job_postings[df_job_postings['Title'].notna()]\n",
    "df_job_postings = df_job_postings[df_job_postings['JobDescription'].notna()].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>jobpost</th>\n",
       "      <th>Title</th>\n",
       "      <th>JobDescription</th>\n",
       "      <th>JobRequirment</th>\n",
       "      <th>RequiredQual</th>\n",
       "      <th>ApplicationP</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AMERIA Investment Consulting Company\\r\\nJOB TI...</td>\n",
       "      <td>Chief Financial Officer</td>\n",
       "      <td>AMERIA Investment Consulting Company is seekin...</td>\n",
       "      <td>- Supervises financial management and administ...</td>\n",
       "      <td>To perform this job successfully, an\\r\\nindivi...</td>\n",
       "      <td>To apply for this position, please submit a\\r\\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Caucasus Environmental NGO Network (CENN)\\r\\nJ...</td>\n",
       "      <td>Country Coordinator</td>\n",
       "      <td>Public outreach and strengthening of a growing...</td>\n",
       "      <td>- Working with the Country Director to provide...</td>\n",
       "      <td>- Degree in environmentally related field, or ...</td>\n",
       "      <td>Please send resume or CV toursula.kazarian@......</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Manoff Group\\r\\nJOB TITLE:  BCC Specialist\\r\\n...</td>\n",
       "      <td>BCC Specialist</td>\n",
       "      <td>The LEAD (Local Enhancement and Development fo...</td>\n",
       "      <td>- Identify gaps in knowledge and overseeing in...</td>\n",
       "      <td>- Advanced degree in public health, social sci...</td>\n",
       "      <td>Please send cover letter and resume to Amy\\r\\n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Boutique \"Appollo\"\\r\\nJOB TITLE:  Saleswoman\\r...</td>\n",
       "      <td>Saleswoman</td>\n",
       "      <td>Saleswoman will sell menswear and accessories.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>- Candidates should be female, 20-30 years old...</td>\n",
       "      <td>For further information, please contact Irina\\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>OSI Assistance Foundation - Armenian Branch Of...</td>\n",
       "      <td>Chief Accountant/ Finance Assistant</td>\n",
       "      <td>The Armenian Branch Office of the Open Society...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>- University degree in finance/ accounting; \\r...</td>\n",
       "      <td>For submission of applications/ CVs, please\\r\\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15085</th>\n",
       "      <td>Macadamian AR CJSC\\r\\n\\r\\n\\r\\nTITLE:  .NET Dev...</td>\n",
       "      <td>.NET Developer</td>\n",
       "      <td>The incumbent will develop software applicatio...</td>\n",
       "      <td>- Participate in all the steps of the software...</td>\n",
       "      <td>- 2 - 5 years of experience in software develo...</td>\n",
       "      <td>To apply for this position, please email your ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15086</th>\n",
       "      <td>\"Transport PIU\" State Institution of the RA Mi...</td>\n",
       "      <td>Deputy Director</td>\n",
       "      <td>The incumbent will be responsible for supporti...</td>\n",
       "      <td>- Support the Director in organizing the activ...</td>\n",
       "      <td>- University degree in Civil Engineering, Econ...</td>\n",
       "      <td>Interested candidates are asked to submit the\\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15087</th>\n",
       "      <td>Technolinguistics NGO\\r\\n\\r\\n\\r\\nTITLE:  Senio...</td>\n",
       "      <td>Senior Creative UX/ UI Designer</td>\n",
       "      <td>A tech startup of Technolinguistics based in N...</td>\n",
       "      <td>- Work closely with product and business teams...</td>\n",
       "      <td>- At least 5 years of experience in Interface/...</td>\n",
       "      <td>To apply for this position, please send your\\r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15088</th>\n",
       "      <td>San Lazzaro   LLC\\r\\n\\r\\n\\r\\nTITLE:  Head of O...</td>\n",
       "      <td>Head of Online Sales Department</td>\n",
       "      <td>San Lazzaro LLC is looking for a well-experien...</td>\n",
       "      <td>- Handle the project activites of the online s...</td>\n",
       "      <td>- At least 1 year of experience in online sale...</td>\n",
       "      <td>Interested candidates can send their CVs to:\\r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15089</th>\n",
       "      <td>\"Kamurj\" UCO CJSC\\r\\n\\r\\n\\r\\nTITLE:  Lawyer in...</td>\n",
       "      <td>Lawyer in Legal Department</td>\n",
       "      <td>\"Kamurj\" UCO CJSC is looking for a Lawyer in L...</td>\n",
       "      <td>- Properly provide internal legal services of ...</td>\n",
       "      <td>- Higher legal education; Master's degree is a...</td>\n",
       "      <td>All qualified applicants are encouraged to\\r\\n...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>15090 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 jobpost  \\\n",
       "0      AMERIA Investment Consulting Company\\r\\nJOB TI...   \n",
       "1      Caucasus Environmental NGO Network (CENN)\\r\\nJ...   \n",
       "2      Manoff Group\\r\\nJOB TITLE:  BCC Specialist\\r\\n...   \n",
       "3      Boutique \"Appollo\"\\r\\nJOB TITLE:  Saleswoman\\r...   \n",
       "4      OSI Assistance Foundation - Armenian Branch Of...   \n",
       "...                                                  ...   \n",
       "15085  Macadamian AR CJSC\\r\\n\\r\\n\\r\\nTITLE:  .NET Dev...   \n",
       "15086  \"Transport PIU\" State Institution of the RA Mi...   \n",
       "15087  Technolinguistics NGO\\r\\n\\r\\n\\r\\nTITLE:  Senio...   \n",
       "15088  San Lazzaro   LLC\\r\\n\\r\\n\\r\\nTITLE:  Head of O...   \n",
       "15089  \"Kamurj\" UCO CJSC\\r\\n\\r\\n\\r\\nTITLE:  Lawyer in...   \n",
       "\n",
       "                                     Title  \\\n",
       "0                  Chief Financial Officer   \n",
       "1                      Country Coordinator   \n",
       "2                           BCC Specialist   \n",
       "3                               Saleswoman   \n",
       "4      Chief Accountant/ Finance Assistant   \n",
       "...                                    ...   \n",
       "15085                       .NET Developer   \n",
       "15086                      Deputy Director   \n",
       "15087      Senior Creative UX/ UI Designer   \n",
       "15088      Head of Online Sales Department   \n",
       "15089           Lawyer in Legal Department   \n",
       "\n",
       "                                          JobDescription  \\\n",
       "0      AMERIA Investment Consulting Company is seekin...   \n",
       "1      Public outreach and strengthening of a growing...   \n",
       "2      The LEAD (Local Enhancement and Development fo...   \n",
       "3         Saleswoman will sell menswear and accessories.   \n",
       "4      The Armenian Branch Office of the Open Society...   \n",
       "...                                                  ...   \n",
       "15085  The incumbent will develop software applicatio...   \n",
       "15086  The incumbent will be responsible for supporti...   \n",
       "15087  A tech startup of Technolinguistics based in N...   \n",
       "15088  San Lazzaro LLC is looking for a well-experien...   \n",
       "15089  \"Kamurj\" UCO CJSC is looking for a Lawyer in L...   \n",
       "\n",
       "                                           JobRequirment  \\\n",
       "0      - Supervises financial management and administ...   \n",
       "1      - Working with the Country Director to provide...   \n",
       "2      - Identify gaps in knowledge and overseeing in...   \n",
       "3                                                    NaN   \n",
       "4                                                    NaN   \n",
       "...                                                  ...   \n",
       "15085  - Participate in all the steps of the software...   \n",
       "15086  - Support the Director in organizing the activ...   \n",
       "15087  - Work closely with product and business teams...   \n",
       "15088  - Handle the project activites of the online s...   \n",
       "15089  - Properly provide internal legal services of ...   \n",
       "\n",
       "                                            RequiredQual  \\\n",
       "0      To perform this job successfully, an\\r\\nindivi...   \n",
       "1      - Degree in environmentally related field, or ...   \n",
       "2      - Advanced degree in public health, social sci...   \n",
       "3      - Candidates should be female, 20-30 years old...   \n",
       "4      - University degree in finance/ accounting; \\r...   \n",
       "...                                                  ...   \n",
       "15085  - 2 - 5 years of experience in software develo...   \n",
       "15086  - University degree in Civil Engineering, Econ...   \n",
       "15087  - At least 5 years of experience in Interface/...   \n",
       "15088  - At least 1 year of experience in online sale...   \n",
       "15089  - Higher legal education; Master's degree is a...   \n",
       "\n",
       "                                            ApplicationP  \n",
       "0      To apply for this position, please submit a\\r\\...  \n",
       "1      Please send resume or CV toursula.kazarian@......  \n",
       "2      Please send cover letter and resume to Amy\\r\\n...  \n",
       "3      For further information, please contact Irina\\...  \n",
       "4      For submission of applications/ CVs, please\\r\\...  \n",
       "...                                                  ...  \n",
       "15085  To apply for this position, please email your ...  \n",
       "15086  Interested candidates are asked to submit the\\...  \n",
       "15087  To apply for this position, please send your\\r...  \n",
       "15088  Interested candidates can send their CVs to:\\r...  \n",
       "15089  All qualified applicants are encouraged to\\r\\n...  \n",
       "\n",
       "[15090 rows x 6 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_job_postings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Label Aquisition\n",
    "Many of the job labels are very similar but not entirely the same, so we have to group these similar job titles together to form a more general job category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function\n",
    "# return: LIST of tokenized words\n",
    "def preprocess(text, with_stopwords=False, lemmatize=True, stemming=False):\n",
    "    text = text.lower()\n",
    "    words = word_tokenize(text)\n",
    "    words = [word for word in words if word.isalpha()]\n",
    "    if with_stopwords==False:\n",
    "        stop_words = set(stopwords.words('english'))\n",
    "        words = [word for word in words if not word in stop_words]\n",
    "    if lemmatize==True:\n",
    "        lemmatizer=WordNetLemmatizer()\n",
    "        words = [lemmatizer.lemmatize(word) for word in words]\n",
    "    if stemming==True:\n",
    "        stemmer=PorterStemmer()\n",
    "        words = [stemmer.stem(word) for word in words]\n",
    "\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess and tokenize all the job titles\n",
    "job_titles = df_job_postings['Title']\n",
    "job_titles_tokenized = job_titles.apply(preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader as api\n",
    "\n",
    "# load a pretrained word2vec model\n",
    "model_w2v = api.load('word2vec-google-news-300')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create document vectors for word embedding via average\n",
    "def vectorize_document(documents, model):\n",
    "    '''\n",
    "    Generate vectors of a list of documents using a word embedding model\n",
    "    '''\n",
    "\n",
    "    features = []\n",
    "\n",
    "    for tokens in documents:\n",
    "        zero_vector = np.zeros(model.vector_size)\n",
    "        vectors = []\n",
    "        for token in tokens:\n",
    "            if token in model:\n",
    "                try:\n",
    "                    vectors.append(model[token])\n",
    "                except KeyError: continue\n",
    "        if vectors:\n",
    "            vectors = np.asarray(vectors)\n",
    "            avg_vec = vectors.mean(axis=0)\n",
    "            features.append(avg_vec)\n",
    "        else:\n",
    "            features.append(zero_vector)\n",
    "    \n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vectorize each job title via average\n",
    "vectorized_job_titles = vectorize_document(job_titles_tokenized, model=model_w2v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import MiniBatchKMeans\n",
    "from sklearn.metrics import silhouette_samples, silhouette_score\n",
    "\n",
    "def mbkmeans_clusters(\n",
    "\tX, \n",
    "    k, \n",
    "    mb, \n",
    "    print_silhouette_values, \n",
    "):\n",
    "    \"\"\"Generate clusters and print Silhouette metrics using MBKmeans\n",
    "\n",
    "    Args:\n",
    "        X: Matrix of features.\n",
    "        k: Number of clusters.\n",
    "        mb: Size of mini-batches.\n",
    "        print_silhouette_values: Print silhouette values per cluster.\n",
    "\n",
    "    Returns:\n",
    "        Trained clustering model and labels based on X.\n",
    "    \"\"\"\n",
    "    km = MiniBatchKMeans(n_clusters=k, batch_size=mb).fit(X)\n",
    "    print(f\"For n_clusters = {k}\")\n",
    "    s_score = silhouette_score(X, km.labels_)\n",
    "    print(f\"Silhouette coefficient: {silhouette_score(X, km.labels_):0.2f}\")\n",
    "    print(f\"Inertia:{km.inertia_}\")\n",
    "\n",
    "    if print_silhouette_values:\n",
    "        sample_silhouette_values = silhouette_samples(X, km.labels_)\n",
    "        print(f\"Silhouette values:\")\n",
    "        silhouette_values = []\n",
    "        for i in range(k):\n",
    "            cluster_silhouette_values = sample_silhouette_values[km.labels_ == i]\n",
    "            silhouette_values.append(\n",
    "                (\n",
    "                    i,\n",
    "                    cluster_silhouette_values.shape[0],\n",
    "                    cluster_silhouette_values.mean(),\n",
    "                    cluster_silhouette_values.min(),\n",
    "                    cluster_silhouette_values.max(),\n",
    "                )\n",
    "            )\n",
    "        silhouette_values = sorted(\n",
    "            silhouette_values, key=lambda tup: tup[2], reverse=True\n",
    "        )\n",
    "        for s in silhouette_values:\n",
    "            print(\n",
    "                f\"    Cluster {s[0]}: Size:{s[1]} | Avg:{s[2]:.2f} | Min:{s[3]:.2f} | Max: {s[4]:.2f}\"\n",
    "            )\n",
    "    return km, km.labels_, s_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Kackie\\anaconda3\\envs\\DataScience\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1044: UserWarning: MiniBatchKMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can prevent it by setting batch_size >= 4096 or by setting the environment variable OMP_NUM_THREADS=2\n",
      "  \"MiniBatchKMeans is known to have a memory leak on \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For n_clusters = 2\n",
      "Silhouette coefficient: 0.11\n",
      "Inertia:47355.33056837851\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Kackie\\anaconda3\\envs\\DataScience\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1044: UserWarning: MiniBatchKMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can prevent it by setting batch_size >= 4096 or by setting the environment variable OMP_NUM_THREADS=2\n",
      "  \"MiniBatchKMeans is known to have a memory leak on \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For n_clusters = 3\n",
      "Silhouette coefficient: 0.08\n",
      "Inertia:45219.41286506242\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Kackie\\anaconda3\\envs\\DataScience\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1044: UserWarning: MiniBatchKMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can prevent it by setting batch_size >= 4096 or by setting the environment variable OMP_NUM_THREADS=2\n",
      "  \"MiniBatchKMeans is known to have a memory leak on \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For n_clusters = 4\n",
      "Silhouette coefficient: 0.10\n",
      "Inertia:46159.392230315374\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Kackie\\anaconda3\\envs\\DataScience\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1044: UserWarning: MiniBatchKMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can prevent it by setting batch_size >= 4096 or by setting the environment variable OMP_NUM_THREADS=2\n",
      "  \"MiniBatchKMeans is known to have a memory leak on \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For n_clusters = 5\n",
      "Silhouette coefficient: 0.10\n",
      "Inertia:41078.6230571767\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Kackie\\anaconda3\\envs\\DataScience\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1044: UserWarning: MiniBatchKMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can prevent it by setting batch_size >= 4096 or by setting the environment variable OMP_NUM_THREADS=2\n",
      "  \"MiniBatchKMeans is known to have a memory leak on \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For n_clusters = 6\n",
      "Silhouette coefficient: 0.08\n",
      "Inertia:41284.19105595874\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Kackie\\anaconda3\\envs\\DataScience\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1044: UserWarning: MiniBatchKMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can prevent it by setting batch_size >= 4096 or by setting the environment variable OMP_NUM_THREADS=2\n",
      "  \"MiniBatchKMeans is known to have a memory leak on \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For n_clusters = 7\n",
      "Silhouette coefficient: 0.10\n",
      "Inertia:39491.229585416455\n",
      "For n_clusters = 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Kackie\\anaconda3\\envs\\DataScience\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1044: UserWarning: MiniBatchKMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can prevent it by setting batch_size >= 4096 or by setting the environment variable OMP_NUM_THREADS=2\n",
      "  \"MiniBatchKMeans is known to have a memory leak on \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Silhouette coefficient: 0.09\n",
      "Inertia:39313.72530811372\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Kackie\\anaconda3\\envs\\DataScience\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1044: UserWarning: MiniBatchKMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can prevent it by setting batch_size >= 4096 or by setting the environment variable OMP_NUM_THREADS=2\n",
      "  \"MiniBatchKMeans is known to have a memory leak on \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For n_clusters = 9\n",
      "Silhouette coefficient: 0.10\n",
      "Inertia:37386.23928683061\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Kackie\\anaconda3\\envs\\DataScience\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1044: UserWarning: MiniBatchKMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can prevent it by setting batch_size >= 4096 or by setting the environment variable OMP_NUM_THREADS=2\n",
      "  \"MiniBatchKMeans is known to have a memory leak on \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For n_clusters = 10\n",
      "Silhouette coefficient: 0.12\n",
      "Inertia:36508.161114071234\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Kackie\\anaconda3\\envs\\DataScience\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1044: UserWarning: MiniBatchKMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can prevent it by setting batch_size >= 4096 or by setting the environment variable OMP_NUM_THREADS=2\n",
      "  \"MiniBatchKMeans is known to have a memory leak on \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For n_clusters = 11\n",
      "Silhouette coefficient: 0.11\n",
      "Inertia:36246.245810667715\n",
      "For n_clusters = 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Kackie\\anaconda3\\envs\\DataScience\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1044: UserWarning: MiniBatchKMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can prevent it by setting batch_size >= 4096 or by setting the environment variable OMP_NUM_THREADS=2\n",
      "  \"MiniBatchKMeans is known to have a memory leak on \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Silhouette coefficient: 0.09\n",
      "Inertia:35462.37160719347\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Kackie\\anaconda3\\envs\\DataScience\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1044: UserWarning: MiniBatchKMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can prevent it by setting batch_size >= 4096 or by setting the environment variable OMP_NUM_THREADS=2\n",
      "  \"MiniBatchKMeans is known to have a memory leak on \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For n_clusters = 13\n",
      "Silhouette coefficient: 0.07\n",
      "Inertia:35818.891595784815\n",
      "For n_clusters = 14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Kackie\\anaconda3\\envs\\DataScience\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1044: UserWarning: MiniBatchKMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can prevent it by setting batch_size >= 4096 or by setting the environment variable OMP_NUM_THREADS=2\n",
      "  \"MiniBatchKMeans is known to have a memory leak on \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Silhouette coefficient: 0.10\n",
      "Inertia:35333.942953705504\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Kackie\\anaconda3\\envs\\DataScience\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1044: UserWarning: MiniBatchKMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can prevent it by setting batch_size >= 4096 or by setting the environment variable OMP_NUM_THREADS=2\n",
      "  \"MiniBatchKMeans is known to have a memory leak on \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For n_clusters = 15\n",
      "Silhouette coefficient: 0.10\n",
      "Inertia:33753.229704006655\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Kackie\\anaconda3\\envs\\DataScience\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1044: UserWarning: MiniBatchKMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can prevent it by setting batch_size >= 4096 or by setting the environment variable OMP_NUM_THREADS=2\n",
      "  \"MiniBatchKMeans is known to have a memory leak on \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For n_clusters = 16\n",
      "Silhouette coefficient: 0.12\n",
      "Inertia:33199.08018566824\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Kackie\\anaconda3\\envs\\DataScience\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1044: UserWarning: MiniBatchKMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can prevent it by setting batch_size >= 4096 or by setting the environment variable OMP_NUM_THREADS=2\n",
      "  \"MiniBatchKMeans is known to have a memory leak on \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For n_clusters = 17\n",
      "Silhouette coefficient: 0.11\n",
      "Inertia:32839.19063098314\n",
      "For n_clusters = 18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Kackie\\anaconda3\\envs\\DataScience\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1044: UserWarning: MiniBatchKMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can prevent it by setting batch_size >= 4096 or by setting the environment variable OMP_NUM_THREADS=2\n",
      "  \"MiniBatchKMeans is known to have a memory leak on \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Silhouette coefficient: 0.10\n",
      "Inertia:33416.155371644825\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Kackie\\anaconda3\\envs\\DataScience\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1044: UserWarning: MiniBatchKMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can prevent it by setting batch_size >= 4096 or by setting the environment variable OMP_NUM_THREADS=2\n",
      "  \"MiniBatchKMeans is known to have a memory leak on \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For n_clusters = 19\n",
      "Silhouette coefficient: 0.12\n",
      "Inertia:31991.93542250681\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Kackie\\anaconda3\\envs\\DataScience\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1044: UserWarning: MiniBatchKMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can prevent it by setting batch_size >= 4096 or by setting the environment variable OMP_NUM_THREADS=2\n",
      "  \"MiniBatchKMeans is known to have a memory leak on \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For n_clusters = 20\n",
      "Silhouette coefficient: 0.14\n",
      "Inertia:30638.531176557957\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Kackie\\anaconda3\\envs\\DataScience\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1044: UserWarning: MiniBatchKMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can prevent it by setting batch_size >= 4096 or by setting the environment variable OMP_NUM_THREADS=2\n",
      "  \"MiniBatchKMeans is known to have a memory leak on \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For n_clusters = 21\n",
      "Silhouette coefficient: 0.12\n",
      "Inertia:31134.537576138235\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Kackie\\anaconda3\\envs\\DataScience\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1044: UserWarning: MiniBatchKMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can prevent it by setting batch_size >= 4096 or by setting the environment variable OMP_NUM_THREADS=2\n",
      "  \"MiniBatchKMeans is known to have a memory leak on \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For n_clusters = 22\n",
      "Silhouette coefficient: 0.11\n",
      "Inertia:31229.06161637942\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Kackie\\anaconda3\\envs\\DataScience\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1044: UserWarning: MiniBatchKMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can prevent it by setting batch_size >= 4096 or by setting the environment variable OMP_NUM_THREADS=2\n",
      "  \"MiniBatchKMeans is known to have a memory leak on \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For n_clusters = 23\n",
      "Silhouette coefficient: 0.13\n",
      "Inertia:30302.53453673642\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Kackie\\anaconda3\\envs\\DataScience\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1044: UserWarning: MiniBatchKMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can prevent it by setting batch_size >= 4096 or by setting the environment variable OMP_NUM_THREADS=2\n",
      "  \"MiniBatchKMeans is known to have a memory leak on \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For n_clusters = 24\n",
      "Silhouette coefficient: 0.13\n",
      "Inertia:30126.452438622906\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Kackie\\anaconda3\\envs\\DataScience\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1044: UserWarning: MiniBatchKMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can prevent it by setting batch_size >= 4096 or by setting the environment variable OMP_NUM_THREADS=2\n",
      "  \"MiniBatchKMeans is known to have a memory leak on \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For n_clusters = 25\n",
      "Silhouette coefficient: 0.14\n",
      "Inertia:29256.136459844372\n",
      "For n_clusters = 26\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Kackie\\anaconda3\\envs\\DataScience\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1044: UserWarning: MiniBatchKMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can prevent it by setting batch_size >= 4096 or by setting the environment variable OMP_NUM_THREADS=2\n",
      "  \"MiniBatchKMeans is known to have a memory leak on \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Silhouette coefficient: 0.13\n",
      "Inertia:29756.350084746144\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Kackie\\anaconda3\\envs\\DataScience\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1044: UserWarning: MiniBatchKMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can prevent it by setting batch_size >= 4096 or by setting the environment variable OMP_NUM_THREADS=2\n",
      "  \"MiniBatchKMeans is known to have a memory leak on \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For n_clusters = 27\n",
      "Silhouette coefficient: 0.14\n",
      "Inertia:29011.708481526977\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Kackie\\anaconda3\\envs\\DataScience\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1044: UserWarning: MiniBatchKMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can prevent it by setting batch_size >= 4096 or by setting the environment variable OMP_NUM_THREADS=2\n",
      "  \"MiniBatchKMeans is known to have a memory leak on \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For n_clusters = 28\n",
      "Silhouette coefficient: 0.15\n",
      "Inertia:28291.140014529083\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Kackie\\anaconda3\\envs\\DataScience\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1044: UserWarning: MiniBatchKMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can prevent it by setting batch_size >= 4096 or by setting the environment variable OMP_NUM_THREADS=2\n",
      "  \"MiniBatchKMeans is known to have a memory leak on \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For n_clusters = 29\n",
      "Silhouette coefficient: 0.15\n",
      "Inertia:28420.956228570623\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Kackie\\anaconda3\\envs\\DataScience\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1044: UserWarning: MiniBatchKMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can prevent it by setting batch_size >= 4096 or by setting the environment variable OMP_NUM_THREADS=2\n",
      "  \"MiniBatchKMeans is known to have a memory leak on \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For n_clusters = 30\n",
      "Silhouette coefficient: 0.15\n",
      "Inertia:28170.58724717598\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Kackie\\anaconda3\\envs\\DataScience\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1044: UserWarning: MiniBatchKMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can prevent it by setting batch_size >= 4096 or by setting the environment variable OMP_NUM_THREADS=2\n",
      "  \"MiniBatchKMeans is known to have a memory leak on \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For n_clusters = 31\n",
      "Silhouette coefficient: 0.14\n",
      "Inertia:28287.958185862124\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Kackie\\anaconda3\\envs\\DataScience\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1044: UserWarning: MiniBatchKMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can prevent it by setting batch_size >= 4096 or by setting the environment variable OMP_NUM_THREADS=2\n",
      "  \"MiniBatchKMeans is known to have a memory leak on \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For n_clusters = 32\n",
      "Silhouette coefficient: 0.15\n",
      "Inertia:27856.992256024045\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Kackie\\anaconda3\\envs\\DataScience\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1044: UserWarning: MiniBatchKMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can prevent it by setting batch_size >= 4096 or by setting the environment variable OMP_NUM_THREADS=2\n",
      "  \"MiniBatchKMeans is known to have a memory leak on \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For n_clusters = 33\n",
      "Silhouette coefficient: 0.14\n",
      "Inertia:27783.831885627522\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Kackie\\anaconda3\\envs\\DataScience\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1044: UserWarning: MiniBatchKMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can prevent it by setting batch_size >= 4096 or by setting the environment variable OMP_NUM_THREADS=2\n",
      "  \"MiniBatchKMeans is known to have a memory leak on \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For n_clusters = 34\n",
      "Silhouette coefficient: 0.15\n",
      "Inertia:27023.95202486168\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Kackie\\anaconda3\\envs\\DataScience\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1044: UserWarning: MiniBatchKMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can prevent it by setting batch_size >= 4096 or by setting the environment variable OMP_NUM_THREADS=2\n",
      "  \"MiniBatchKMeans is known to have a memory leak on \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For n_clusters = 35\n",
      "Silhouette coefficient: 0.17\n",
      "Inertia:26509.271990037414\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Kackie\\anaconda3\\envs\\DataScience\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1044: UserWarning: MiniBatchKMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can prevent it by setting batch_size >= 4096 or by setting the environment variable OMP_NUM_THREADS=2\n",
      "  \"MiniBatchKMeans is known to have a memory leak on \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For n_clusters = 36\n",
      "Silhouette coefficient: 0.15\n",
      "Inertia:26759.416664326367\n",
      "For n_clusters = 37\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Kackie\\anaconda3\\envs\\DataScience\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1044: UserWarning: MiniBatchKMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can prevent it by setting batch_size >= 4096 or by setting the environment variable OMP_NUM_THREADS=2\n",
      "  \"MiniBatchKMeans is known to have a memory leak on \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Silhouette coefficient: 0.15\n",
      "Inertia:26789.472675943696\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Kackie\\anaconda3\\envs\\DataScience\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1044: UserWarning: MiniBatchKMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can prevent it by setting batch_size >= 4096 or by setting the environment variable OMP_NUM_THREADS=2\n",
      "  \"MiniBatchKMeans is known to have a memory leak on \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For n_clusters = 38\n",
      "Silhouette coefficient: 0.15\n",
      "Inertia:26419.898203914236\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Kackie\\anaconda3\\envs\\DataScience\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1044: UserWarning: MiniBatchKMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can prevent it by setting batch_size >= 4096 or by setting the environment variable OMP_NUM_THREADS=2\n",
      "  \"MiniBatchKMeans is known to have a memory leak on \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For n_clusters = 39\n",
      "Silhouette coefficient: 0.15\n",
      "Inertia:26543.89417707379\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Kackie\\anaconda3\\envs\\DataScience\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1044: UserWarning: MiniBatchKMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can prevent it by setting batch_size >= 4096 or by setting the environment variable OMP_NUM_THREADS=2\n",
      "  \"MiniBatchKMeans is known to have a memory leak on \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For n_clusters = 40\n",
      "Silhouette coefficient: 0.16\n",
      "Inertia:26079.827414884712\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Kackie\\anaconda3\\envs\\DataScience\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1044: UserWarning: MiniBatchKMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can prevent it by setting batch_size >= 4096 or by setting the environment variable OMP_NUM_THREADS=2\n",
      "  \"MiniBatchKMeans is known to have a memory leak on \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For n_clusters = 41\n",
      "Silhouette coefficient: 0.17\n",
      "Inertia:25746.025032734622\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Kackie\\anaconda3\\envs\\DataScience\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1044: UserWarning: MiniBatchKMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can prevent it by setting batch_size >= 4096 or by setting the environment variable OMP_NUM_THREADS=2\n",
      "  \"MiniBatchKMeans is known to have a memory leak on \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For n_clusters = 42\n",
      "Silhouette coefficient: 0.18\n",
      "Inertia:25347.63907915088\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Kackie\\anaconda3\\envs\\DataScience\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1044: UserWarning: MiniBatchKMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can prevent it by setting batch_size >= 4096 or by setting the environment variable OMP_NUM_THREADS=2\n",
      "  \"MiniBatchKMeans is known to have a memory leak on \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For n_clusters = 43\n",
      "Silhouette coefficient: 0.17\n",
      "Inertia:25418.492329658337\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Kackie\\anaconda3\\envs\\DataScience\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1044: UserWarning: MiniBatchKMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can prevent it by setting batch_size >= 4096 or by setting the environment variable OMP_NUM_THREADS=2\n",
      "  \"MiniBatchKMeans is known to have a memory leak on \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For n_clusters = 44\n",
      "Silhouette coefficient: 0.16\n",
      "Inertia:25848.770904243825\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Kackie\\anaconda3\\envs\\DataScience\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1044: UserWarning: MiniBatchKMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can prevent it by setting batch_size >= 4096 or by setting the environment variable OMP_NUM_THREADS=2\n",
      "  \"MiniBatchKMeans is known to have a memory leak on \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For n_clusters = 45\n",
      "Silhouette coefficient: 0.17\n",
      "Inertia:25274.01329781402\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Kackie\\anaconda3\\envs\\DataScience\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1044: UserWarning: MiniBatchKMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can prevent it by setting batch_size >= 4096 or by setting the environment variable OMP_NUM_THREADS=2\n",
      "  \"MiniBatchKMeans is known to have a memory leak on \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For n_clusters = 46\n",
      "Silhouette coefficient: 0.18\n",
      "Inertia:24636.87472931819\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Kackie\\anaconda3\\envs\\DataScience\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1044: UserWarning: MiniBatchKMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can prevent it by setting batch_size >= 4096 or by setting the environment variable OMP_NUM_THREADS=2\n",
      "  \"MiniBatchKMeans is known to have a memory leak on \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For n_clusters = 47\n",
      "Silhouette coefficient: 0.19\n",
      "Inertia:24605.1013433928\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Kackie\\anaconda3\\envs\\DataScience\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1044: UserWarning: MiniBatchKMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can prevent it by setting batch_size >= 4096 or by setting the environment variable OMP_NUM_THREADS=2\n",
      "  \"MiniBatchKMeans is known to have a memory leak on \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For n_clusters = 48\n",
      "Silhouette coefficient: 0.17\n",
      "Inertia:24655.495890012855\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Kackie\\anaconda3\\envs\\DataScience\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1044: UserWarning: MiniBatchKMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can prevent it by setting batch_size >= 4096 or by setting the environment variable OMP_NUM_THREADS=2\n",
      "  \"MiniBatchKMeans is known to have a memory leak on \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For n_clusters = 49\n",
      "Silhouette coefficient: 0.17\n",
      "Inertia:24707.52231961341\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Kackie\\anaconda3\\envs\\DataScience\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1044: UserWarning: MiniBatchKMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can prevent it by setting batch_size >= 4096 or by setting the environment variable OMP_NUM_THREADS=2\n",
      "  \"MiniBatchKMeans is known to have a memory leak on \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For n_clusters = 50\n",
      "Silhouette coefficient: 0.17\n",
      "Inertia:24645.937130363433\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Kackie\\anaconda3\\envs\\DataScience\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1044: UserWarning: MiniBatchKMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can prevent it by setting batch_size >= 4096 or by setting the environment variable OMP_NUM_THREADS=2\n",
      "  \"MiniBatchKMeans is known to have a memory leak on \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For n_clusters = 51\n",
      "Silhouette coefficient: 0.16\n",
      "Inertia:23865.29372852291\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Kackie\\anaconda3\\envs\\DataScience\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1044: UserWarning: MiniBatchKMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can prevent it by setting batch_size >= 4096 or by setting the environment variable OMP_NUM_THREADS=2\n",
      "  \"MiniBatchKMeans is known to have a memory leak on \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For n_clusters = 52\n",
      "Silhouette coefficient: 0.18\n",
      "Inertia:23886.58611611557\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Kackie\\anaconda3\\envs\\DataScience\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1044: UserWarning: MiniBatchKMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can prevent it by setting batch_size >= 4096 or by setting the environment variable OMP_NUM_THREADS=2\n",
      "  \"MiniBatchKMeans is known to have a memory leak on \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For n_clusters = 53\n",
      "Silhouette coefficient: 0.19\n",
      "Inertia:23541.8892961579\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Kackie\\anaconda3\\envs\\DataScience\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1044: UserWarning: MiniBatchKMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can prevent it by setting batch_size >= 4096 or by setting the environment variable OMP_NUM_THREADS=2\n",
      "  \"MiniBatchKMeans is known to have a memory leak on \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For n_clusters = 54\n",
      "Silhouette coefficient: 0.18\n",
      "Inertia:24172.619506940453\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Kackie\\anaconda3\\envs\\DataScience\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1044: UserWarning: MiniBatchKMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can prevent it by setting batch_size >= 4096 or by setting the environment variable OMP_NUM_THREADS=2\n",
      "  \"MiniBatchKMeans is known to have a memory leak on \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For n_clusters = 55\n",
      "Silhouette coefficient: 0.18\n",
      "Inertia:23679.444845102607\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Kackie\\anaconda3\\envs\\DataScience\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1044: UserWarning: MiniBatchKMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can prevent it by setting batch_size >= 4096 or by setting the environment variable OMP_NUM_THREADS=2\n",
      "  \"MiniBatchKMeans is known to have a memory leak on \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For n_clusters = 56\n",
      "Silhouette coefficient: 0.18\n",
      "Inertia:23354.480646004286\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Kackie\\anaconda3\\envs\\DataScience\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1044: UserWarning: MiniBatchKMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can prevent it by setting batch_size >= 4096 or by setting the environment variable OMP_NUM_THREADS=2\n",
      "  \"MiniBatchKMeans is known to have a memory leak on \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For n_clusters = 57\n",
      "Silhouette coefficient: 0.20\n",
      "Inertia:23201.879245745622\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Kackie\\anaconda3\\envs\\DataScience\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1044: UserWarning: MiniBatchKMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can prevent it by setting batch_size >= 4096 or by setting the environment variable OMP_NUM_THREADS=2\n",
      "  \"MiniBatchKMeans is known to have a memory leak on \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For n_clusters = 58\n",
      "Silhouette coefficient: 0.17\n",
      "Inertia:23285.26037389945\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Kackie\\anaconda3\\envs\\DataScience\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1044: UserWarning: MiniBatchKMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can prevent it by setting batch_size >= 4096 or by setting the environment variable OMP_NUM_THREADS=2\n",
      "  \"MiniBatchKMeans is known to have a memory leak on \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For n_clusters = 59\n",
      "Silhouette coefficient: 0.18\n",
      "Inertia:23568.89049329067\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Kackie\\anaconda3\\envs\\DataScience\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1044: UserWarning: MiniBatchKMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can prevent it by setting batch_size >= 4096 or by setting the environment variable OMP_NUM_THREADS=2\n",
      "  \"MiniBatchKMeans is known to have a memory leak on \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For n_clusters = 60\n",
      "Silhouette coefficient: 0.19\n",
      "Inertia:22892.823820121535\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Kackie\\anaconda3\\envs\\DataScience\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1044: UserWarning: MiniBatchKMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can prevent it by setting batch_size >= 4096 or by setting the environment variable OMP_NUM_THREADS=2\n",
      "  \"MiniBatchKMeans is known to have a memory leak on \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For n_clusters = 61\n",
      "Silhouette coefficient: 0.20\n",
      "Inertia:22554.734493623597\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Kackie\\anaconda3\\envs\\DataScience\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1044: UserWarning: MiniBatchKMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can prevent it by setting batch_size >= 4096 or by setting the environment variable OMP_NUM_THREADS=2\n",
      "  \"MiniBatchKMeans is known to have a memory leak on \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For n_clusters = 62\n",
      "Silhouette coefficient: 0.19\n",
      "Inertia:22843.230408362586\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Kackie\\anaconda3\\envs\\DataScience\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1044: UserWarning: MiniBatchKMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can prevent it by setting batch_size >= 4096 or by setting the environment variable OMP_NUM_THREADS=2\n",
      "  \"MiniBatchKMeans is known to have a memory leak on \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For n_clusters = 63\n",
      "Silhouette coefficient: 0.19\n",
      "Inertia:22635.202746541472\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Kackie\\anaconda3\\envs\\DataScience\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1044: UserWarning: MiniBatchKMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can prevent it by setting batch_size >= 4096 or by setting the environment variable OMP_NUM_THREADS=2\n",
      "  \"MiniBatchKMeans is known to have a memory leak on \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For n_clusters = 64\n",
      "Silhouette coefficient: 0.19\n",
      "Inertia:22848.272167219806\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Kackie\\anaconda3\\envs\\DataScience\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1044: UserWarning: MiniBatchKMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can prevent it by setting batch_size >= 4096 or by setting the environment variable OMP_NUM_THREADS=2\n",
      "  \"MiniBatchKMeans is known to have a memory leak on \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For n_clusters = 65\n",
      "Silhouette coefficient: 0.18\n",
      "Inertia:22664.487221912666\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Kackie\\anaconda3\\envs\\DataScience\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1044: UserWarning: MiniBatchKMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can prevent it by setting batch_size >= 4096 or by setting the environment variable OMP_NUM_THREADS=2\n",
      "  \"MiniBatchKMeans is known to have a memory leak on \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For n_clusters = 66\n",
      "Silhouette coefficient: 0.20\n",
      "Inertia:22439.076336288697\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Kackie\\anaconda3\\envs\\DataScience\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1044: UserWarning: MiniBatchKMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can prevent it by setting batch_size >= 4096 or by setting the environment variable OMP_NUM_THREADS=2\n",
      "  \"MiniBatchKMeans is known to have a memory leak on \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For n_clusters = 67\n",
      "Silhouette coefficient: 0.21\n",
      "Inertia:21902.04239734648\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Kackie\\anaconda3\\envs\\DataScience\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1044: UserWarning: MiniBatchKMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can prevent it by setting batch_size >= 4096 or by setting the environment variable OMP_NUM_THREADS=2\n",
      "  \"MiniBatchKMeans is known to have a memory leak on \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For n_clusters = 68\n",
      "Silhouette coefficient: 0.21\n",
      "Inertia:22046.762544211313\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Kackie\\anaconda3\\envs\\DataScience\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1044: UserWarning: MiniBatchKMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can prevent it by setting batch_size >= 4096 or by setting the environment variable OMP_NUM_THREADS=2\n",
      "  \"MiniBatchKMeans is known to have a memory leak on \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For n_clusters = 69\n",
      "Silhouette coefficient: 0.19\n",
      "Inertia:22395.767730828\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Kackie\\anaconda3\\envs\\DataScience\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1044: UserWarning: MiniBatchKMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can prevent it by setting batch_size >= 4096 or by setting the environment variable OMP_NUM_THREADS=2\n",
      "  \"MiniBatchKMeans is known to have a memory leak on \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For n_clusters = 70\n",
      "Silhouette coefficient: 0.20\n",
      "Inertia:22283.548219803004\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Kackie\\anaconda3\\envs\\DataScience\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1044: UserWarning: MiniBatchKMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can prevent it by setting batch_size >= 4096 or by setting the environment variable OMP_NUM_THREADS=2\n",
      "  \"MiniBatchKMeans is known to have a memory leak on \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For n_clusters = 71\n",
      "Silhouette coefficient: 0.21\n",
      "Inertia:21809.729937901062\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Kackie\\anaconda3\\envs\\DataScience\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1044: UserWarning: MiniBatchKMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can prevent it by setting batch_size >= 4096 or by setting the environment variable OMP_NUM_THREADS=2\n",
      "  \"MiniBatchKMeans is known to have a memory leak on \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For n_clusters = 72\n",
      "Silhouette coefficient: 0.21\n",
      "Inertia:21713.110821584603\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Kackie\\anaconda3\\envs\\DataScience\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1044: UserWarning: MiniBatchKMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can prevent it by setting batch_size >= 4096 or by setting the environment variable OMP_NUM_THREADS=2\n",
      "  \"MiniBatchKMeans is known to have a memory leak on \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For n_clusters = 73\n",
      "Silhouette coefficient: 0.22\n",
      "Inertia:21311.958071734916\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Kackie\\anaconda3\\envs\\DataScience\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1044: UserWarning: MiniBatchKMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can prevent it by setting batch_size >= 4096 or by setting the environment variable OMP_NUM_THREADS=2\n",
      "  \"MiniBatchKMeans is known to have a memory leak on \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For n_clusters = 74\n",
      "Silhouette coefficient: 0.21\n",
      "Inertia:21684.243753467887\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Kackie\\anaconda3\\envs\\DataScience\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1044: UserWarning: MiniBatchKMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can prevent it by setting batch_size >= 4096 or by setting the environment variable OMP_NUM_THREADS=2\n",
      "  \"MiniBatchKMeans is known to have a memory leak on \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For n_clusters = 75\n",
      "Silhouette coefficient: 0.21\n",
      "Inertia:21272.299366331117\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Kackie\\anaconda3\\envs\\DataScience\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1044: UserWarning: MiniBatchKMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can prevent it by setting batch_size >= 4096 or by setting the environment variable OMP_NUM_THREADS=2\n",
      "  \"MiniBatchKMeans is known to have a memory leak on \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For n_clusters = 76\n",
      "Silhouette coefficient: 0.21\n",
      "Inertia:20986.9219537582\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Kackie\\anaconda3\\envs\\DataScience\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1044: UserWarning: MiniBatchKMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can prevent it by setting batch_size >= 4096 or by setting the environment variable OMP_NUM_THREADS=2\n",
      "  \"MiniBatchKMeans is known to have a memory leak on \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For n_clusters = 77\n",
      "Silhouette coefficient: 0.21\n",
      "Inertia:21300.653981057687\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Kackie\\anaconda3\\envs\\DataScience\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1044: UserWarning: MiniBatchKMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can prevent it by setting batch_size >= 4096 or by setting the environment variable OMP_NUM_THREADS=2\n",
      "  \"MiniBatchKMeans is known to have a memory leak on \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For n_clusters = 78\n",
      "Silhouette coefficient: 0.20\n",
      "Inertia:20920.122965143724\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Kackie\\anaconda3\\envs\\DataScience\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1044: UserWarning: MiniBatchKMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can prevent it by setting batch_size >= 4096 or by setting the environment variable OMP_NUM_THREADS=2\n",
      "  \"MiniBatchKMeans is known to have a memory leak on \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For n_clusters = 79\n",
      "Silhouette coefficient: 0.21\n",
      "Inertia:21306.384942770943\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Kackie\\anaconda3\\envs\\DataScience\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1044: UserWarning: MiniBatchKMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can prevent it by setting batch_size >= 4096 or by setting the environment variable OMP_NUM_THREADS=2\n",
      "  \"MiniBatchKMeans is known to have a memory leak on \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For n_clusters = 80\n",
      "Silhouette coefficient: 0.21\n",
      "Inertia:21034.41553901881\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Kackie\\anaconda3\\envs\\DataScience\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1044: UserWarning: MiniBatchKMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can prevent it by setting batch_size >= 4096 or by setting the environment variable OMP_NUM_THREADS=2\n",
      "  \"MiniBatchKMeans is known to have a memory leak on \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For n_clusters = 81\n",
      "Silhouette coefficient: 0.21\n",
      "Inertia:21450.66755243313\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Kackie\\anaconda3\\envs\\DataScience\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1044: UserWarning: MiniBatchKMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can prevent it by setting batch_size >= 4096 or by setting the environment variable OMP_NUM_THREADS=2\n",
      "  \"MiniBatchKMeans is known to have a memory leak on \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For n_clusters = 82\n",
      "Silhouette coefficient: 0.21\n",
      "Inertia:20903.86274438094\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Kackie\\anaconda3\\envs\\DataScience\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1044: UserWarning: MiniBatchKMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can prevent it by setting batch_size >= 4096 or by setting the environment variable OMP_NUM_THREADS=2\n",
      "  \"MiniBatchKMeans is known to have a memory leak on \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For n_clusters = 83\n",
      "Silhouette coefficient: 0.22\n",
      "Inertia:20674.145789922248\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Kackie\\anaconda3\\envs\\DataScience\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1044: UserWarning: MiniBatchKMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can prevent it by setting batch_size >= 4096 or by setting the environment variable OMP_NUM_THREADS=2\n",
      "  \"MiniBatchKMeans is known to have a memory leak on \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For n_clusters = 84\n",
      "Silhouette coefficient: 0.22\n",
      "Inertia:20887.414145514085\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Kackie\\anaconda3\\envs\\DataScience\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1044: UserWarning: MiniBatchKMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can prevent it by setting batch_size >= 4096 or by setting the environment variable OMP_NUM_THREADS=2\n",
      "  \"MiniBatchKMeans is known to have a memory leak on \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For n_clusters = 85\n",
      "Silhouette coefficient: 0.23\n",
      "Inertia:20611.39070596435\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Kackie\\anaconda3\\envs\\DataScience\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1044: UserWarning: MiniBatchKMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can prevent it by setting batch_size >= 4096 or by setting the environment variable OMP_NUM_THREADS=2\n",
      "  \"MiniBatchKMeans is known to have a memory leak on \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For n_clusters = 86\n",
      "Silhouette coefficient: 0.22\n",
      "Inertia:20914.07984510131\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Kackie\\anaconda3\\envs\\DataScience\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1044: UserWarning: MiniBatchKMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can prevent it by setting batch_size >= 4096 or by setting the environment variable OMP_NUM_THREADS=2\n",
      "  \"MiniBatchKMeans is known to have a memory leak on \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For n_clusters = 87\n",
      "Silhouette coefficient: 0.22\n",
      "Inertia:20784.084676394552\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Kackie\\anaconda3\\envs\\DataScience\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1044: UserWarning: MiniBatchKMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can prevent it by setting batch_size >= 4096 or by setting the environment variable OMP_NUM_THREADS=2\n",
      "  \"MiniBatchKMeans is known to have a memory leak on \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For n_clusters = 88\n",
      "Silhouette coefficient: 0.22\n",
      "Inertia:20555.171828782066\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Kackie\\anaconda3\\envs\\DataScience\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1044: UserWarning: MiniBatchKMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can prevent it by setting batch_size >= 4096 or by setting the environment variable OMP_NUM_THREADS=2\n",
      "  \"MiniBatchKMeans is known to have a memory leak on \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For n_clusters = 89\n",
      "Silhouette coefficient: 0.23\n",
      "Inertia:20640.734646838624\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Kackie\\anaconda3\\envs\\DataScience\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1044: UserWarning: MiniBatchKMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can prevent it by setting batch_size >= 4096 or by setting the environment variable OMP_NUM_THREADS=2\n",
      "  \"MiniBatchKMeans is known to have a memory leak on \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For n_clusters = 90\n",
      "Silhouette coefficient: 0.22\n",
      "Inertia:20507.7479717459\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Kackie\\anaconda3\\envs\\DataScience\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1044: UserWarning: MiniBatchKMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can prevent it by setting batch_size >= 4096 or by setting the environment variable OMP_NUM_THREADS=2\n",
      "  \"MiniBatchKMeans is known to have a memory leak on \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For n_clusters = 91\n",
      "Silhouette coefficient: 0.21\n",
      "Inertia:20541.38549774305\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Kackie\\anaconda3\\envs\\DataScience\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1044: UserWarning: MiniBatchKMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can prevent it by setting batch_size >= 4096 or by setting the environment variable OMP_NUM_THREADS=2\n",
      "  \"MiniBatchKMeans is known to have a memory leak on \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For n_clusters = 92\n",
      "Silhouette coefficient: 0.21\n",
      "Inertia:19914.156978086994\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Kackie\\anaconda3\\envs\\DataScience\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1044: UserWarning: MiniBatchKMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can prevent it by setting batch_size >= 4096 or by setting the environment variable OMP_NUM_THREADS=2\n",
      "  \"MiniBatchKMeans is known to have a memory leak on \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For n_clusters = 93\n",
      "Silhouette coefficient: 0.23\n",
      "Inertia:20101.86426325515\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Kackie\\anaconda3\\envs\\DataScience\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1044: UserWarning: MiniBatchKMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can prevent it by setting batch_size >= 4096 or by setting the environment variable OMP_NUM_THREADS=2\n",
      "  \"MiniBatchKMeans is known to have a memory leak on \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For n_clusters = 94\n",
      "Silhouette coefficient: 0.23\n",
      "Inertia:20157.96471869652\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Kackie\\anaconda3\\envs\\DataScience\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1044: UserWarning: MiniBatchKMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can prevent it by setting batch_size >= 4096 or by setting the environment variable OMP_NUM_THREADS=2\n",
      "  \"MiniBatchKMeans is known to have a memory leak on \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For n_clusters = 95\n",
      "Silhouette coefficient: 0.23\n",
      "Inertia:19876.958017255063\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Kackie\\anaconda3\\envs\\DataScience\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1044: UserWarning: MiniBatchKMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can prevent it by setting batch_size >= 4096 or by setting the environment variable OMP_NUM_THREADS=2\n",
      "  \"MiniBatchKMeans is known to have a memory leak on \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For n_clusters = 96\n",
      "Silhouette coefficient: 0.24\n",
      "Inertia:19984.40479696871\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Kackie\\anaconda3\\envs\\DataScience\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1044: UserWarning: MiniBatchKMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can prevent it by setting batch_size >= 4096 or by setting the environment variable OMP_NUM_THREADS=2\n",
      "  \"MiniBatchKMeans is known to have a memory leak on \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For n_clusters = 97\n",
      "Silhouette coefficient: 0.22\n",
      "Inertia:19986.846225201927\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Kackie\\anaconda3\\envs\\DataScience\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1044: UserWarning: MiniBatchKMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can prevent it by setting batch_size >= 4096 or by setting the environment variable OMP_NUM_THREADS=2\n",
      "  \"MiniBatchKMeans is known to have a memory leak on \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For n_clusters = 98\n",
      "Silhouette coefficient: 0.23\n",
      "Inertia:19870.229262906254\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Kackie\\anaconda3\\envs\\DataScience\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1044: UserWarning: MiniBatchKMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can prevent it by setting batch_size >= 4096 or by setting the environment variable OMP_NUM_THREADS=2\n",
      "  \"MiniBatchKMeans is known to have a memory leak on \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For n_clusters = 99\n",
      "Silhouette coefficient: 0.23\n",
      "Inertia:19553.343210712937\n"
     ]
    }
   ],
   "source": [
    "# find the best number of clusters by silhouette score\n",
    "scores = []\n",
    "for i in range(2, 100):\n",
    "    _, _, s_score = mbkmeans_clusters(\n",
    "        X=vectorized_job_titles,\n",
    "        k=i,\n",
    "        mb=500,\n",
    "        print_silhouette_values=False\n",
    "    )\n",
    "    scores.append(s_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(list(range(2, 100)), scores)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Kackie\\anaconda3\\envs\\DataScience\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1044: UserWarning: MiniBatchKMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can prevent it by setting batch_size >= 4096 or by setting the environment variable OMP_NUM_THREADS=2\n",
      "  \"MiniBatchKMeans is known to have a memory leak on \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For n_clusters = 200\n",
      "Silhouette coefficient: 0.28\n",
      "Inertia:15825.359843852397\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Kackie\\anaconda3\\envs\\DataScience\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1044: UserWarning: MiniBatchKMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can prevent it by setting batch_size >= 4096 or by setting the environment variable OMP_NUM_THREADS=2\n",
      "  \"MiniBatchKMeans is known to have a memory leak on \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For n_clusters = 250\n",
      "Silhouette coefficient: 0.32\n",
      "Inertia:14617.628627013433\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Kackie\\anaconda3\\envs\\DataScience\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1044: UserWarning: MiniBatchKMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can prevent it by setting batch_size >= 4096 or by setting the environment variable OMP_NUM_THREADS=2\n",
      "  \"MiniBatchKMeans is known to have a memory leak on \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For n_clusters = 300\n",
      "Silhouette coefficient: 0.33\n",
      "Inertia:13824.50251127476\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Kackie\\anaconda3\\envs\\DataScience\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1044: UserWarning: MiniBatchKMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can prevent it by setting batch_size >= 4096 or by setting the environment variable OMP_NUM_THREADS=2\n",
      "  \"MiniBatchKMeans is known to have a memory leak on \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For n_clusters = 350\n",
      "Silhouette coefficient: 0.35\n",
      "Inertia:13071.68640267748\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Kackie\\anaconda3\\envs\\DataScience\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1044: UserWarning: MiniBatchKMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can prevent it by setting batch_size >= 4096 or by setting the environment variable OMP_NUM_THREADS=2\n",
      "  \"MiniBatchKMeans is known to have a memory leak on \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For n_clusters = 400\n",
      "Silhouette coefficient: 0.38\n",
      "Inertia:12453.440300247179\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Kackie\\anaconda3\\envs\\DataScience\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1044: UserWarning: MiniBatchKMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can prevent it by setting batch_size >= 4096 or by setting the environment variable OMP_NUM_THREADS=2\n",
      "  \"MiniBatchKMeans is known to have a memory leak on \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For n_clusters = 450\n",
      "Silhouette coefficient: 0.39\n",
      "Inertia:11893.413274632052\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Kackie\\anaconda3\\envs\\DataScience\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1044: UserWarning: MiniBatchKMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can prevent it by setting batch_size >= 4096 or by setting the environment variable OMP_NUM_THREADS=2\n",
      "  \"MiniBatchKMeans is known to have a memory leak on \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For n_clusters = 500\n",
      "Silhouette coefficient: 0.39\n",
      "Inertia:11304.147320019547\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Kackie\\anaconda3\\envs\\DataScience\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1044: UserWarning: MiniBatchKMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can prevent it by setting batch_size >= 4096 or by setting the environment variable OMP_NUM_THREADS=2\n",
      "  \"MiniBatchKMeans is known to have a memory leak on \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For n_clusters = 550\n",
      "Silhouette coefficient: 0.41\n",
      "Inertia:10809.324244532638\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Kackie\\anaconda3\\envs\\DataScience\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1044: UserWarning: MiniBatchKMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can prevent it by setting batch_size >= 4096 or by setting the environment variable OMP_NUM_THREADS=2\n",
      "  \"MiniBatchKMeans is known to have a memory leak on \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For n_clusters = 600\n",
      "Silhouette coefficient: 0.42\n",
      "Inertia:10282.081075416378\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Kackie\\anaconda3\\envs\\DataScience\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1044: UserWarning: MiniBatchKMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can prevent it by setting batch_size >= 4096 or by setting the environment variable OMP_NUM_THREADS=2\n",
      "  \"MiniBatchKMeans is known to have a memory leak on \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For n_clusters = 650\n",
      "Silhouette coefficient: 0.43\n",
      "Inertia:10261.3728999326\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Kackie\\anaconda3\\envs\\DataScience\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1044: UserWarning: MiniBatchKMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can prevent it by setting batch_size >= 4096 or by setting the environment variable OMP_NUM_THREADS=2\n",
      "  \"MiniBatchKMeans is known to have a memory leak on \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For n_clusters = 700\n",
      "Silhouette coefficient: 0.45\n",
      "Inertia:9642.38753686911\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Kackie\\anaconda3\\envs\\DataScience\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1044: UserWarning: MiniBatchKMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can prevent it by setting batch_size >= 4096 or by setting the environment variable OMP_NUM_THREADS=2\n",
      "  \"MiniBatchKMeans is known to have a memory leak on \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For n_clusters = 750\n",
      "Silhouette coefficient: 0.45\n",
      "Inertia:9419.585956817464\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Kackie\\anaconda3\\envs\\DataScience\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1044: UserWarning: MiniBatchKMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can prevent it by setting batch_size >= 4096 or by setting the environment variable OMP_NUM_THREADS=2\n",
      "  \"MiniBatchKMeans is known to have a memory leak on \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For n_clusters = 800\n",
      "Silhouette coefficient: 0.45\n",
      "Inertia:9448.488293959916\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Kackie\\anaconda3\\envs\\DataScience\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1044: UserWarning: MiniBatchKMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can prevent it by setting batch_size >= 4096 or by setting the environment variable OMP_NUM_THREADS=2\n",
      "  \"MiniBatchKMeans is known to have a memory leak on \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For n_clusters = 850\n",
      "Silhouette coefficient: 0.47\n",
      "Inertia:8891.61019425498\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Kackie\\anaconda3\\envs\\DataScience\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1044: UserWarning: MiniBatchKMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can prevent it by setting batch_size >= 4096 or by setting the environment variable OMP_NUM_THREADS=2\n",
      "  \"MiniBatchKMeans is known to have a memory leak on \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For n_clusters = 900\n",
      "Silhouette coefficient: 0.47\n",
      "Inertia:8745.287973470695\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Kackie\\anaconda3\\envs\\DataScience\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1044: UserWarning: MiniBatchKMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can prevent it by setting batch_size >= 4096 or by setting the environment variable OMP_NUM_THREADS=2\n",
      "  \"MiniBatchKMeans is known to have a memory leak on \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For n_clusters = 950\n",
      "Silhouette coefficient: 0.49\n",
      "Inertia:8389.989457042118\n"
     ]
    }
   ],
   "source": [
    "# find the best number of clusters by silhouette score\n",
    "scores = []\n",
    "for i in range(200, 1000, 50):\n",
    "    _, _, s_score = mbkmeans_clusters(\n",
    "        X=vectorized_job_titles,\n",
    "        k=i,\n",
    "        mb=500,\n",
    "        print_silhouette_values=False\n",
    "    )\n",
    "    scores.append(s_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjUAAAGdCAYAAADqsoKGAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAA9hAAAPYQGoP6dpAABPg0lEQVR4nO3de1yUVf4H8M8wAzM4XESuooCIIip4A0Ugs9KlyExzS8QVNXPVXa1Yfu2m28XLsuGaW+YWpmUqWkpXq81SrFSIvBGYtxRFRZGLoDAgMgMz5/cHOdsIKIPAXPi8X695rXOe8zzP96grn85zORIhhAARERGRhbMxdQFEREREbYGhhoiIiKwCQw0RERFZBYYaIiIisgoMNURERGQVGGqIiIjIKjDUEBERkVVgqCEiIiKrIDN1AR1Jp9Ph8uXLcHR0hEQiMXU5RERE1AJCCFRVVcHb2xs2Ns3Px3SqUHP58mX4+PiYugwiIiJqhYsXL6Jnz57Nbu9UocbR0RFAw2+Kk5OTiashIiKillCpVPDx8dH/HG9Opwo1Ny85OTk5MdQQERFZmDvdOsIbhYmIiMgqMNQQERGRVWCoISIiIqvAUENERERWgaGGiIiIrAJDDREREVkFhhoiIiKyCgw1REREZBUYaoiIiMgqMNQQERGRVWCoISIiIqvAUENERERWgaGGiIiI7ooQApv3X8CiT4+atI5OtUo3ERERta0rVWo8/8nP+O6XUgDAuJDuuKevm0lqYaghIiKiVvn2ZAn+9vHPKL+ugZ3MBgsfCkJkgKvJ6mGoISIiIqPc0GiR9NUJvH+gAAAQ5OWIVVOGIMjLyaR1MdQQERFRix29VIln03KQf+U6AGD2Pf547sF+UNhKTVwZQw0RERG1gFYn8Pbes3g9/TTqdQJeTgr8e/JgRPUxzf0zTWGoISIiotu6dK0GiWlHcPD8VQDAwyFeeOWxEHTtYmfiygwx1BAREVGztucU4qXtx1ClrofSTooljw7E46E9IZFITF1aIww1RERE1EjljTq8tP0YvjhyGQAwzLcrVsUOha9rFxNX1jyGGiIiIjKwP78ciWm5uFxZC6mNBM+O6Ys/3xcAmdS839nLUENEREQAAE29Dq+ln8bafWchBODn2gWrYodgqK+LqUtrEYYaIiIiwpnSaiSk5eBYoQoAEBvmg5fHD4BSbjlRwXIqJSIiojYnhMCW/Rfwzx0nUVung0sXWyRPGoSHgr1MXZrRGGqIiIg6qStVavzt4yP4/tQVAMCovm5Y+cRgeDopTFxZ6zDUEBERdUK3rtu0KCYIMyJ6wcbG/B7VbqlW3cackpICf39/KBQKhIaGIiMjo0X7/fDDD5DJZBgyZIhB+3333QeJRNLoM27cOH2fJUuWNNru5WV5U2NERESmdEOjxQufHcVTmw6j/LoGQV6O+HLBPXgyyt+iAw3QipmatLQ0JCQkICUlBVFRUVi7di1iYmJw4sQJ+Pr6NrtfZWUlpk+fjjFjxqCkpMRg26effgqNRqP/Xl5ejsGDB+OJJ54w6Ddw4EDs3r1b/10qNf06E0RERJbi1nWb/jiqYd0mucw6fp4aHWpee+01PPXUU5g9ezYAYNWqVdi5cyfWrFmD5OTkZvebO3cupk6dCqlUiu3btxts69atm8H3bdu2oUuXLo1CjUwm4+wMERGRkSxh3aa2YNTlJ41Gg+zsbERHRxu0R0dHIysrq9n9NmzYgLNnz2Lx4sUtOs/69esxZcoUKJVKg/a8vDx4e3vD398fU6ZMQX5+/m2Po1aroVKpDD5ERESdyaVrNYhbtx+v7jyFep3AuJDu+CZhlNUFGsDImZqysjJotVp4enoatHt6eqK4uLjJffLy8rBw4UJkZGRAJrvz6Q4ePIhjx45h/fr1Bu3h4eFITU1FYGAgSkpKkJSUhMjISBw/fhyurq5NHis5ORlLly5t4eiIiIisy63rNi2dEIzfD+thlus2tYVWPf1062+GEKLJ3yCtVoupU6di6dKlCAwMbNGx169fj+DgYIwYMcKgPSYmRv/rkJAQREREICAgAJs2bUJiYmKTx1q0aJHBNpVKBR8fnxbVQUREZKluXbcp1M8Fr08eYtbrNrUFo0KNm5sbpFJpo1mZ0tLSRrM3AFBVVYXDhw8jJycHCxYsAADodDoIISCTybBr1y488MAD+v41NTXYtm0bli1bdsdalEolQkJCkJeX12wfuVwOuVze0uERERFZJE29Dleq1bhSpcaF8uv419e/WNy6TW3BqFBjZ2eH0NBQpKen47HHHtO3p6enY8KECY36Ozk54ejRowZtKSkp+O677/Dxxx/D39/fYNuHH34ItVqNadOm3bEWtVqNkydPYtSoUcYMgYiIyCIIIVClrkepqiGslFbV4krVzV//r620So2KmrpG+/dy7YJVU4ZiiE/Xji/eRIy+/JSYmIj4+HiEhYUhIiIC69atQ0FBAebNmweg4ZJPYWEhUlNTYWNjg+DgYIP9PTw8oFAoGrUDDZeeJk6c2OQ9Ms899xzGjx8PX19flJaWIikpCSqVCjNmzDB2CERERCaj1QmUVxsGk5tBpVSlxpXq/7XV1ulafFxbqQTuDnK4Oykw3M8Ff/ldoEWt29QWjB5tbGwsysvLsWzZMhQVFSE4OBg7duyAn58fAKCoqAgFBQVGF3L69GlkZmZi165dTW6/dOkS4uLiUFZWBnd3d4wcORL79+/Xn5eIiMic7Dt9BQfOlf8vqKgagsvV62roRMuP4yiXwd1JDncHOTycFPBwlMPdUQ4PRzk8HBX6X3ftYmu1NwC3lEQIYcRvrWVTqVRwdnZGZWUlnJycTF0OERFZqY8OX8RfP/652e02EsDVQW4QUNx/DSm//bW7oxz2dtbxYry70dKf351rXoqIiKid/Xi2HH//rOF+0gcHeiKkh7M+oLg7yuHhJIerUg6phS9JYI4YaoiIiNrI2SvVmLclG3VagUcGdcfqKUMtfj0lS2L9z3cRERF1gKvXNZi18RAqb9RhqG9XrHxiMANNB2OoISIiuku1dVrMST2MC+U18Olmj3emh0Fhy3thOhpDDRER0V0QQuD5T37G4QvX4KiQYcPM4XBz4ItfTYGhhoiI6C68vjsPn+dehsxGgrenhaKPh6OpS+q0GGqIiIha6dOfLmH1tw3L9fzzsWCrXPnakjDUEBERtcKB/HI8/0nDu2jmjQ5A7HBfE1dEDDVERERGOld2HXN/fXT74RAv/O3BfqYuicBQQ0REZJRr1zV4csNBVNTUYbBPV7w2eQgf3TYTDDVEREQtpK7XYu7mbJwvr0GPrvZ4l49umxWGGiIiohYQQmDhJ0dx8PxVOMpl2PDkcLg78tFtc8JQQ0RE1AKrvz2Dz3IKIbWRIGXaMAR68tFtc8NQQ0REdAef5xbi9d2nAQD/mBCMUX3dTVwRNYWhhoiI6DYOnb+Kv37U8Oj2nHt7Y2o4H902Vww1REREzThfdh1zUg9Do9XhwYGeWPhQkKlLottgqCEiImpCRU3DqtvXauowqKczVsUO5aPbZo6hhoiI6Baaeh3mbclGftl1eDsr8O70MNjb8dFtc8dQQ0RE9BtCCCz69Cj251+Fg1yG954cDg8nhanLohZgqCEiIvqNt74/g09+ugSpjQRvTh2KIC8nU5dELcRQQ0RE9Ksvj1zGyl0Nj24veXQg7uvnYeKKyBgMNURERACyL1zF/310BADw1D3+iB/pZ+KKyFgMNURE1OkVlNfgj6nZ0NTrMLa/J/7+cH9Tl0StwFBDRESdWmVNHZ7ceBBXr2sQ3MMJq+OGQMpHty0SQw0REXVamnod/vR+Ns5euY7uzgqsnzEcXexkpi6LWomhhoiIOiUhBF7cfhRZZ8uhtJNi/Yzh8OSj2xaNoYaIiDqlNXvP4sPDl2AjAd6cOgwDvPnotqVjqCEiok7nq5+LsOKbUwCAxeMH4v4gPrptDRhqiIioU/mp4BoSP8wFAMyM7IUZkb1MWg+1HYYaIiLqNC5ercGc1MNQ1+swJsgDLz0ywNQlURtiqCEiok6h8kYdZm08hLJqDQZ0d8LquKF8dNvKtCrUpKSkwN/fHwqFAqGhocjIyGjRfj/88ANkMhmGDBli0L5x40ZIJJJGn9ra2jY5LxERdW51Wh3mv/8T8kqr4ekkx/qZYVDK+ei2tTE61KSlpSEhIQEvvPACcnJyMGrUKMTExKCgoOC2+1VWVmL69OkYM2ZMk9udnJxQVFRk8FEo/vdoXWvPS0REnZsQAi9/fgyZZ8rQ5ddHt7s725u6LGoHEiGEMGaH8PBwDBs2DGvWrNG39e/fHxMnTkRycnKz+02ZMgV9+/aFVCrF9u3bkZubq9+2ceNGJCQkoKKios3P+1sqlQrOzs6orKyEkxMf3SMismZCCJRWqfHBgQK88W0ebCTAuvgwjB3gaerSyEgt/flt1NybRqNBdnY2Fi5caNAeHR2NrKysZvfbsGEDzp49iy1btiApKanJPtXV1fDz84NWq8WQIUPwj3/8A0OHDr2r86rVaqjVav13lUp1xzESEZHluXZdg1MlVcgrqcKpkiqcLq7GqZIqVN6o0/d5cdwABhorZ1SoKSsrg1arhaen4V8KT09PFBcXN7lPXl4eFi5ciIyMDMhkTZ8uKCgIGzduREhICFQqFd544w1ERUXhyJEj6Nu3b6vOCwDJyclYunSpMUMkIiIzVlVbh7zSapwurvo1xDSElytV6ib720iAXq5KTB7ugyejenVssdThWnWXlERieLe4EKJRGwBotVpMnToVS5cuRWBgYLPHGzlyJEaOHKn/HhUVhWHDhuE///kPVq9ebfR5b1q0aBESExP131UqFXx8fJofGBERmYXaOi3OlFbjtH7mpQqnS6pRWHGj2X16utijn6cjAr0cEejpgEBPRwS4O0BhK+3AysmUjAo1bm5ukEqljWZHSktLG82iAEBVVRUOHz6MnJwcLFiwAACg0+kghIBMJsOuXbvwwAMPNNrPxsYGw4cPR15eXqvOe5NcLodcLjdmiERE1IHqtDqcK7uOU8W/uXRUUo0L5deha+aOT08nOQI9HRHo6agPMX08HODAp5k6PaP+BtjZ2SE0NBTp6el47LHH9O3p6emYMGFCo/5OTk44evSoQVtKSgq+++47fPzxx/D392/yPEII5ObmIiQkpFXnJSKi1hFCQCf+9786ISB+/V9dE9sMtwM6XcN3gd/u3/Dreq3AxWs1DbMuv15Cyi+rRp226fTi0sW2Ibh4OaLvzQDj6YCuXew6+HeFLIXRsTYxMRHx8fEICwtDREQE1q1bh4KCAsybNw9AwyWfwsJCpKamwsbGBsHBwQb7e3h4QKFQGLQvXboUI0eORN++faFSqbB69Wrk5ubirbfeavF5iYjozn4quIbnPjqCooraJgOLKTjIZejr6fBraLkZYhzg7iC/7S0GRLcyOtTExsaivLwcy5YtQ1FREYKDg7Fjxw74+fkBAIqKiox+d0xFRQXmzJmD4uJiODs7Y+jQodi3bx9GjBjR4vMSEdHt7T19BfM2Z+NGnbbNjimRADYSCWwkDfc92ui/Swy23fzu5axodOnI21nB8EJtwuj31FgyvqeGiDqrL49cRuKHuajTCtwb6I6ljw6ErVSiDyBNhhKbxqHERiKBBP/7zjBCHaFd3lNDRESWZ/P+C3j582MQAnhkUHe8NnkI7GRc+o+sD0MNEZGVEkLgP9+dwWvppwEA8SP9sOTRgVzEkawWQw0RkRXS6QSW/fcENmadBwA8M6Yv/jK2Ly8XkVVjqCEisjJ1Wh3++tERbM+9DABYPH4Anoxq+hUaRNaEoYaIyIrc0Ggx/4Of8N0vpZDZSLDyicGYOLSHqcsi6hAMNUREVqLyRh2e2ngIhy9cg8LWBmv+EIr7gzxMXRZRh2GoISKyAqWqWkx/7yB+Ka6Co0KG92YOx/Be3UxdFlGHYqghIrJwBeU1mLb+AAqu1sDdUY7UWSPQvzvfxUWdD0MNEZEFO1mkwvT3DuJKlRq+3bpg81Mj4OeqNHVZRCbBUENEZKEOnb+KWRsPoaq2HkFejkidNQIeTgpTl0VkMgw1REQW6PtfSvGn97NRW6dDmJ8L1s8cDmd7W1OXRWRSDDVERBZme04hnvvoCOp1Avf3c0fKH0Jhbyc1dVlEJsdQQ0RkQTb8cA5LvzwBAHhsaA+seHwQbKVcx4kIYKghIrIIQgi8vjsPq7/NAwDMjOyFlx8ZABuu40Skx1BDRGTmdDqBxV8cx+b9FwAAib8LxNMP9OE6TkS3YKghIjJjmnod/u+jI/jyyGVIJMCyCcGIH+ln6rKIzBJDDRGRmarR1GPelp+w7/QV2EoleG3yEIwf7G3qsojMFkMNEZEZqqjRYNbGQ/ipoAL2tlK8HR+K0YHupi6LyKwx1BARmZniylpMf+8ATpdUw9neFu/NHI5QPxdTl0Vk9hhqiIjMyLmy64hffwCXrt2Ap5Mcm58KR6Cno6nLIrIIDDVERGbiWGElZm44iLJqDXq5dsHmp8Lh062LqcsishgMNUREZuBAfjlmbzqMKnU9BnR3wqZZI+DuKDd1WUQWhaGGiMjE0k+UYMEHP0Fdr8MI/254d0YYnBRcx4nIWAw1REQm9En2Jfztk5+h1QmM7e+JN6cOhcKW6zgRtQZDDRGRCeh0Auszz+GfO04CAH4/rCf+9fsQyLiOE1GrMdQQEXWQOq0O+/PL8c2xYuw6UYIrVWoAwOx7/PH3h/tzHSeiu8RQQ0TUjmrrtMjIK8M3x4qx+2QJKm/U6bc5KmR45oG+mD3Kn+s4EbUBhhoiojZWra7H97+U4pvjxdjzSymua7T6ba5KO0QP9MSDA70QGeAGOxkvNxG1FYYaIqI2UFGjQfqJEuw8Xox9eWXQ1Ov027o7K/DgQC88FOyF4b26QcrLTETtgqGGiKiVSqtqsfN4CXYeK8aP+eXQ6oR+Wy/XLngouDseCvbC4J7OvLxE1AFaNe+ZkpICf39/KBQKhIaGIiMjo0X7/fDDD5DJZBgyZIhB+zvvvINRo0bBxcUFLi4uGDt2LA4ePGjQZ8mSJZBIJAYfLy+v1pRPRNRqF6/W4N2MfDy+Jgvhr3yLl7YfQ+aZMmh1AkFejkgY2xffJIzC98/dh4UxQRji05WBhqiDGD1Tk5aWhoSEBKSkpCAqKgpr165FTEwMTpw4AV9f32b3q6ysxPTp0zFmzBiUlJQYbNuzZw/i4uIQGRkJhUKBFStWIDo6GsePH0ePHj30/QYOHIjdu3frv0ulfJcDEbW/M6XV+OZYEb45XoxjhSqDbYN9uiIm2AsPDvSCv5vSRBUSEQBIhBDizt3+Jzw8HMOGDcOaNWv0bf3798fEiRORnJzc7H5TpkxB3759IZVKsX37duTm5jbbV6vVwsXFBW+++SamT58OoGGm5k773YlKpYKzszMqKyvh5OTU6uMQkXUTQuD4ZRW+OVaMb44X40xptX6bjQQY3qsbYoK9ED3QC95d7U1YKVHn0NKf30bN1Gg0GmRnZ2PhwoUG7dHR0cjKymp2vw0bNuDs2bPYsmULkpKS7niempoa1NXVoVu3bgbteXl58Pb2hlwuR3h4OF555RX07t3bmCEQETVJpxPIuXgNXx9tCDKXrt3Qb7OVShAZ4IaYYC+MHeAJNweuyURkjowKNWVlZdBqtfD09DRo9/T0RHFxcZP75OXlYeHChcjIyIBM1rLTLVy4ED169MDYsWP1beHh4UhNTUVgYCBKSkqQlJSEyMhIHD9+HK6urk0eR61WQ61W67+rVKom+xFR51VRo8Gq3XnYcbQIpVX/+/dCYWuD0YHuiAnujvuDPOBsz7WYiMxdq55+uvWmNyFEkzfCabVaTJ06FUuXLkVgYGCLjr1ixQps3boVe/bsgUKh0LfHxMTofx0SEoKIiAgEBARg06ZNSExMbPJYycnJWLp0aYvOS0SdT6mqFtPWH8DpkobLS45yGR7o74GYYC/cG+iOLnZ8QJTIkhj1/1g3NzdIpdJGszKlpaWNZm8AoKqqCocPH0ZOTg4WLFgAANDpdBBCQCaTYdeuXXjggQf0/VeuXIlXXnkFu3fvxqBBg25bi1KpREhICPLy8prts2jRIoPAo1Kp4OPj06KxEpF1u3i1BtPWH8CF8hp4OMqRPCkE9/R1g1zGBxCILJVRocbOzg6hoaFIT0/HY489pm9PT0/HhAkTGvV3cnLC0aNHDdpSUlLw3Xff4eOPP4a/v7++/dVXX0VSUhJ27tyJsLCwO9aiVqtx8uRJjBo1qtk+crkccjmvfRORoTOl1Zj27gEUq2rh080e7z81Er6uXUxdFhHdJaPnVhMTExEfH4+wsDBERERg3bp1KCgowLx58wA0zI4UFhYiNTUVNjY2CA4ONtjfw8MDCoXCoH3FihV46aWX8MEHH6BXr176mSAHBwc4ODgAAJ577jmMHz8evr6+KC0tRVJSElQqFWbMmNHqwRNR53OssBLT3zuIq9c16OPhgC1PhcPLWXHnHYnI7BkdamJjY1FeXo5ly5ahqKgIwcHB2LFjB/z8/AAARUVFKCgoMOqYKSkp0Gg0ePzxxw3aFy9ejCVLlgAALl26hLi4OJSVlcHd3R0jR47E/v379eclIrqTw+ev4skNh1ClrkdID2dsmjUC3ZR2pi6LiNqI0e+psWR8Tw1R57Xv9BXM3ZyNG3VajOjVDe/ODIOTgk80EVmCdnlPDRGRJfrmWBGe2ZoLjVaH0YHueHtaKOzteEMwkbVhqCEiq/ZJ9iX89eMj0Ang4RAvrIodCjtZq5a9IyIzx1BDRFYr9cfzePnz4wCAJ0J7InlSCGRSBhoia8VQQ0RWRwiBlD1n8erOUwCAJ6N64aVxA2Bjw9WyiawZQw0RWRUhBJZ/8wvW7s0HADwzpi/+MrZvk289JyLrwlBDRFZDpxN46fNjeP9Aw2slXni4P/54Lxe9JeosGGqIyCrUaXV47qMj+Dz3MiQS4JXHQhA3wtfUZRFRB2KoISKLV1unxYIPcrD7ZAlkNhK8FjsEjw72NnVZRNTBGGqIyKJdV9djzubD+OFMOexkNljzh2EY07/xArtEZP0YaojIYlXW1GHmxoPIKaiA0k6Kd2aEITLAzdRlEZGJMNQQkUW6UqVG/PoD+KW4Cs72ttg0awSG+HQ1dVlEZEIMNURkcQorbiD+3QPIL7sONwc5tswegSAvrudG1Nkx1BCRRcm/Uo1p7x7A5cpa9Ohqj/dnh6OXm9LUZRGRGWCoISKLcbJIhfj1B1BWrUFvNyW2zA6Hd1d7U5dFRGaCoYaILMJPBdcw872DUNXWY0B3J6Q+NQJuDnJTl0VEZoShhojMXtaZMsxOPYwajRahfi54b+ZwONvbmrosIjIzDDVEZNbST5Rg/gc/QVOvwz193LBueii62PGfLiJqjP8yEJHZ+jy3EIkfHoFWJxA9wBOr44ZCYSs1dVlEZKYYaojILL1/4AJe3H4MQgCThvbAiscHQSa1MXVZRGTGGGqIyOy8vfcsln/9CwAgfqQflj46EDY2EhNXRUTmjqGGiMyGEAL/3nUab35/BgDw5/sC8NcH+0EiYaAhojtjqCEis6DTCSz77wlszDoPAPjbQ/3w5/v6mLYoIrIoDDVEZHKaeh3+9vERbM+9DIkEWDYhGPEj/UxdFhFZGIYaIjKpanU9/rQlGxl5ZZDZSPDqE4Pw2NCepi6LiCwQQw0RmcyVKjVmbTyEo4WVsLeVYs20Ybivn4epyyIiC8VQQ0QmcaH8Oqa/dxAXymvQTWmH92YOxxCfrqYui4gsGEMNEXW4o5cq8eTGgyir1sCnmz1SZ4XDnyttE9FdYqghog6VkXcF8zZn47pGiwHdnbBx1nB4OCpMXRYRWQGGGiLqMNtzCvHcR0dQrxOI6uOKt6eFwlHBhSmJqG0w1BBRh3hnXz7+ueMkAGD8YG+sfGIQ5DKu40REbYehhojalU4n8MqOk3g38xwAYFaUP14c15/LHhBRm2vV6nApKSnw9/eHQqFAaGgoMjIyWrTfDz/8AJlMhiFDhjTa9sknn2DAgAGQy+UYMGAAPvvsszY7LxGZhqZeh798mKsPNItigvDSIww0RNQ+jA41aWlpSEhIwAsvvICcnByMGjUKMTExKCgouO1+lZWVmD59OsaMGdNo248//ojY2FjEx8fjyJEjiI+Px+TJk3HgwIG7Pi8RmUa1uh5PbTqEz3MvQ2YjwWuTB2Pu6ACu40RE7UYihBDG7BAeHo5hw4ZhzZo1+rb+/ftj4sSJSE5Obna/KVOmoG/fvpBKpdi+fTtyc3P122JjY6FSqfD111/r2x566CG4uLhg69atd3Xe31KpVHB2dkZlZSWcnJxaOmQiMtKVKjWe3HgQxwpV6GInRcof+FI9Imq9lv78NmqmRqPRIDs7G9HR0Qbt0dHRyMrKana/DRs24OzZs1i8eHGT23/88cdGx3zwwQf1x2ztedVqNVQqlcGHiNrX+bLr+P2aLBwrVMFVaYetfxzJQENEHcKoUFNWVgatVgtPT0+Ddk9PTxQXFze5T15eHhYuXIj3338fMlnT9yUXFxff9pitOS8AJCcnw9nZWf/x8fG54xiJqPV+vlSB36/JQsHVGvh0s8fHf4rEYL4lmIg6SKtuFL71mrgQosnr5FqtFlOnTsXSpUsRGBh418ds6XlvWrRoESorK/Wfixcv3rYGImq9faevYMq6/Si/rsFAbyd88qdIviWYiDqUUY90u7m5QSqVNpodKS0tbTSLAgBVVVU4fPgwcnJysGDBAgCATqeDEAIymQy7du3CAw88AC8vr9se09jz3iSXyyGXy40ZIhG1Al+qR0TmwKiZGjs7O4SGhiI9Pd2gPT09HZGRkY36Ozk54ejRo8jNzdV/5s2bh379+iE3Nxfh4eEAgIiIiEbH3LVrl/6Yxp6XiDrOO/vykZCWi3qdwKODvbFh5ggGGiIyCaNfvpeYmIj4+HiEhYUhIiIC69atQ0FBAebNmweg4ZJPYWEhUlNTYWNjg+DgYIP9PTw8oFAoDNqfffZZ3HvvvfjXv/6FCRMm4PPPP8fu3buRmZnZ4vMSUcfiS/WIyNwYHWpiY2NRXl6OZcuWoaioCMHBwdixYwf8/PwAAEVFRUa/OyYyMhLbtm3Diy++iJdeegkBAQFIS0vTz+S05LxE1HE09To899ERfHHkMgDg7w8H4Y+jevMdNERkUka/p8aS8T01RHevWl2PeZuzkXmmDDIbCVY8PgiThvU0dVlEZMVa+vObaz8RUYvd+lK9NdNCMTrQ3dRlEREBYKghMluFFTfw8eFL6Olij35ejujj4QCFrelWtT5fdh3T3zuIgqs1cFXaYcOTwzGoZ1eT1UNEdCuGGiIzVFR5A5Pf/hGFFTf0bVIbCfzdlAjycvz144R+Xo7o6WLf7vey/HypAk9uOITy6xr4duuC1Fkj0IvvoCEiM8NQQ2RmyqvVmPbuARRW3EBPF3v06GqPUyVVqKipw5nSapwprcZ/fy7S93eUyxCoDzqOCOruhEBPRzjbt81j1XtPX8GftmSjRqPFQG8nbHhyODwcFW1ybCKitsRQQ2RGVLV1mP7eQZy9ch3ezgqkzY1Aj672EEKgRKXGL8Uq/FJchVPFVThZpMLZK9WoUtcj+8I1ZF+4ZnCsHl0bLlv1+83MTm93JWylLX891Wc5l/DXj35GvU7gnj5ueDs+FA5y/rNBROaJTz8RmYkbGi2mv3cAh85fg6vSDh/Oi0CAu8Nt96nT6pB/5bpB2PmlSIXLlbVN9reVShDg7oD+3Z0Mwo6nk9zgEpYQAu9k5OOVHb8AAB4d7I2VTwyGnaxVK6sQEd2Vlv78ZqghMgOaeh3+mHoYe09fgaNChm1zRmKgt3Orj1d5ow6niqtw6tewczPwVKvrm+zftYst+nk66sPOL0UqbPrxAgDgqXv88cLDfKkeEZkOQ00TGGrIHGl1As9szcFXR4tgbyvFltkjEOrXrc3PI4TApWs3GmZzfhN2zpVdh1bX9D8DLzzcH3+8t3eb10JEZAy+p4bIAgghsOjTn/HV0SLYSW2wbnpouwQaoGGVe59uXeDTrQvGDvjfQrC1dVqcKa02CDsVNXWYPcofE4b0aJdaiIjaA0MNkYkIIZD01Ul8ePgSbCTA6rghGNW3419kp7CVIriHM4J7tP5yFxGROeBdf0QmsvrbM1j/62KQKx4fjIeCu5u4IiIiy8ZQQ2QC72Wew+u7TwMAFo8fgMdDuXYSEdHdYqgh6mAfHr6IZf89AQBI/F0gnozyN3FFRETWgaGGqAN9fbQICz/5GQDwx1H+ePqBPiauiIjIejDUEHWQvaev4JltOdAJYMpwH/z94f7tvmYTEVFnwlBD1AEOnb+KuZsPo04rMG5Qd/zzsRAGGiKiNsZQQ9TOjhVWYtaGQ6it0+G+fu54ffIQSPl2XiKiNsdQQ9SOzpRWY/p7B1GlrscI/25Y84dQrp9ERNRO+K8rUTu5dK0G8esP4Op1DUJ6OGP9jDDY20lNXRYRkdViqCFqB6VVtZj27gEUVdaij4cDNs0aAUeFranLIiKyagw1RG2ssqYO09cfxPnyGvR0sceWp8LRTWln6rKIiKweQw1RG7qursfMjQfxS3EVPBzleH92OLycFaYui4ioU2CoIWojtXVazNl8GDkFFejaxRabnwqHn6vS1GUREXUaDDVEbaBOq8PTW3Pww5lyKO2k2PTkCPTzcjR1WUREnQpDDdFd0ukE/vbxz0g/UQI7mQ3enTEcg326mrosIqJOh6GG6C4IIbDky+P4LKcQMhsJ1vxhGCICXE1dFhFRp8RQQ3QXVu46hdQfL0AiAf49eTDG9Pc0dUlERJ0WQw1RK7299yze+v4sAOCfE0MwYUgPE1dERNS5MdQQtcL7By5g+de/AAAWxQRhariviSsiIiKGGiIjfZ5biBe3HwMAzL8/AHNHB5i4IiIiAhhqiIzy7ckS/N+HRyAEMD3CD89F9zN1SURE9KtWhZqUlBT4+/tDoVAgNDQUGRkZzfbNzMxEVFQUXF1dYW9vj6CgILz++usGfe677z5IJJJGn3Hjxun7LFmypNF2Ly+v1pRP1Co/ni3Hn9//CfU6gceG9sCS8QMhkUhMXRYREf1KZuwOaWlpSEhIQEpKCqKiorB27VrExMTgxIkT8PVtfF+BUqnEggULMGjQICiVSmRmZmLu3LlQKpWYM2cOAODTTz+FRqPR71NeXo7BgwfjiSeeMDjWwIEDsXv3bv13qZQrHlPHOHKxArM3HYK6XoffDfDEq48Pgo0NAw0RkTmRCCGEMTuEh4dj2LBhWLNmjb6tf//+mDhxIpKTk1t0jEmTJkGpVGLz5s1Nbl+1ahVefvllFBUVQalseM38kiVLsH37duTm5hpTrgGVSgVnZ2dUVlbCycmp1cehzuV0SRUmr/0RFTV1iOrjivUzhkNhy0BNRNRRWvrz26jLTxqNBtnZ2YiOjjZoj46ORlZWVouOkZOTg6ysLIwePbrZPuvXr8eUKVP0geamvLw8eHt7w9/fH1OmTEF+fv5tz6VWq6FSqQw+RMYorarFzPcOoqKmDkN9u2JdfBgDDRGRmTIq1JSVlUGr1cLT0/AFY56eniguLr7tvj179oRcLkdYWBjmz5+P2bNnN9nv4MGDOHbsWKPt4eHhSE1Nxc6dO/HOO++guLgYkZGRKC8vb/acycnJcHZ21n98fHxaOFIi4IZGiz9uOozLlbXo7abEhpnDoZQbfcWWiIg6SKtuFL715kghxB1vmMzIyMDhw4fx9ttvY9WqVdi6dWuT/davX4/g4GCMGDHCoD0mJga///3vERISgrFjx+Krr74CAGzatKnZcy5atAiVlZX6z8WLF1syPCLodAKJH+biyKVKuHSxxYYnh6NrFztTl0VERLdh1H92urm5QSqVNpqVKS0tbTR7cyt/f38AQEhICEpKSrBkyRLExcUZ9KmpqcG2bduwbNmyO9aiVCoREhKCvLy8ZvvI5XLI5fI7HovoVq/uOoWvjxXDTmqDddPD4OeqvPNORERkUkbN1NjZ2SE0NBTp6ekG7enp6YiMjGzxcYQQUKvVjdo//PBDqNVqTJs27Y7HUKvVOHnyJLp3797i8xK1xIeHLmLNnoblD1Y8PgjDe3UzcUVERNQSRt8gkJiYiPj4eISFhSEiIgLr1q1DQUEB5s2bB6Dhkk9hYSFSU1MBAG+99RZ8fX0RFBQEoOG9NStXrsTTTz/d6Njr16/HxIkT4eraeJXj5557DuPHj4evry9KS0uRlJQElUqFGTNmGDsEomZlnS3D3z87CgB4dkxfTBzK9ZyIiCyF0aEmNjYW5eXlWLZsGYqKihAcHIwdO3bAz88PAFBUVISCggJ9f51Oh0WLFuHcuXOQyWQICAjA8uXLMXfuXIPjnj59GpmZmdi1a1eT57106RLi4uJQVlYGd3d3jBw5Evv379efl+hunb1SjXmbs1GvE3h0sDcSxvY1dUlERGQEo99TY8n4nhpqztXrGjyW8gMulNcg1M8F788O56PbRERmol3eU0NkjdT1WsxJPYwL5TXw6WaPdfGhDDRERBaIoYY6NSEEnv/4Zxy+cA2OChk2zBwOVwc+MUdEZIkYaqhTW/3tGWzPvQyZjQRvTwtFHw9HU5dEREStxFBDndbnuYV4ffdpAMA/JgYjqo+biSsiIqK7wVBDndLh81fx149+BgDMvbc34kY0XmGeiIgsC0MNdToF5TWYszkbGq0O0QM88fxDQaYuiYiI2gBDDXUqlTfq8OTGg7h6XYOQHs5YNWUIbGxuv24ZERFZBoYa6jTqtDr8+f1snL1yHd2dFXh3Rhi62HHVbSIia8FQQ52CEAIvbT+GH86UQ2knxfoZw+HppDB1WURE1IYYaqhTWLcvH9sOXYSNBPjP1KEY4M03ShMRWRuGGrJ63xwrwvJvfgEAvPzIADwQ5GniioiIqD0w1JBV+/lSBRLSciEEMCPCDzOj/E1dEhERtROGGrJalytu4KlNh1Fbp8N9/dzx0iMDTF0SERG1I4YaskrV6nrM2ngIV6rUCPJyxH/ihkIm5V93IiJrxn/lyerUa3V4+oOf8EtxFdwc5Fg/czgcFbamLouIiNoZQw1ZnaSvTuL7U1egsLXB+hlh6NHV3tQlERFRB2CoIauyKes8NmadBwC8PnkIBvt0NWk9RETUcRhqyGp8/0spln55HADw/ENBiAnpbuKKiIioIzHUkFU4WaTCgg9+gk4Ak8N6Yt7o3qYuiYiIOhhDDVm8UlUtntp4CNc1WkT0dkXSxBBIJFykkoios2GoIYt2Q6PF7NTDuFxZi97uSrw9LRR2Mv61JiLqjPivP1ksnU7gL2m5+PlSJVy62GLDzOFw7sJHt4mIOiuGGrJYK3aewjfHi2EntcG66WHwc1WauiQiIjIhhhqySGmHCvD23rMAgBWPD8LwXt1MXBEREZkaQw1ZnKwzZXjhs2MAgGfH9MXEoT1MXBEREZkDhhqyKGdKqzFvSzbqdQKPDvZGwti+pi6JiIjMBEMNWYyr1zWYtfEQVLX1CPVzwYrHB/HRbSIi0mOoIYug0wk8vfUnFFytgU83e6yLD4XCVmrqsoiIyIww1JBFeCcjHz+cKYe9rRTvzRgOVwe5qUsiIiIzw1BDZu9YYSVW7joFAFg8fgD6ejqauCIiIjJHDDVk1mo09XhmWw7qtAIPDvRE7HAfU5dERERmqlWhJiUlBf7+/lAoFAgNDUVGRkazfTMzMxEVFQVXV1fY29sjKCgIr7/+ukGfjRs3QiKRNPrU1ta2+rxkHZK+Oon8K9fh6STH8km8MZiIiJonM3aHtLQ0JCQkICUlBVFRUVi7di1iYmJw4sQJ+Pr6NuqvVCqxYMECDBo0CEqlEpmZmZg7dy6USiXmzJmj7+fk5IRTp04Z7KtQKFp9XrJ8u44X44MDBZBIgNcmD4GL0s7UJRERkRmTCCGEMTuEh4dj2LBhWLNmjb6tf//+mDhxIpKTk1t0jEmTJkGpVGLz5s0AGmZqEhISUFFR0a7nValUcHZ2RmVlJZycnFq0D5lGiaoWD63ah2s1dZh7b28seri/qUsiIiITaenPb6MuP2k0GmRnZyM6OtqgPTo6GllZWS06Rk5ODrKysjB69GiD9urqavj5+aFnz5545JFHkJOTc9fnVavVUKlUBh8yfzqdwHMfHcG1mjoM9HZCYnSgqUsiIiILYFSoKSsrg1arhaenp0G7p6cniouLb7tvz549IZfLERYWhvnz52P27Nn6bUFBQdi4cSO++OILbN26FQqFAlFRUcjLy7ur8yYnJ8PZ2Vn/8fHhTaaW4L0fziEjrwwKWxu8MWUo5DK+j4aIiO7M6HtqADS6WVMIcccbODMyMlBdXY39+/dj4cKF6NOnD+Li4gAAI0eOxMiRI/V9o6KiMGzYMPznP//B6tWrW33eRYsWITExUf9dpVIx2Ji545crseKbhnurXnpkAPp4OJi4IiIishRGhRo3NzdIpdJGsyOlpaWNZlFu5e/vDwAICQlBSUkJlixZog81t7KxscHw4cP1MzWtPa9cLodczpe0WYobGi2e3ZYLjVaHsf09MXUEbwAnIqKWM+ryk52dHUJDQ5Genm7Qnp6ejsjIyBYfRwgBtVp92+25ubno3r17m56XzNsrO07iTGk13B3l+NfvQ/j4NhERGcXoy0+JiYmIj49HWFgYIiIisG7dOhQUFGDevHkAGi75FBYWIjU1FQDw1ltvwdfXF0FBQQAa3luzcuVKPP300/pjLl26FCNHjkTfvn2hUqmwevVq5Obm4q233mrxecmyfXuyBJv3XwAA/PuJwVwGgYiIjGZ0qImNjUV5eTmWLVuGoqIiBAcHY8eOHfDz8wMAFBUVoaCgQN9fp9Nh0aJFOHfuHGQyGQICArB8+XLMnTtX36eiogJz5sxBcXExnJ2dMXToUOzbtw8jRoxo8XnJcpVW1eJvH/8MAHjqHn/cG+hu4oqIiMgSGf2eGkvG99SYH51O4MmNh7D39BUEeTni8wVRfNqJiIgMtMt7aoja2qYfz2Pv6SuQy2ywOo6PbxMRUesx1JDJ/FKsQvLXvwAAXhjXH4FcfZuIiO4CQw2ZRG2dFs9uzYWmXocHgjwQP5L3RhER0d1hqCGTWP71LzhVUgU3BzuseJyrbxMR0d1jqKEO9/2pUmzMOg8AePWJwXDj49tERNQGGGqoQ5VVq/HXj44AAGZG9sL9/TxMXBEREVkLhhrqMEII/PWjIyir1qCfpyMWxgSZuiQiIrIiDDXUYTbvv4DvT12BncwGb8QNgcKWj28TEVHbYaihDnG6pAr//OokAGBRTBCCvPjyQyIialsMNdTuauu0eGZrDtT1OowOdMfMyF6mLomIiKwQQw21u1d3nsIvxVVwVdrh1Sf4+DYREbUPhhpqV/tOX8H6zHMAgBWPD4KHo8LEFRERkbViqKF2U16txv/9+vh2/Eg/jOnvaeKKiIjImjHUULsQQuD5T47iSpUafTwc8MK4/qYuiYiIrBxDDbWL9w8UYPfJEthJbbB6ylA+vk1ERO2OoYba3JnSKiR9dQIA8LeH+mGANx/fJiKi9sdQQ21KXa/FM1tzUVunw6i+bpgV5W/qkoiIqJNgqKE29e9dp3GiSAWXLrZY+cRg2Njw8W0iIuoYDDXUZjLzyrBuXz4A4F+/HwRPJz6+TUREHYehhtrEtesa/N9HuQCAqeG+iB7oZdqCiIio02GoobsmhMDCT39GiUqN3u5KvMjHt4mIyAQYauiupR26iJ3HS2ArlWD1lKHoYiczdUlERNQJMdTQXTl7pRpLv2x4fPu56H4I7uFs4oqIiKizYqihVtPU65CwLRc36rSIDHDFH0f1NnVJRETUiTHUUKu9ln4aRwsr4Wxvi9cmD+Hj20REZFIMNdQqWWfLsHbfWQDAv34fAi9nPr5NRESmxVBDRquo0SAx7QiEAKYM98FDwd1NXRIRERH4mAq1WL1Wh6+OFiHl+7MoVtXC302Jlx4ZYOqyiIiIADDUUAvUaOrx4aGLeDfzHC5duwEAcJTLsHrKUCjl/CtERETmgT+RqFll1WqkZp1H6v4LqKipAwC4Ku0wI7IX4kf6wUVpZ+IKiYiI/qdV99SkpKTA398fCoUCoaGhyMjIaLZvZmYmoqKi4OrqCnt7ewQFBeH111836PPOO+9g1KhRcHFxgYuLC8aOHYuDBw8a9FmyZAkkEonBx8uLr+JvD+fLruOFz44iavl3WP3dGVTU1MHPtQuSJgbjh4UP4JkxfRloiIjI7Bg9U5OWloaEhASkpKQgKioKa9euRUxMDE6cOAFfX99G/ZVKJRYsWIBBgwZBqVQiMzMTc+fOhVKpxJw5cwAAe/bsQVxcHCIjI6FQKLBixQpER0fj+PHj6NGjh/5YAwcOxO7du/XfpVJpa8ZMzThysQJr953F18eKIURD2+Cezpg3OgDRA70g5SPbRERkxiRC3Pzx1TLh4eEYNmwY1qxZo2/r378/Jk6ciOTk5BYdY9KkSVAqldi8eXOT27VaLVxcXPDmm29i+vTpABpmarZv347c3FxjyjWgUqng7OyMyspKODk5tfo41kQIgT2nrmDtvrPYn39V335/P3fMHR2AcP9ukEgYZoiIyHRa+vPbqJkajUaD7OxsLFy40KA9OjoaWVlZLTpGTk4OsrKykJSU1Gyfmpoa1NXVoVu3bgbteXl58Pb2hlwuR3h4OF555RX07s232LaGpl6HL49cxrp9+ThVUgUAkNlIMGFID8y5tzf6eTmauEIiIiLjGBVqysrKoNVq4enpadDu6emJ4uLi2+7bs2dPXLlyBfX19ViyZAlmz57dbN+FCxeiR48eGDt2rL4tPDwcqampCAwMRElJCZKSkhAZGYnjx4/D1dW1yeOo1Wqo1Wr9d5VK1ZJhWrWq2jpsO3gR7/1wDkWVtQAApZ0UU8N98WSUP7y72pu4QiIiotZp1dNPt16OEELc8RJFRkYGqqursX//fixcuBB9+vRBXFxco34rVqzA1q1bsWfPHigU/3tLbUxMjP7XISEhiIiIQEBAADZt2oTExMQmz5mcnIylS5caMzSrVaqqxYas89iy/wKqausBAO6OcsyK8sfUcF8429uauEIiIqK7Y1SocXNzg1QqbTQrU1pa2mj25lb+/v4AGgJJSUkJlixZ0ijUrFy5Eq+88gp2796NQYMG3fZ4SqUSISEhyMvLa7bPokWLDAKPSqWCj4/PbY9rbc6UVuOdffn4LKcQGq0OABDgrsSce3tj4tAekMt4szUREVkHo0KNnZ0dQkNDkZ6ejscee0zfnp6ejgkTJrT4OEIIg8tCAPDqq68iKSkJO3fuRFhY2B2PoVarcfLkSYwaNarZPnK5HHK5vMV1WZPD569i7b58pJ8o0beF+blg7ugAjAny4OKTRERkdYy+/JSYmIj4+HiEhYUhIiIC69atQ0FBAebNmwegYXaksLAQqampAIC33noLvr6+CAoKAtDw3pqVK1fi6aef1h9zxYoVeOmll/DBBx+gV69e+pkgBwcHODg4AACee+45jB8/Hr6+vigtLUVSUhJUKhVmzJhxd78DVkSnE9h9sgRr9+Uj+8I1fXv0AE/MHd0boX7dbrM3ERGRZTM61MTGxqK8vBzLli1DUVERgoODsWPHDvj5+QEAioqKUFBQoO+v0+mwaNEinDt3DjKZDAEBAVi+fDnmzp2r75OSkgKNRoPHH3/c4FyLFy/GkiVLAACXLl1CXFwcysrK4O7ujpEjR2L//v3683Zm6nottucUYu2+fORfuQ4AsJPaYNKwHpg9qjf6eDiYuEIiIqL2Z/R7aiyZtb2npqq2Dpv3X8CGH87jSlXD5TxHhQzxI/0wM7IXPJwUdzgCERGR+WuX99SQ+RBCYOaGQ/rLTN2dFXjqHn9MGeELBy4ySUREnRB/+lmob0+WIvvCNXSxkyJpYjAeGeQNO1mrlvIiIiKyCgw1FkgIgTe+bXiUfXpEL0wa1tPEFREREZke/9PeAn1/qhRHCythbyvFH0f5m7ocIiIis8BQY2GEEHhj981ZGj+4OnTO9/AQERHdiqHGwuw5fQVHLlVCYWuDP97LxTyJiIhuYqixIL+dpZkW7gc3ztIQERHpMdRYkIy8MuRerIBcZoM5ozlLQ0RE9FsMNRbit088/SHcDx6OfLEeERHRbzHUWIgfzpQj+8I1yGU2mMdZGiIiokYYaixAwyzNaQBA3AhfLn9ARETUBIYaC/Dj2XIcOn8NdjIb/Om+AFOXQ0REZJYYaizAql/vpYkb7gNPztIQERE1iaHGzO3PL8fBc1dhJ7XBPM7SEBERNYuhxszdfC/N5OE90d3Z3sTVEBERmS+GGjN28NxV/JhfDlupBH+6r4+pyyEiIjJrDDVm7OYTT0+E+aBHV87SEBER3Q5DjZk6fP4qfjjTMEvzZ95LQ0REdEcMNWbq5tuDHw/tiZ4uXUxcDRERkfljqDFD2ReuISOvDDIbCf7Me2mIiIhahKHGDN2cpZk0rAd8unGWhoiIqCUYasxMTsE17Dt9BVIbCRbc39fU5RAREVkMhhozs/rXWZrHhvaArytnaYiIiFqKocaMHLlYge9P3Zyl4b00RERExmCoMSM3Z2kmDPFGLzeliashIiKyLAw1ZuLopUp8+0spbCTA0w/wXhoiIiJjMdSYiTf0szQ94M9ZGiIiIqMx1JiBY4WV2H2yBBIJMJ/30hAREbUKQ40Z+M93DbM04wd5o4+Hg4mrISIiskwMNSZ2skiFnccbZmmeGcNZGiIiotZiqDGxm088jQvpjj4ejiauhoiIyHK1KtSkpKTA398fCoUCoaGhyMjIaLZvZmYmoqKi4OrqCnt7ewQFBeH1119v1O+TTz7BgAEDIJfLMWDAAHz22Wd3dV5L8EuxCl8fK/51loZPPBEREd0No0NNWloaEhIS8MILLyAnJwejRo1CTEwMCgoKmuyvVCqxYMEC7Nu3DydPnsSLL76IF198EevWrdP3+fHHHxEbG4v4+HgcOXIE8fHxmDx5Mg4cONDq81qC/3x7BgDwcHB3BHpyloaIiOhuSIQQwpgdwsPDMWzYMKxZs0bf1r9/f0ycOBHJycktOsakSZOgVCqxefNmAEBsbCxUKhW+/vprfZ+HHnoILi4u2Lp1a5udV6VSwdnZGZWVlXBycmrRPu3ldEkVHly1D0IA3ySMQpCXaeshIiIyVy39+W3UTI1Go0F2djaio6MN2qOjo5GVldWiY+Tk5CArKwujR4/Wt/3444+Njvnggw/qj9na86rVaqhUKoOPuVj9bR6EAB4a6MVAQ0RE1AaMCjVlZWXQarXw9PQ0aPf09ERxcfFt9+3ZsyfkcjnCwsIwf/58zJ49W7+tuLj4tsds7XmTk5Ph7Oys//j4+LRonO3tTGkVvjpaBID30hAREbWVVt0oLJFIDL4LIRq13SojIwOHDx/G22+/jVWrVukvKxlzTGPPu2jRIlRWVuo/Fy9evG2NHeU/352BEED0AE8M8OYsDRERUVuQGdPZzc0NUqm00exIaWlpo1mUW/n7+wMAQkJCUFJSgiVLliAuLg4A4OXlddtjtva8crkccrm8ZYPrIGevVOPLI5cBcJaGiIioLRk1U2NnZ4fQ0FCkp6cbtKenpyMyMrLFxxFCQK1W679HREQ0OuauXbv0x2yr85qDN787A50Axvb3RHAPZ1OXQ0REZDWMmqkBgMTERMTHxyMsLAwRERFYt24dCgoKMG/ePAANl3wKCwuRmpoKAHjrrbfg6+uLoKAgAA3vrVm5ciWefvpp/TGfffZZ3HvvvfjXv/6FCRMm4PPPP8fu3buRmZnZ4vNagvwr1fg8txAA8CxnaYiIiNqU0aEmNjYW5eXlWLZsGYqKihAcHIwdO3bAz88PAFBUVGTw7hidTodFixbh3LlzkMlkCAgIwPLlyzF37lx9n8jISGzbtg0vvvgiXnrpJQQEBCAtLQ3h4eEtPq8lePP7hlmaMUEeCOnJWRoiIqK2ZPR7aiyZKd9Tc77sOsa8thdancDn86Mw2Kdrh56fiIjIUrXLe2qo9d78/gy0OoH7+rkz0BAREbUDhpoOUFBeg89yeC8NERFRe2Ko6QBv/TpLc2+gO4b6upi6HCIiIqvEUNPOLl6twSc/XQLAWRoiIqL2xFDTzlL2nEG9TmBUXzeE+nGWhoiIqL0w1LSjS9dq8NFhztIQERF1BIaadpSy5yzqdQKRAa4I69XN1OUQERFZNYaadlJYcQMfHW5YQJOzNERERO2PoaadvL3nLOq0AiN7d0N4b1dTl0NERGT1GGraQVHlDaQdujlLE2jiaoiIiDoHhpp28Paes9BodRjh3w0RAZylISIi6ggMNW2sRFWLrb/O0iTwXhoiIqIOw1DTxtbsOQtNvQ7De7lwloaIiKgDMdS0oVJVLbYeLAAAPDOmLyQSiYkrIiIi6jwYatrQ23vzoa7XYZhvV9zTx83U5RAREXUqDDVtpLSqFu8fuAAAeHZsIGdpiIiIOhhDTRt5Z1/DLM0Qn664ty9naYiIiDoaQ00bKKtWY/P+m7M0vJeGiIjIFBhq2sA7+/JRW6fD4J7OuC/Q3dTlEBERdUoMNXfp6nUNUn/kLA0REZGpyUxdgKXram+L1yYPRvqJEtzfz8PU5RAREXVaDDV3ycZGgpiQ7ogJ6W7qUoiIiDo1Xn4iIiIiq8BQQ0RERFaBoYaIiIisAkMNERERWQWGGiIiIrIKDDVERERkFRhqiIiIyCow1BAREZFVYKghIiIiq9CqUJOSkgJ/f38oFAqEhoYiIyOj2b6ffvopfve738Hd3R1OTk6IiIjAzp07Dfrcd999kEgkjT7jxo3T91myZEmj7V5eXq0pn4iIiKyQ0aEmLS0NCQkJeOGFF5CTk4NRo0YhJiYGBQUFTfbft28ffve732HHjh3Izs7G/fffj/HjxyMnJ0ff59NPP0VRUZH+c+zYMUilUjzxxBMGxxo4cKBBv6NHjxpbPhEREVkpiRBCGLNDeHg4hg0bhjVr1ujb+vfvj4kTJyI5OblFxxg4cCBiY2Px8ssvN7l91apVePnll1FUVASlUgmgYaZm+/btyM3NNaZcAyqVCs7OzqisrISTk1Orj0NEREQdp6U/v42aqdFoNMjOzkZ0dLRBe3R0NLKyslp0DJ1Oh6qqKnTr1q3ZPuvXr8eUKVP0geamvLw8eHt7w9/fH1OmTEF+fv5tz6VWq6FSqQw+REREZJ2MWqW7rKwMWq0Wnp6eBu2enp4oLi5u0TH+/e9/4/r165g8eXKT2w8ePIhjx45h/fr1Bu3h4eFITU1FYGAgSkpKkJSUhMjISBw/fhyurq5NHis5ORlLly5t1M5wQ0REZDlu/ty+48UlYYTCwkIBQGRlZRm0JyUliX79+t1x/w8++EB06dJFpKenN9tnzpw5Ijg4+I7Hqq6uFp6enuLf//53s31qa2tFZWWl/nPixAkBgB9++OGHH374scDPxYsXb5sNjJqpcXNzg1QqbTQrU1pa2mj25lZpaWl46qmn8NFHH2Hs2LFN9qmpqcG2bduwbNmyO9aiVCoREhKCvLy8ZvvI5XLI5XL9dwcHB1y8eBGOjo6QSCR3PEdLqVQq+Pj44OLFi53iXh2O1/p1tjFzvNaN47V8QghUVVXB29v7tv2MCjV2dnYIDQ1Feno6HnvsMX17eno6JkyY0Ox+W7duxaxZs7B161aDx7Rv9eGHH0KtVmPatGl3rEWtVuPkyZMYNWpUi+u3sbFBz549W9zfWE5OTlbzF6glOF7r19nGzPFaN47Xsjk7O9+xj1GhBgASExMRHx+PsLAwREREYN26dSgoKMC8efMAAIsWLUJhYSFSU1MBNASa6dOn44033sDIkSP1szz29vaNCly/fj0mTpzY5D0yzz33HMaPHw9fX1+UlpYiKSkJKpUKM2bMMHYIREREZIWMDjWxsbEoLy/HsmXLUFRUhODgYOzYsQN+fn4AgKKiIoN31qxduxb19fWYP38+5s+fr2+fMWMGNm7cqP9++vRpZGZmYteuXU2e99KlS4iLi0NZWRnc3d0xcuRI7N+/X39eIiIi6tyMDjUA8Oc//xl//vOfm9z226ACAHv27GnRMQMDA297V/O2bdtaWl6Hk8vlWLx4scH9O9aM47V+nW3MHK9143g7D6NfvkdERERkjrigJREREVkFhhoiIiKyCgw1REREZBUYaoiIiMgqMNQ0Izk5GcOHD4ejoyM8PDwwceJEnDp1yqCPEAJLliyBt7c37O3tcd999+H48eMGfdRqNZ5++mm4ublBqVTi0UcfxaVLlzpyKC2yZs0aDBo0SP+ypoiICHz99df67dY01qYkJydDIpEgISFB32ZNY16yZAkkEonBx8vLS7/dmsZ6U2FhIaZNmwZXV1d06dIFQ4YMQXZ2tn67tY25V69ejf6MJRKJ/lUa1jbe+vp6vPjii/D394e9vT169+6NZcuWQafT6ftY25irqqqQkJAAPz8/2NvbIzIyEocOHdJvt7bxtsodF1nqpB588EGxYcMGcezYMZGbmyvGjRsnfH19RXV1tb7P8uXLhaOjo/jkk0/E0aNHRWxsrOjevbtQqVT6PvPmzRM9evQQ6enp4qeffhL333+/GDx4sKivrzfFsJr1xRdfiK+++kqcOnVKnDp1Svz9738Xtra24tixY0II6xrrrQ4ePCh69eolBg0aJJ599ll9uzWNefHixWLgwIGiqKhI/yktLdVvt6axCiHE1atXhZ+fn5g5c6Y4cOCAOHfunNi9e7c4c+aMvo+1jbm0tNTgzzc9PV0AEN9//70QwvrGm5SUJFxdXcV///tfce7cOfHRRx8JBwcHsWrVKn0faxvz5MmTxYABA8TevXtFXl6eWLx4sXBychKXLl0SQljfeFuDoaaFSktLBQCxd+9eIYQQOp1OeHl5ieXLl+v71NbWCmdnZ/H2228LIYSoqKgQtra2Ytu2bfo+hYWFwsbGRnzzzTcdO4BWcHFxEe+++65Vj7Wqqkr07dtXpKeni9GjR+tDjbWNefHixWLw4MFNbrO2sQohxPPPPy/uueeeZrdb45hv9eyzz4qAgACh0+mscrzjxo0Ts2bNMmibNGmSmDZtmhDC+v6Ma2pqhFQqFf/9738N2gcPHixeeOEFqxtva/HyUwtVVlYCALp16wYAOHfuHIqLixEdHa3vI5fLMXr0aGRlZQEAsrOzUVdXZ9DH29sbwcHB+j7mSKvVYtu2bbh+/ToiIiKseqzz58/HuHHjGi2yao1jzsvLg7e3N/z9/TFlyhTk5+cDsM6xfvHFFwgLC8MTTzwBDw8PDB06FO+8845+uzWO+bc0Gg22bNmCWbNmQSKRWOV477nnHnz77bc4ffo0AODIkSPIzMzEww8/DMD6/ozr6+uh1WqhUCgM2u3t7ZGZmWl1420thpoWEEIgMTER99xzD4KDgwFAv4bVrauTe3p66rcVFxfDzs4OLi4uzfYxJ0ePHoWDgwPkcjnmzZuHzz77DAMGDLDKsQINb6nOzs5GcnJyo23WNubw8HCkpqZi586deOedd1BcXIzIyEiUl5db3VgBID8/H2vWrEHfvn2xc+dOzJs3D88884x+TTprHPNvbd++HRUVFZg5cyYA6xzv888/j7i4OAQFBcHW1hZDhw5FQkIC4uLiAFjfmB0dHREREYF//OMfuHz5MrRaLbZs2YIDBw6gqKjI6sbbWq1aJqGzWbBgAX7++WdkZmY22iaRSAy+CyEatd2qJX1MoV+/fsjNzUVFRQU++eQTzJgxA3v37tVvt6axXrx4Ec8++yx27drV6L98fstaxhwTE6P/dUhICCIiIhAQEIBNmzZh5MiRAKxnrACg0+kQFhaGV155BQAwdOhQHD9+HGvWrMH06dP1/axpzL+1fv16xMTEwNvb26DdmsablpaGLVu24IMPPsDAgQORm5uLhIQEeHt7Gyx0bE1j3rx5M2bNmoUePXpAKpVi2LBhmDp1Kn766Sd9H2sab2twpuYOnn76aXzxxRf4/vvv0bNnT337zSdHbk23paWl+qTs5eUFjUaDa9euNdvHnNjZ2aFPnz4ICwtDcnIyBg8ejDfeeMMqx5qdnY3S0lKEhoZCJpNBJpNh7969WL16NWQymb5maxrzbymVSoSEhCAvL88q/3y7d++OAQMGGLT1799fv9iuNY75pgsXLmD37t2YPXu2vs0ax/vXv/4VCxcuxJQpUxASEoL4+Hj85S9/0c+8WuOYAwICsHfvXlRXV+PixYs4ePAg6urq4O/vb5XjbQ2GmmYIIbBgwQJ8+umn+O677+Dv72+w/eZfovT0dH2bRqPB3r17ERkZCQAIDQ2Fra2tQZ+ioiIcO3ZM38ecCSGgVqutcqxjxozB0aNHkZubq/+EhYXhD3/4A3Jzc9G7d2+rG/NvqdVqnDx5Et27d7fKP9+oqKhGr2A4ffo0/Pz8AFj3/383bNgADw8PjBs3Tt9mjeOtqamBjY3hjzCpVKp/pNsax3yTUqlE9+7dce3aNezcuRMTJkyw6vEapYNvTLYYf/rTn4Szs7PYs2ePwWOSNTU1+j7Lly8Xzs7O4tNPPxVHjx4VcXFxTT4+17NnT7F7927x008/iQceeMAsH59btGiR2Ldvnzh37pz4+eefxd///ndhY2Mjdu3aJYSwrrE257dPPwlhXWP+v//7P7Fnzx6Rn58v9u/fLx555BHh6Ogozp8/L4SwrrEK0fCYvkwmE//85z9FXl6eeP/990WXLl3Eli1b9H2sbcxCCKHVaoWvr694/vnnG22ztvHOmDFD9OjRQ/9I96effirc3NzE3/72N30faxvzN998I77++muRn58vdu3aJQYPHixGjBghNBqNEML6xtsaDDXNANDkZ8OGDfo+Op1OLF68WHh5eQm5XC7uvfdecfToUYPj3LhxQyxYsEB069ZN2Nvbi0ceeUQUFBR08GjubNasWcLPz0/Y2dkJd3d3MWbMGH2gEcK6xtqcW0ONNY355vsqbG1thbe3t5g0aZI4fvy4frs1jfWmL7/8UgQHBwu5XC6CgoLEunXrDLZb45h37twpAIhTp0412mZt41WpVOLZZ58Vvr6+QqFQiN69e4sXXnhBqNVqfR9rG3NaWpro3bu3sLOzE15eXmL+/PmioqJCv93axtsaEiGEMOFEEREREVGb4D01REREZBUYaoiIiMgqMNQQERGRVWCoISIiIqvAUENERERWgaGGiIiIrAJDDREREVkFhhoiIiKyCgw1REREZBUYaoiIiMgqMNQQERGRVWCoISIiIqvw/yVkc8643l4KAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(list(range(200, 1000, 50)), scores)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Kackie\\anaconda3\\envs\\DataScience\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1044: UserWarning: MiniBatchKMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can prevent it by setting batch_size >= 4096 or by setting the environment variable OMP_NUM_THREADS=2\n",
      "  \"MiniBatchKMeans is known to have a memory leak on \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For n_clusters = 1000\n",
      "Silhouette coefficient: 0.48\n",
      "Inertia:8343.486808075375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Kackie\\anaconda3\\envs\\DataScience\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1044: UserWarning: MiniBatchKMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can prevent it by setting batch_size >= 4096 or by setting the environment variable OMP_NUM_THREADS=2\n",
      "  \"MiniBatchKMeans is known to have a memory leak on \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For n_clusters = 1100\n",
      "Silhouette coefficient: 0.50\n",
      "Inertia:7777.411516201758\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Kackie\\anaconda3\\envs\\DataScience\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1044: UserWarning: MiniBatchKMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can prevent it by setting batch_size >= 4096 or by setting the environment variable OMP_NUM_THREADS=2\n",
      "  \"MiniBatchKMeans is known to have a memory leak on \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For n_clusters = 1200\n",
      "Silhouette coefficient: 0.51\n",
      "Inertia:7761.139395784848\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Kackie\\anaconda3\\envs\\DataScience\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1044: UserWarning: MiniBatchKMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can prevent it by setting batch_size >= 4096 or by setting the environment variable OMP_NUM_THREADS=2\n",
      "  \"MiniBatchKMeans is known to have a memory leak on \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For n_clusters = 1300\n",
      "Silhouette coefficient: 0.52\n",
      "Inertia:7373.042643033305\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Kackie\\anaconda3\\envs\\DataScience\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1044: UserWarning: MiniBatchKMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can prevent it by setting batch_size >= 4096 or by setting the environment variable OMP_NUM_THREADS=2\n",
      "  \"MiniBatchKMeans is known to have a memory leak on \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For n_clusters = 1400\n",
      "Silhouette coefficient: 0.53\n",
      "Inertia:6887.373218033455\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Kackie\\anaconda3\\envs\\DataScience\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1044: UserWarning: MiniBatchKMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can prevent it by setting batch_size >= 4096 or by setting the environment variable OMP_NUM_THREADS=2\n",
      "  \"MiniBatchKMeans is known to have a memory leak on \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For n_clusters = 1500\n",
      "Silhouette coefficient: 0.54\n",
      "Inertia:6830.686332492045\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Kackie\\anaconda3\\envs\\DataScience\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1044: UserWarning: MiniBatchKMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can prevent it by setting batch_size >= 4096 or by setting the environment variable OMP_NUM_THREADS=2\n",
      "  \"MiniBatchKMeans is known to have a memory leak on \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For n_clusters = 1600\n",
      "Silhouette coefficient: 0.55\n",
      "Inertia:6390.16845304599\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Kackie\\anaconda3\\envs\\DataScience\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1044: UserWarning: MiniBatchKMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can prevent it by setting batch_size >= 4096 or by setting the environment variable OMP_NUM_THREADS=2\n",
      "  \"MiniBatchKMeans is known to have a memory leak on \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For n_clusters = 1700\n",
      "Silhouette coefficient: 0.55\n",
      "Inertia:6204.529100369795\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Kackie\\anaconda3\\envs\\DataScience\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1044: UserWarning: MiniBatchKMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can prevent it by setting batch_size >= 4096 or by setting the environment variable OMP_NUM_THREADS=2\n",
      "  \"MiniBatchKMeans is known to have a memory leak on \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For n_clusters = 1800\n",
      "Silhouette coefficient: 0.56\n",
      "Inertia:5924.020241958763\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Kackie\\anaconda3\\envs\\DataScience\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1044: UserWarning: MiniBatchKMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can prevent it by setting batch_size >= 4096 or by setting the environment variable OMP_NUM_THREADS=2\n",
      "  \"MiniBatchKMeans is known to have a memory leak on \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For n_clusters = 1900\n",
      "Silhouette coefficient: 0.54\n",
      "Inertia:7176.675247781882\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Kackie\\anaconda3\\envs\\DataScience\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1044: UserWarning: MiniBatchKMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can prevent it by setting batch_size >= 4096 or by setting the environment variable OMP_NUM_THREADS=2\n",
      "  \"MiniBatchKMeans is known to have a memory leak on \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For n_clusters = 2000\n",
      "Silhouette coefficient: 0.57\n",
      "Inertia:5662.494243199713\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Kackie\\anaconda3\\envs\\DataScience\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1044: UserWarning: MiniBatchKMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can prevent it by setting batch_size >= 4096 or by setting the environment variable OMP_NUM_THREADS=2\n",
      "  \"MiniBatchKMeans is known to have a memory leak on \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For n_clusters = 2100\n",
      "Silhouette coefficient: 0.58\n",
      "Inertia:5163.984166894702\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Kackie\\anaconda3\\envs\\DataScience\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1044: UserWarning: MiniBatchKMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can prevent it by setting batch_size >= 4096 or by setting the environment variable OMP_NUM_THREADS=2\n",
      "  \"MiniBatchKMeans is known to have a memory leak on \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For n_clusters = 2200\n",
      "Silhouette coefficient: 0.56\n",
      "Inertia:6697.64243208389\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Kackie\\anaconda3\\envs\\DataScience\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1044: UserWarning: MiniBatchKMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can prevent it by setting batch_size >= 4096 or by setting the environment variable OMP_NUM_THREADS=2\n",
      "  \"MiniBatchKMeans is known to have a memory leak on \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For n_clusters = 2300\n",
      "Silhouette coefficient: 0.56\n",
      "Inertia:6559.62461370682\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Kackie\\anaconda3\\envs\\DataScience\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1044: UserWarning: MiniBatchKMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can prevent it by setting batch_size >= 4096 or by setting the environment variable OMP_NUM_THREADS=2\n",
      "  \"MiniBatchKMeans is known to have a memory leak on \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For n_clusters = 2400\n",
      "Silhouette coefficient: 0.57\n",
      "Inertia:6174.540756537664\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Kackie\\anaconda3\\envs\\DataScience\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1044: UserWarning: MiniBatchKMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can prevent it by setting batch_size >= 4096 or by setting the environment variable OMP_NUM_THREADS=2\n",
      "  \"MiniBatchKMeans is known to have a memory leak on \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For n_clusters = 2500\n",
      "Silhouette coefficient: 0.57\n",
      "Inertia:6413.947063407154\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Kackie\\anaconda3\\envs\\DataScience\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1044: UserWarning: MiniBatchKMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can prevent it by setting batch_size >= 4096 or by setting the environment variable OMP_NUM_THREADS=2\n",
      "  \"MiniBatchKMeans is known to have a memory leak on \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For n_clusters = 2600\n",
      "Silhouette coefficient: 0.58\n",
      "Inertia:6034.977155981688\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Kackie\\anaconda3\\envs\\DataScience\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1044: UserWarning: MiniBatchKMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can prevent it by setting batch_size >= 4096 or by setting the environment variable OMP_NUM_THREADS=2\n",
      "  \"MiniBatchKMeans is known to have a memory leak on \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For n_clusters = 2700\n",
      "Silhouette coefficient: 0.62\n",
      "Inertia:4029.9464895121487\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Kackie\\anaconda3\\envs\\DataScience\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1044: UserWarning: MiniBatchKMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can prevent it by setting batch_size >= 4096 or by setting the environment variable OMP_NUM_THREADS=2\n",
      "  \"MiniBatchKMeans is known to have a memory leak on \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For n_clusters = 2800\n",
      "Silhouette coefficient: 0.60\n",
      "Inertia:5404.028642280558\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Kackie\\anaconda3\\envs\\DataScience\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1044: UserWarning: MiniBatchKMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can prevent it by setting batch_size >= 4096 or by setting the environment variable OMP_NUM_THREADS=2\n",
      "  \"MiniBatchKMeans is known to have a memory leak on \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For n_clusters = 2900\n",
      "Silhouette coefficient: 0.59\n",
      "Inertia:5649.454779404123\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Kackie\\anaconda3\\envs\\DataScience\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1044: UserWarning: MiniBatchKMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can prevent it by setting batch_size >= 4096 or by setting the environment variable OMP_NUM_THREADS=2\n",
      "  \"MiniBatchKMeans is known to have a memory leak on \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For n_clusters = 3000\n",
      "Silhouette coefficient: 0.62\n",
      "Inertia:4632.43962957574\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Kackie\\anaconda3\\envs\\DataScience\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1044: UserWarning: MiniBatchKMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can prevent it by setting batch_size >= 4096 or by setting the environment variable OMP_NUM_THREADS=2\n",
      "  \"MiniBatchKMeans is known to have a memory leak on \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For n_clusters = 3100\n",
      "Silhouette coefficient: 0.62\n",
      "Inertia:4414.659408898083\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Kackie\\anaconda3\\envs\\DataScience\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1044: UserWarning: MiniBatchKMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can prevent it by setting batch_size >= 4096 or by setting the environment variable OMP_NUM_THREADS=2\n",
      "  \"MiniBatchKMeans is known to have a memory leak on \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For n_clusters = 3200\n",
      "Silhouette coefficient: 0.64\n",
      "Inertia:3459.049868439754\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Kackie\\anaconda3\\envs\\DataScience\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1044: UserWarning: MiniBatchKMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can prevent it by setting batch_size >= 4096 or by setting the environment variable OMP_NUM_THREADS=2\n",
      "  \"MiniBatchKMeans is known to have a memory leak on \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For n_clusters = 3300\n",
      "Silhouette coefficient: 0.61\n",
      "Inertia:4963.3186509241505\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Kackie\\anaconda3\\envs\\DataScience\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1044: UserWarning: MiniBatchKMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can prevent it by setting batch_size >= 4096 or by setting the environment variable OMP_NUM_THREADS=2\n",
      "  \"MiniBatchKMeans is known to have a memory leak on \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For n_clusters = 3400\n",
      "Silhouette coefficient: 0.61\n",
      "Inertia:4765.006750420458\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Kackie\\anaconda3\\envs\\DataScience\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1044: UserWarning: MiniBatchKMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can prevent it by setting batch_size >= 4096 or by setting the environment variable OMP_NUM_THREADS=2\n",
      "  \"MiniBatchKMeans is known to have a memory leak on \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For n_clusters = 3500\n",
      "Silhouette coefficient: 0.64\n",
      "Inertia:3778.5435306793734\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Kackie\\anaconda3\\envs\\DataScience\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1044: UserWarning: MiniBatchKMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can prevent it by setting batch_size >= 4096 or by setting the environment variable OMP_NUM_THREADS=2\n",
      "  \"MiniBatchKMeans is known to have a memory leak on \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For n_clusters = 3600\n",
      "Silhouette coefficient: 0.63\n",
      "Inertia:4087.1862214795615\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Kackie\\anaconda3\\envs\\DataScience\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1044: UserWarning: MiniBatchKMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can prevent it by setting batch_size >= 4096 or by setting the environment variable OMP_NUM_THREADS=2\n",
      "  \"MiniBatchKMeans is known to have a memory leak on \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For n_clusters = 3700\n",
      "Silhouette coefficient: 0.63\n",
      "Inertia:4227.417484854356\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Kackie\\anaconda3\\envs\\DataScience\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1044: UserWarning: MiniBatchKMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can prevent it by setting batch_size >= 4096 or by setting the environment variable OMP_NUM_THREADS=2\n",
      "  \"MiniBatchKMeans is known to have a memory leak on \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For n_clusters = 3800\n",
      "Silhouette coefficient: 0.64\n",
      "Inertia:3879.8698453737034\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Kackie\\anaconda3\\envs\\DataScience\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1044: UserWarning: MiniBatchKMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can prevent it by setting batch_size >= 4096 or by setting the environment variable OMP_NUM_THREADS=2\n",
      "  \"MiniBatchKMeans is known to have a memory leak on \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For n_clusters = 3900\n",
      "Silhouette coefficient: 0.65\n",
      "Inertia:3409.080062117052\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Kackie\\anaconda3\\envs\\DataScience\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1044: UserWarning: MiniBatchKMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can prevent it by setting batch_size >= 4096 or by setting the environment variable OMP_NUM_THREADS=2\n",
      "  \"MiniBatchKMeans is known to have a memory leak on \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For n_clusters = 4000\n",
      "Silhouette coefficient: 0.65\n",
      "Inertia:3619.717380637986\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Kackie\\anaconda3\\envs\\DataScience\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1044: UserWarning: MiniBatchKMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can prevent it by setting batch_size >= 4096 or by setting the environment variable OMP_NUM_THREADS=2\n",
      "  \"MiniBatchKMeans is known to have a memory leak on \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For n_clusters = 4100\n",
      "Silhouette coefficient: 0.64\n",
      "Inertia:3611.674678347521\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Kackie\\anaconda3\\envs\\DataScience\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1044: UserWarning: MiniBatchKMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can prevent it by setting batch_size >= 4096 or by setting the environment variable OMP_NUM_THREADS=2\n",
      "  \"MiniBatchKMeans is known to have a memory leak on \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For n_clusters = 4200\n",
      "Silhouette coefficient: 0.64\n",
      "Inertia:3826.136585115016\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Kackie\\anaconda3\\envs\\DataScience\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1044: UserWarning: MiniBatchKMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can prevent it by setting batch_size >= 4096 or by setting the environment variable OMP_NUM_THREADS=2\n",
      "  \"MiniBatchKMeans is known to have a memory leak on \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For n_clusters = 4300\n",
      "Silhouette coefficient: 0.66\n",
      "Inertia:3493.9657346415693\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Kackie\\anaconda3\\envs\\DataScience\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1044: UserWarning: MiniBatchKMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can prevent it by setting batch_size >= 4096 or by setting the environment variable OMP_NUM_THREADS=2\n",
      "  \"MiniBatchKMeans is known to have a memory leak on \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For n_clusters = 4400\n",
      "Silhouette coefficient: 0.67\n",
      "Inertia:2747.3641925810257\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Kackie\\anaconda3\\envs\\DataScience\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1044: UserWarning: MiniBatchKMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can prevent it by setting batch_size >= 4096 or by setting the environment variable OMP_NUM_THREADS=2\n",
      "  \"MiniBatchKMeans is known to have a memory leak on \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For n_clusters = 4500\n",
      "Silhouette coefficient: 0.67\n",
      "Inertia:2774.458000238058\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Kackie\\anaconda3\\envs\\DataScience\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1044: UserWarning: MiniBatchKMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can prevent it by setting batch_size >= 4096 or by setting the environment variable OMP_NUM_THREADS=2\n",
      "  \"MiniBatchKMeans is known to have a memory leak on \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For n_clusters = 4600\n",
      "Silhouette coefficient: 0.65\n",
      "Inertia:3496.3123415478744\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Kackie\\anaconda3\\envs\\DataScience\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1044: UserWarning: MiniBatchKMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can prevent it by setting batch_size >= 4096 or by setting the environment variable OMP_NUM_THREADS=2\n",
      "  \"MiniBatchKMeans is known to have a memory leak on \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For n_clusters = 4700\n",
      "Silhouette coefficient: 0.67\n",
      "Inertia:2763.1063398242272\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Kackie\\anaconda3\\envs\\DataScience\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1044: UserWarning: MiniBatchKMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can prevent it by setting batch_size >= 4096 or by setting the environment variable OMP_NUM_THREADS=2\n",
      "  \"MiniBatchKMeans is known to have a memory leak on \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For n_clusters = 4800\n",
      "Silhouette coefficient: 0.68\n",
      "Inertia:2302.41215587872\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Kackie\\anaconda3\\envs\\DataScience\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1044: UserWarning: MiniBatchKMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can prevent it by setting batch_size >= 4096 or by setting the environment variable OMP_NUM_THREADS=2\n",
      "  \"MiniBatchKMeans is known to have a memory leak on \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For n_clusters = 4900\n",
      "Silhouette coefficient: 0.69\n",
      "Inertia:2294.6910943352823\n"
     ]
    }
   ],
   "source": [
    "# find the best number of clusters by silhouette score\n",
    "scores = []\n",
    "for i in range(1000, 5000, 100):\n",
    "    _, _, s_score = mbkmeans_clusters(\n",
    "        X=vectorized_job_titles,\n",
    "        k=i,\n",
    "        mb=500,\n",
    "        print_silhouette_values=False\n",
    "    )\n",
    "    scores.append(s_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(list(range(1000, 5000, 100)), scores)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the clusters\n",
    "df_clusters = pd.DataFrame({\n",
    "    \"text\": job_titles,\n",
    "    \"tokens\": [\" \".join(text) for text in job_titles_tokenized],\n",
    "    \"cluster\": cluster_labels\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>tokens</th>\n",
       "      <th>cluster</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Chief Financial Officer</td>\n",
       "      <td>chief financial officer</td>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Country Coordinator</td>\n",
       "      <td>country coordinator</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>BCC Specialist</td>\n",
       "      <td>bcc specialist</td>\n",
       "      <td>38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Saleswoman</td>\n",
       "      <td>saleswoman</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Chief Accountant/ Finance Assistant</td>\n",
       "      <td>chief finance assistant</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15085</th>\n",
       "      <td>.NET Developer</td>\n",
       "      <td>developer</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15086</th>\n",
       "      <td>Deputy Director</td>\n",
       "      <td>deputy director</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15087</th>\n",
       "      <td>Senior Creative UX/ UI Designer</td>\n",
       "      <td>senior creative ui designer</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15088</th>\n",
       "      <td>Head of Online Sales Department</td>\n",
       "      <td>head online sale department</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15089</th>\n",
       "      <td>Lawyer in Legal Department</td>\n",
       "      <td>lawyer legal department</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>15090 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      text                       tokens  \\\n",
       "0                  Chief Financial Officer      chief financial officer   \n",
       "1                      Country Coordinator          country coordinator   \n",
       "2                           BCC Specialist               bcc specialist   \n",
       "3                               Saleswoman                   saleswoman   \n",
       "4      Chief Accountant/ Finance Assistant      chief finance assistant   \n",
       "...                                    ...                          ...   \n",
       "15085                       .NET Developer                    developer   \n",
       "15086                      Deputy Director              deputy director   \n",
       "15087      Senior Creative UX/ UI Designer  senior creative ui designer   \n",
       "15088      Head of Online Sales Department  head online sale department   \n",
       "15089           Lawyer in Legal Department      lawyer legal department   \n",
       "\n",
       "       cluster  \n",
       "0           34  \n",
       "1           10  \n",
       "2           38  \n",
       "3           11  \n",
       "4           32  \n",
       "...        ...  \n",
       "15085        8  \n",
       "15086       17  \n",
       "15087       18  \n",
       "15088       30  \n",
       "15089       16  \n",
       "\n",
       "[15090 rows x 3 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most representative terms per cluster (based on centroids):\n",
      "Cluster 0: merchandiser merchandising visual_merchandiser merchandisers merchandizing \n",
      "Cluster 1: engineer engineering mechanical_engineer electrical_engineer Mike_Di_Filippo \n",
      "Cluster 2: sale manager buyer sold sell \n",
      "Cluster 3: manager vice_president manger director supervisor \n",
      "Cluster 4: finance treasurer executive vice_president Ellen_Roseman_writes \n",
      "Cluster 5: developer developers Active_Endpoints_www.activevos.com Panda_Software_www.pandasoftware.com Developer \n",
      "Cluster 6: administrative assistant adminstrative Sherri_Sera representative_Laurie_Hamit \n",
      "Cluster 7: journalist writer researcher scientist editor \n",
      "Cluster 8: developer developers Developer Developers Jack_Antaramian \n",
      "Cluster 9: translator interpreter interpretor FOREIGN_MINISTER_Via PRESIDENT_ABBAS_Via \n",
      "Cluster 10: administrator coordinator director secretary adminstrator \n",
      "Cluster 11: receptionist waitress salesperson cashier saleswoman \n",
      "Cluster 12: java developer developers Panda_Software_www.pandasoftware.com Active_Endpoints_www.activevos.com \n",
      "Cluster 13: medical representative physician doctor pharmacist \n",
      "Cluster 14: cashier teller clerk cashiers tellers \n",
      "Cluster 15: accountant chartered_accountant bookkeeper accountants auditor \n",
      "Cluster 16: specialist consultant Axispoint_specializes LLC_www.equityallianceir.com LISA_MICHALS_covers \n",
      "Cluster 17: director executive vice_president coordinator managing_director \n",
      "Cluster 18: designer designers Designer fashion_designer desinger \n",
      "Cluster 19: developer developers Active_Endpoints_www.activevos.com software Panda_Software_www.pandasoftware.com \n",
      "Cluster 20: engineer electrical_engineer mechanical_engineer engineering Engineer \n",
      "Cluster 21: lawyer attorney lawyers laywer solicitor \n",
      "Cluster 22: manager vice_president manger director mananger \n",
      "Cluster 23: loan credit officer Sandra_Jansky loans \n",
      "Cluster 24: programmer programmers coder Programmer programer \n",
      "Cluster 25: project manager coordinator director supervisor \n",
      "Cluster 26: intern internship unpaid_intern interns interning \n",
      "Cluster 27: manager marketing vice_president vp svp \n",
      "Cluster 28: auditor audit auditors auditing audits \n",
      "Cluster 29: engineer software engineering electrical_engineer programmer \n",
      "Cluster 30: officer supervisor employee MAST_ambulance Randal_Stumpf \n",
      "Cluster 31: developer developers Panda_Software_www.pandasoftware.com Active_Endpoints_www.activevos.com Developer \n",
      "Cluster 32: assistant associate assitant director manager \n",
      "Cluster 33: analyst economist Analyst anlayst strategist \n",
      "Cluster 34: accountant chief officer administrator consultant \n",
      "Cluster 35: Axispoint_specializes Elizabeth_Lechleitner_editorial Sharon_Schmickle_writes By_Jonas_Elmerraji LISA_MICHALS_covers \n",
      "Cluster 36: hr mgr N_TVPG min_##.#sec ##.##/hr \n",
      "Cluster 37: assurance quality engineer PowerMax_Looking SQO \n",
      "Cluster 38: specialist specialists consultant expert consultancy \n",
      "Cluster 39: php mysql developer FFmpeg MySql \n"
     ]
    }
   ],
   "source": [
    "print(\"Most representative terms per cluster (based on centroids):\")\n",
    "for i in range(40):\n",
    "    tokens_per_cluster = \"\"\n",
    "    most_representative = model_w2v.most_similar(positive=[clustering.cluster_centers_[i]], topn=5)\n",
    "    for t in most_representative:\n",
    "        tokens_per_cluster += f\"{t[0]} \"\n",
    "    print(f\"Cluster {i}: {tokens_per_cluster}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merchandiser\n",
      "Reseller/ Merchandiser\n",
      "Merchandiser, Vanadzor\n",
      "Merchandiser, Yerevan\n",
      "Merchandiser\n",
      "Merchandiser\n",
      "Merchandiser\n",
      "Merchandiser\n",
      "Merchandiser\n",
      "Merchandiser\n",
      "----------------------------------------\n",
      "Senior Software QA Engineer\n",
      "Senior Software QA Engineer\n",
      "Senior Software Engineer (EDA)\n",
      "Senior Software Engineer\n",
      "Senior Software Engineer\n",
      "Senior Software Engineer  Java/DBMS\n",
      "Senior Software Engineer\n",
      "Senior Software Engineer\n",
      "Senior Software Engineer\n",
      "Senior Software Engineer\n",
      "----------------------------------------\n",
      "Sales Manager\n",
      "Sales Manager\n",
      "Sales Manager\n",
      "Sales Manager\n",
      "Sales Manager\n",
      "Account/ Sales Manager\n",
      "Sales Manager\n",
      "Sales Manager\n",
      "Sales Manager\n",
      "Sales Manager\n",
      "----------------------------------------\n",
      "BTL Manager\n",
      "Office Manager of the Country Director\n",
      "Business Center Manager\n",
      "Business Center Manager\n",
      "Manager of Commercial Department\n",
      "Manager of Commercial Department\n",
      "Manager of Commercial Department\n",
      "Manager of Commercial Department\n",
      "Business Manager\n",
      "Business Manager\n",
      "----------------------------------------\n",
      "Assistant to the Controller of Finance and Administration (CFA)\n",
      "Head of Finance and Accounting Department/ Chief Accountant\n",
      "Associate Finance and Administration Officer\n",
      "Associate Finance and Administration Officer\n",
      "Finance and Administration Manager\n",
      "Finance and Administration Manager\n",
      "Finance and Administration Manager\n",
      "Finance and Administration Manager\n",
      "Associate, Investment Banking Department, Corporate Finance Unit\n",
      "Factoring Officer, Corporate Finance Department\n",
      "----------------------------------------\n",
      "Senior Software Developer C#, C++, .Net\n",
      "Senior Software Developer C#, C++, .Net\n",
      "C#/C++ Senior Software Developer\n",
      "C#\\ASP.Net Senior Software Developer\n",
      "C++/ C#  Senior Software Developer\n",
      "C# Senior Software Developer\n",
      "Senior Software Developer C#, .Net\n",
      "Senior C Software Developer\n",
      "C# Senior Software Developer\n",
      "C Software Developer\n",
      "----------------------------------------\n",
      "Administrative Assistant\n",
      "Receptionist/ Administrative Assistant\n",
      "Administrative Assistant\n",
      "Administrative Assistant\n",
      "Receptionist/ Administrative Assistant\n",
      "Receptionist/ Administrative Assistant\n",
      "Receptionist/ Administrative Assistant\n",
      "Administrative Assistant\n",
      "Administrative Assistant\n",
      "Receptionist/ Administrative Assistant\n",
      "----------------------------------------\n",
      "QA Technical Writer\n",
      "Lead Technical Writer\n",
      "Junior Technical Writer\n",
      "Junior Technical Writer\n",
      "Junior Technical Writer\n",
      "Staff Writer: News & Politics\n",
      "Local Proposal Writing Expert\n",
      "Human Development Economist, Local Consultant\n",
      "Software Technical Writer\n",
      "Software Technical Writer\n",
      "----------------------------------------\n",
      ".NET Developer\n",
      ".Net Developer\n",
      "C++ Developer\n",
      ".NET Developer\n",
      ".NET Developer\n",
      ".Net Developer\n",
      ".NET Developer\n",
      "Flash/ AS3 Developer\n",
      ".NET Developer\n",
      "Front-End Developer\n",
      "----------------------------------------\n",
      "Turkish-Armenian Translator\n",
      "Translator\n",
      "Receptionist/ Translator\n",
      "Translator\n",
      "Translator\n",
      "Translator\n",
      "Translator\n",
      "Translator\n",
      "Translator\n",
      "Translator\n",
      "----------------------------------------\n",
      "System Administrator - General Specialist\n",
      "Reports Systems Assistant - Administrator (Financial\n",
      "Senior System Administrator\n",
      "Senior Accountant/ Systems Administrator\n",
      "Senior System Administrator\n",
      "Senior System Administrator\n",
      "Senior System Administrator\n",
      "Senior System Administrator\n",
      "IT Senior Systems Administrator\n",
      "Senior Accountant/ Systems Administrator\n",
      "----------------------------------------\n",
      "Program Assistant/ Receptionist\n",
      "Program Assistant / Receptionist\n",
      "Administrative Assistant/ Receptionist\n",
      "Administrative Assistant/ Receptionist\n",
      "Administrative Assistant/ Receptionist\n",
      "Administrative Secretary/ Receptionist\n",
      "Administrative Assistant/ Receptionist\n",
      "Receptionist\n",
      "Receptionist\n",
      "Receptionist\n",
      "----------------------------------------\n",
      "Senior Java Developer\n",
      "Senior Java Developer\n",
      "Senior Java Developer\n",
      "Senior Java Developer\n",
      "Senior Java Developer\n",
      "Senior Java Developer\n",
      "Senior Java Developer\n",
      "Senior Java Developer\n",
      "Senior Java Developer\n",
      "Senior Java Developer\n",
      "----------------------------------------\n",
      "Medical Representative\n",
      "Medical Representative\n",
      "Medical Representative\n",
      "Medical Representative\n",
      "Medical Representative in Yerevan\n",
      "Medical Representative\n",
      "Medical Representative\n",
      "Medical Representative\n",
      "Medical Representative\n",
      "Medical Representative in Yerevan\n",
      "----------------------------------------\n",
      "Accountant/ Cashier\n",
      "Cashier\n",
      "Salesperson/ Cashier\n",
      "Cashier\n",
      "Cashier\n",
      "Cashier\n",
      "Accountant/ Cashier\n",
      "Cashier\n",
      "Cashier\n",
      "Cashier\n",
      "----------------------------------------\n",
      "Accountant\n",
      "Accountant\n",
      "Accountant\n",
      "Accountant\n",
      "Accountant\n",
      "Accountant\n",
      "Accountant\n",
      "Accountant\n",
      "Accountant\n",
      "Accountant\n",
      "----------------------------------------\n",
      "Local Consultant for Armenia SME Finance Program\n",
      "Senior Specialist of Training and Development Group, Human\n",
      "Senior Specialist, Business Planning Department\n",
      "Senior Specialist in Retail Credit Risks Management Group\n",
      "Leading Specialist - Financial Markets Operations Department\n",
      "Leading Specialist, Financial Markets Operations Department\n",
      "Community & Business Development (CBD)Technical Coordinator\n",
      "Business Development Services and Market Linkages Specialist\n",
      "Senior Specialist of Financial Monitoring Service\n",
      "Senior Specialist of Financial Planning, Analysis and Methodology\n",
      "----------------------------------------\n",
      "Deputy Executive Director\n",
      "Deputy Executive Director\n",
      "Deputy Executive Director\n",
      "Executive Director\n",
      "Executive Director\n",
      "Executive Director\n",
      "Executive Director\n",
      "Executive Director\n",
      "Executive Director\n",
      "Executive Director of HFH Armenia\n",
      "----------------------------------------\n",
      "Graphic Designer\n",
      "Graphic Designer\n",
      "Graphic Designer\n",
      "Graphic Designer\n",
      "Graphic Designer\n",
      "Graphic Designer\n",
      "Industrial/ Graphic Designer\n",
      "Graphics Designer\n",
      "Graphics Designer\n",
      "Graphic Designer\n",
      "----------------------------------------\n",
      "Software Developer\n",
      "Software Developer\n",
      "Software Developer\n",
      "C++ Software Developer\n",
      "C++/C#  Software Developer\n",
      "Software Developer\n",
      "Software Developer\n",
      "Software Developer\n",
      "Software Developer 3\n",
      "Software Developer 1\n",
      "----------------------------------------\n",
      "HVAC Engineer\n",
      "HVAC Engineer\n",
      "QA Engineer\n",
      "QA Engineer\n",
      "QA Engineer\n",
      "QA Engineer\n",
      "QA Engineer\n",
      "QA Engineer\n",
      "QA Engineer\n",
      "QA Engineer\n",
      "----------------------------------------\n",
      "Lawyer\n",
      "Lawyer\n",
      "Lawyer\n",
      "Lawyer\n",
      "Lawyer\n",
      "Lawyer\n",
      "Lawyer\n",
      "Lawyer\n",
      "Lawyer\n",
      "Lawyer\n",
      "----------------------------------------\n",
      "Fishfarm Manager\n",
      "Supply/Purchasing Manager\n",
      "Manager\n",
      "Gavar ADP Manager\n",
      "In-store Manager\n",
      "IT Manager\n",
      "Manager\n",
      "Sisian ADP Manager\n",
      "IT Manager\n",
      "Manager\n",
      "----------------------------------------\n",
      "Credit Officer, House-Improvement Loans Department\n",
      "Credit Officer, Group Lending Department\n",
      "Credit Officer\n",
      "Credit Officer\n",
      "Credit Officer in Abovyan\n",
      "Credit Officer\n",
      "Credit Officer in Yerevan\n",
      "Credit Officer\n",
      "Credit Officers\n",
      "Credit Officer\n",
      "----------------------------------------\n",
      "1C Programmer\n",
      "Programmer\n",
      "Programmer\n",
      "Programmer\n",
      "Programmer\n",
      "Programmer\n",
      "Programmer\n",
      "VHDL Programmer\n",
      ".Net Programmer\n",
      "3d Programmer\n",
      "----------------------------------------\n",
      "Project Manager for Cross-border Programs\n",
      "Project Manager\n",
      "Project Manager\n",
      "Project Manager /Web Project/\n",
      "Project Manager\n",
      "Project Manager\n",
      "Project Manager\n",
      "Project Manager\n",
      "Project Manager\n",
      "Project Manager\n",
      "----------------------------------------\n",
      "Contractor/ Intern\n",
      "Contractor/ Intern\n",
      "Contractor/ Intern\n",
      "Contractor/ Intern\n",
      "SQA Engineer/ Intern\n",
      "Intern\n",
      "Contractor/ Intern\n",
      "Contractor/ Intern\n",
      "SQA Engineer/ Intern\n",
      "Contractor/ Intern\n",
      "----------------------------------------\n",
      "Local Marketing Manager\n",
      "Local Marketing Manager\n",
      "Local Marketing Manager\n",
      "Product Marketing Manager\n",
      "Product Marketing Manager\n",
      "Manager of Marketing Department\n",
      "Marketing and Business Development Manager\n",
      "Marketing and Business Development Manager\n",
      "Marketing and Business Development Manager\n",
      "Sales and Marketing Manager\n",
      "----------------------------------------\n",
      "Auditor, Internal Audit Department\n",
      "Auditor, Internal Audit Department\n",
      "Internal Auditor\n",
      "Internal Auditor\n",
      "Internal Auditor\n",
      "Internal Auditor\n",
      "Internal Auditor\n",
      "Internal Auditor\n",
      "Internal Auditor\n",
      "Internal Auditor\n",
      "----------------------------------------\n",
      "Software Engineer\n",
      "Software Engineer\n",
      "Software Engineer\n",
      "C++ Software Engineer\n",
      "Software Engineer\n",
      "Software Engineer\n",
      "Software Engineer\n",
      "Software Engineer\n",
      "Software Engineer\n",
      "Entry-level C++ Software Engineer\n",
      "----------------------------------------\n",
      "Help Desk Administrator, Information and Communication\n",
      "Technologies Department, Software Installation and Maintenance Division\n",
      "Postal Financial Services (PFS) Operations Department Head\n",
      "Postal Financial Services (PFS) Project and Change Division Head\n",
      "Head of Sales and Customer Service Department\n",
      "Postal Financial Services (PFS) Commercial Department Head\n",
      "Postal  Financial Services (PFS) Business Unit Director\n",
      "CRM Group Manager, Development Department, Client Relationship\n",
      "Management Unit\n",
      "Junior Business Management Project Officer\n",
      "Head of Customer Service Department\n",
      "Head of Customer Service Department\n",
      "----------------------------------------\n",
      "Senior ETL Informatica Developer\n",
      "Senior PHP/ Magento Developer\n",
      "Senior Software Developer\n",
      ".NET Senior Software Developer\n",
      "C++ Senior Software Developer\n",
      "Senior Software Developer\n",
      "Senior Software Developer\n",
      "Senior Flash/Flex Software Developer\n",
      "Senior Software Developer\n",
      "Senior Software Developer\n",
      "----------------------------------------\n",
      "Adminsitrative Assistant\n",
      "Information/Education/Communication (IEC) Assistant\n",
      "Event/ BTL Manager Assistant\n",
      "Office Manager/ Assistant to the Director\n",
      "Executive Assistant, Development Department\n",
      "Office Administrator/ Country Director Assistant\n",
      "Senior Executive Assistant\n",
      "Executive Assistant\n",
      "Executive Assistant\n",
      "Receptionist/ Executive Assistant\n",
      "----------------------------------------\n",
      "System/ Business Analyst\n",
      "Business Analyst\n",
      "Business Analyst\n",
      "Business Analyst\n",
      "Business Analyst\n",
      "Business Analyst\n",
      "Business Analyst\n",
      "Business Analyst\n",
      "Business Analyst\n",
      "Business Analyst\n",
      "----------------------------------------\n",
      "Chief Accountant\n",
      "Chief Accountant\n",
      "Chief Accountant\n",
      "Chief Accountant\n",
      "Chief Accountant\n",
      "Chief Accountant\n",
      "Chief Accountant\n",
      "Chief Accountant\n",
      "Chief Accountant\n",
      "Chief Accountant\n",
      "----------------------------------------\n",
      "Administrator-Coordinator\n",
      "Translator-Interpreter\n",
      "Achitect-Designer\n",
      "Secretary-Referent\n",
      "Preseller\n",
      "Waiter/Waitress\n",
      "Architect-Designer\n",
      "Assistant-Translator\n",
      "IT Technician-Intern\n",
      "Preseller\n",
      "----------------------------------------\n",
      "HR Manager\n",
      "HR Manager\n",
      "HR Manager\n",
      "HR Manager\n",
      "HR Manager\n",
      "HR Manager\n",
      "HR Manager\n",
      "Finance/ HR Manager\n",
      "HR Manager\n",
      "HR Manager\n",
      "----------------------------------------\n",
      "QA (Quality Assurance) Engineer\n",
      "Senior Quality Assurance Engineer\n",
      "Quality Assurance Senior Engineer\n",
      "Senior Quality Assurance Engineer\n",
      "Senior Quality Assurance Engineer\n",
      "Senior Quality Assurance Engineer\n",
      "Senior Quality Assurance Engineer\n",
      "Quality Assurance Engineer\n",
      "Quality Assurance Engineer\n",
      "Quality Assurance Engineer\n",
      "----------------------------------------\n",
      "Leading Specialist, Marketing Department\n",
      "Marketing Department Leading Specialist\n",
      "Business Advisor/ Specialist\n",
      "Public Relations/Public Education Specialist (PR/PE Specialist)\n",
      "Specialist in Marketing Department\n",
      "HVAC Specialist\n",
      "IT Specialist (IT Management)\n",
      "Programme Management Specialist\n",
      "QA Specialist\n",
      "QA Specialist\n",
      "----------------------------------------\n",
      "PHP Software Developer\n",
      "PHP Software Developer\n",
      "PHP Software Developers\n",
      "PHP Software Developers\n",
      "PHP Software Developer\n",
      "PHP Software Developer\n",
      "PHP Software Developers\n",
      "PHP Software Developer\n",
      "PHP Software Developer\n",
      "PHP Software Developer\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for n in range(40):\n",
    "    test_cluster = n\n",
    "    most_representative_docs = np.argsort(\n",
    "        np.linalg.norm(vectorized_job_titles - clustering.cluster_centers_[test_cluster], axis=1)\n",
    "    )\n",
    "    for d in most_representative_docs[:10]:\n",
    "        print(job_titles[d])\n",
    "    print('----------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add the cluster category to each job posting\n",
    "df_job_postings['cluster'] = df_clusters['cluster']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the labeled dataset to local\n",
    "df_job_postings.to_csv('data/job_postings_labeled.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Career Classification Training based on Job Description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_job_postings = pd.read_csv('data/job_postings_labeled.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>jobpost</th>\n",
       "      <th>Title</th>\n",
       "      <th>JobDescription</th>\n",
       "      <th>JobRequirment</th>\n",
       "      <th>RequiredQual</th>\n",
       "      <th>ApplicationP</th>\n",
       "      <th>cluster</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AMERIA Investment Consulting Company\\r\\nJOB TI...</td>\n",
       "      <td>Chief Financial Officer</td>\n",
       "      <td>AMERIA Investment Consulting Company is seekin...</td>\n",
       "      <td>- Supervises financial management and administ...</td>\n",
       "      <td>To perform this job successfully, an\\r\\nindivi...</td>\n",
       "      <td>To apply for this position, please submit a\\r\\...</td>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Caucasus Environmental NGO Network (CENN)\\r\\nJ...</td>\n",
       "      <td>Country Coordinator</td>\n",
       "      <td>Public outreach and strengthening of a growing...</td>\n",
       "      <td>- Working with the Country Director to provide...</td>\n",
       "      <td>- Degree in environmentally related field, or ...</td>\n",
       "      <td>Please send resume or CV toursula.kazarian@......</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Manoff Group\\r\\nJOB TITLE:  BCC Specialist\\r\\n...</td>\n",
       "      <td>BCC Specialist</td>\n",
       "      <td>The LEAD (Local Enhancement and Development fo...</td>\n",
       "      <td>- Identify gaps in knowledge and overseeing in...</td>\n",
       "      <td>- Advanced degree in public health, social sci...</td>\n",
       "      <td>Please send cover letter and resume to Amy\\r\\n...</td>\n",
       "      <td>38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Boutique \"Appollo\"\\r\\nJOB TITLE:  Saleswoman\\r...</td>\n",
       "      <td>Saleswoman</td>\n",
       "      <td>Saleswoman will sell menswear and accessories.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>- Candidates should be female, 20-30 years old...</td>\n",
       "      <td>For further information, please contact Irina\\...</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>OSI Assistance Foundation - Armenian Branch Of...</td>\n",
       "      <td>Chief Accountant/ Finance Assistant</td>\n",
       "      <td>The Armenian Branch Office of the Open Society...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>- University degree in finance/ accounting; \\r...</td>\n",
       "      <td>For submission of applications/ CVs, please\\r\\...</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             jobpost  \\\n",
       "0  AMERIA Investment Consulting Company\\r\\nJOB TI...   \n",
       "1  Caucasus Environmental NGO Network (CENN)\\r\\nJ...   \n",
       "2  Manoff Group\\r\\nJOB TITLE:  BCC Specialist\\r\\n...   \n",
       "3  Boutique \"Appollo\"\\r\\nJOB TITLE:  Saleswoman\\r...   \n",
       "4  OSI Assistance Foundation - Armenian Branch Of...   \n",
       "\n",
       "                                 Title  \\\n",
       "0              Chief Financial Officer   \n",
       "1                  Country Coordinator   \n",
       "2                       BCC Specialist   \n",
       "3                           Saleswoman   \n",
       "4  Chief Accountant/ Finance Assistant   \n",
       "\n",
       "                                      JobDescription  \\\n",
       "0  AMERIA Investment Consulting Company is seekin...   \n",
       "1  Public outreach and strengthening of a growing...   \n",
       "2  The LEAD (Local Enhancement and Development fo...   \n",
       "3     Saleswoman will sell menswear and accessories.   \n",
       "4  The Armenian Branch Office of the Open Society...   \n",
       "\n",
       "                                       JobRequirment  \\\n",
       "0  - Supervises financial management and administ...   \n",
       "1  - Working with the Country Director to provide...   \n",
       "2  - Identify gaps in knowledge and overseeing in...   \n",
       "3                                                NaN   \n",
       "4                                                NaN   \n",
       "\n",
       "                                        RequiredQual  \\\n",
       "0  To perform this job successfully, an\\r\\nindivi...   \n",
       "1  - Degree in environmentally related field, or ...   \n",
       "2  - Advanced degree in public health, social sci...   \n",
       "3  - Candidates should be female, 20-30 years old...   \n",
       "4  - University degree in finance/ accounting; \\r...   \n",
       "\n",
       "                                        ApplicationP  cluster  \n",
       "0  To apply for this position, please submit a\\r\\...       34  \n",
       "1  Please send resume or CV toursula.kazarian@......       10  \n",
       "2  Please send cover letter and resume to Amy\\r\\n...       38  \n",
       "3  For further information, please contact Irina\\...       11  \n",
       "4  For submission of applications/ CVs, please\\r\\...       32  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_job_postings.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "cluster\n",
       "0       31\n",
       "1      776\n",
       "2      542\n",
       "3      908\n",
       "4      211\n",
       "5      356\n",
       "6      301\n",
       "7      231\n",
       "8      119\n",
       "9      172\n",
       "10     500\n",
       "11     166\n",
       "12     269\n",
       "13     289\n",
       "14      86\n",
       "15     258\n",
       "16    1772\n",
       "17     270\n",
       "18     294\n",
       "19     528\n",
       "20     485\n",
       "21     163\n",
       "22      78\n",
       "23     230\n",
       "24     107\n",
       "25     521\n",
       "26      50\n",
       "27     415\n",
       "28     146\n",
       "29     145\n",
       "30     939\n",
       "31     312\n",
       "32     430\n",
       "33     241\n",
       "34     341\n",
       "35    1363\n",
       "36     135\n",
       "37     107\n",
       "38     596\n",
       "39     207\n",
       "dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_job_postings.groupby(['cluster'], sort=True).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create training and testing datasets\n",
    "X = df_job_postings['JobDescription']\n",
    "y = df_job_postings['cluster']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 1: TF-IDF + Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split into training and testing datasets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, shuffle=True, random_state=7600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1106    We are seeking a Brand Manager to work for a t...\n",
       "3113    Deep Ray Ltd is seeking candidates for the pos...\n",
       "6990    \"Tonus-Les\" Ltd. is looking for enthusiastic,\\...\n",
       "8086    The incumbent will be responsible for vehicle ...\n",
       "5735    The Director will head Investment Banking Depa...\n",
       "Name: JobDescription, dtype: object"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1106    27\n",
       "3113    30\n",
       "6990    13\n",
       "8086    30\n",
       "5735     4\n",
       "Name: cluster, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Kackie\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Kackie\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Kackie\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\Kackie\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "from nltk import word_tokenize\n",
    "from nltk import sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem import PorterStemmer\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tfidf_preprocess(text):\n",
    "    text = text.lower()\n",
    "    words = word_tokenize(text)\n",
    "    words = [word for word in words if word.isalpha()]\n",
    "    \n",
    "    # stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words = [word for word in words if not word in stop_words]\n",
    "\n",
    "    # stemming\n",
    "    stemmer = PorterStemmer()\n",
    "    words = [stemmer.stem(word) for word in words]\n",
    "\n",
    "    text = ' '.join(words)\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Train TF-IDF**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_10976\\339253982.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;31m# fit the tfidf to the entire X\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[0mtfidf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\Users\\Kackie\\anaconda3\\envs\\DataScience\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   2051\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_check_params\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2052\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_warn_for_unused_params\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2053\u001b[1;33m         \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mraw_documents\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2054\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_tfidf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2055\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Kackie\\anaconda3\\envs\\DataScience\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36mfit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   1328\u001b[0m                     \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1329\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1330\u001b[1;33m         \u001b[0mvocabulary\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_count_vocab\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mraw_documents\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfixed_vocabulary_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1331\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1332\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbinary\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Kackie\\anaconda3\\envs\\DataScience\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36m_count_vocab\u001b[1;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[0;32m   1199\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mraw_documents\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1200\u001b[0m             \u001b[0mfeature_counter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1201\u001b[1;33m             \u001b[1;32mfor\u001b[0m \u001b[0mfeature\u001b[0m \u001b[1;32min\u001b[0m \u001b[0manalyze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1202\u001b[0m                 \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1203\u001b[0m                     \u001b[0mfeature_idx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mfeature\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Kackie\\anaconda3\\envs\\DataScience\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36m_analyze\u001b[1;34m(doc, analyzer, tokenizer, ngrams, preprocessor, decoder, stop_words)\u001b[0m\n\u001b[0;32m    111\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    112\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mpreprocessor\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 113\u001b[1;33m             \u001b[0mdoc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpreprocessor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    114\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mtokenizer\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    115\u001b[0m             \u001b[0mdoc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_10976\\4258998267.py\u001b[0m in \u001b[0;36mtfidf_preprocess\u001b[1;34m(text)\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[1;31m# stopwords\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m     \u001b[0mstop_words\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstopwords\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwords\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'english'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m     \u001b[0mwords\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mword\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mwords\u001b[0m \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mword\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mstop_words\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Kackie\\anaconda3\\envs\\DataScience\\lib\\site-packages\\nltk\\corpus\\reader\\wordlist.py\u001b[0m in \u001b[0;36mwords\u001b[1;34m(self, fileids, ignore_lines_startswith)\u001b[0m\n\u001b[0;32m     19\u001b[0m         return [\n\u001b[0;32m     20\u001b[0m             \u001b[0mline\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m             \u001b[1;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mline_tokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraw\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfileids\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mline\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mignore_lines_startswith\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m         ]\n",
      "\u001b[1;32mc:\\Users\\Kackie\\anaconda3\\envs\\DataScience\\lib\\site-packages\\nltk\\corpus\\reader\\api.py\u001b[0m in \u001b[0;36mraw\u001b[1;34m(self, fileids)\u001b[0m\n\u001b[0;32m    216\u001b[0m         \u001b[0mcontents\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    217\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mfileids\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 218\u001b[1;33m             \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mfp\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    219\u001b[0m                 \u001b[0mcontents\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    220\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mconcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcontents\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Kackie\\anaconda3\\envs\\DataScience\\lib\\site-packages\\nltk\\corpus\\reader\\api.py\u001b[0m in \u001b[0;36mopen\u001b[1;34m(self, file)\u001b[0m\n\u001b[0;32m    229\u001b[0m         \"\"\"\n\u001b[0;32m    230\u001b[0m         \u001b[0mencoding\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencoding\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 231\u001b[1;33m         \u001b[0mstream\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_root\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mencoding\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    232\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mstream\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    233\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Kackie\\anaconda3\\envs\\DataScience\\lib\\site-packages\\nltk\\data.py\u001b[0m in \u001b[0;36mjoin\u001b[1;34m(self, fileid)\u001b[0m\n\u001b[0;32m    332\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfileid\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    333\u001b[0m         \u001b[0m_path\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfileid\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 334\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mFileSystemPathPointer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    335\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    336\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Kackie\\anaconda3\\envs\\DataScience\\lib\\site-packages\\nltk\\compat.py\u001b[0m in \u001b[0;36m_decorator\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     39\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_decorator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m         \u001b[0margs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0madd_py3_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 41\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0minit_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     42\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mwraps\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minit_func\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_decorator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Kackie\\anaconda3\\envs\\DataScience\\lib\\site-packages\\nltk\\data.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, _path)\u001b[0m\n\u001b[0;32m    309\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    310\u001b[0m         \u001b[0m_path\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mabspath\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 311\u001b[1;33m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    312\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mOSError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"No such file or directory: %r\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0m_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    313\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_path\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_path\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Kackie\\anaconda3\\envs\\DataScience\\lib\\genericpath.py\u001b[0m in \u001b[0;36mexists\u001b[1;34m(path)\u001b[0m\n\u001b[0;32m     17\u001b[0m     \u001b[1;34m\"\"\"Test whether a path exists.  Returns False for broken symbolic links\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m         \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# define tfidf vectorizer\n",
    "tfidf = TfidfVectorizer(\n",
    "    preprocessor=tfidf_preprocess,\n",
    "    ngram_range = (1, 2),\n",
    ")\n",
    "\n",
    "# fit the tfidf to the entire X\n",
    "tfidf.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load TF-IDF from local**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load tfidf vectorizer from saved path\n",
    "import pickle\n",
    "tfidf = pickle.load(open('models/career_tfidf_vectorizer.pickle', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform the train and test datasets for X\n",
    "X_train_tfidf_transformed = tfidf.transform(X_train)\n",
    "X_test_tfidf_transformed = tfidf.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10563, 104384)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get shape of the tfidf transformed X_train\n",
    "X_train_tfidf_transformed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the tfidf vectorizer and transformed matrices\n",
    "import pickle\n",
    "\n",
    "pickle.dump(tfidf, open('models/career_tfidf_vectorizer.pickle', 'wb'))\n",
    "pickle.dump(X_train_tfidf_transformed, open('models/career_X_train_tfidf_transformed.pickle', 'wb'))\n",
    "pickle.dump(X_test_tfidf_transformed, open('models/career_X_test_tfidf_transformed.pickle', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Naive Bayes (baseline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_tfidf_mnb = MultinomialNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB()"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_tfidf_mnb.fit(X_train_tfidf_transformed, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3333333333333333"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_test, clf_tfidf_mnb.predict(X_test_tfidf_transformed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        13\n",
      "           1       0.54      0.63      0.58       232\n",
      "           2       0.84      0.45      0.58       157\n",
      "           3       0.75      0.33      0.45       273\n",
      "           4       0.00      0.00      0.00        64\n",
      "           5       0.92      0.20      0.33       111\n",
      "           6       0.88      0.22      0.35        96\n",
      "           7       1.00      0.06      0.11        67\n",
      "           8       0.00      0.00      0.00        36\n",
      "           9       0.00      0.00      0.00        56\n",
      "          10       0.93      0.18      0.30       141\n",
      "          11       0.00      0.00      0.00        54\n",
      "          12       0.83      0.72      0.77        79\n",
      "          13       0.94      0.57      0.71        81\n",
      "          14       0.00      0.00      0.00        29\n",
      "          15       0.67      0.05      0.09        79\n",
      "          16       0.18      0.98      0.31       544\n",
      "          17       0.00      0.00      0.00        75\n",
      "          18       1.00      0.12      0.21        93\n",
      "          19       0.49      0.61      0.54       154\n",
      "          20       0.78      0.21      0.33       146\n",
      "          21       1.00      0.08      0.15        50\n",
      "          22       0.00      0.00      0.00        23\n",
      "          23       1.00      0.19      0.31        75\n",
      "          24       0.00      0.00      0.00        27\n",
      "          25       1.00      0.03      0.07       173\n",
      "          26       0.80      0.31      0.44        13\n",
      "          27       0.96      0.22      0.36       118\n",
      "          28       0.00      0.00      0.00        39\n",
      "          29       0.00      0.00      0.00        36\n",
      "          30       0.71      0.18      0.28       290\n",
      "          31       0.80      0.17      0.28        95\n",
      "          32       1.00      0.07      0.13       134\n",
      "          33       0.00      0.00      0.00        66\n",
      "          34       0.85      0.43      0.57        81\n",
      "          35       0.38      0.41      0.39       408\n",
      "          36       1.00      0.07      0.14        41\n",
      "          37       1.00      0.03      0.06        35\n",
      "          38       1.00      0.03      0.06       170\n",
      "          39       1.00      0.19      0.32        73\n",
      "\n",
      "    accuracy                           0.33      4527\n",
      "   macro avg       0.58      0.19      0.23      4527\n",
      "weighted avg       0.62      0.33      0.30      4527\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Kackie\\anaconda3\\envs\\DataScience\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\Kackie\\anaconda3\\envs\\DataScience\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\Kackie\\anaconda3\\envs\\DataScience\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, clf_tfidf_mnb.predict(X_test_tfidf_transformed)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Linear SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_tfidf_sgd = SGDClassifier(\n",
    "    loss='hinge',\n",
    "    penalty='l2',\n",
    "    alpha=1e-3,\n",
    "    random_state=7600,\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Epoch 1\n",
      "Norm: 0.84, NNZs: 1057, Bias: -1.012067, T: 10563, Avg. loss: 0.003892\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 0.70, NNZs: 1371, Bias: -1.011447, T: 21126, Avg. loss: 0.003115\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 0.66, NNZs: 1688, Bias: -1.010589, T: 31689, Avg. loss: 0.003064\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 0.65, NNZs: 1938, Bias: -1.010838, T: 42252, Avg. loss: 0.003041\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 0.65, NNZs: 2147, Bias: -1.010795, T: 52815, Avg. loss: 0.003031\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 0.64, NNZs: 2277, Bias: -1.010073, T: 63378, Avg. loss: 0.003037\n",
      "Total training time: 0.01 seconds.\n",
      "Convergence after 6 epochs took 0.01 seconds\n",
      "-- Epoch 1\n",
      "Norm: 4.48, NNZs: 11917, Bias: -1.048483, T: 10563, Avg. loss: 0.090813\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 4.32, NNZs: 14555, Bias: -1.042134, T: 21126, Avg. loss: 0.083768\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 4.28, NNZs: 15846, Bias: -1.037839, T: 31689, Avg. loss: 0.082796\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 4.23, NNZs: 16720, Bias: -1.039249, T: 42252, Avg. loss: 0.082121\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 4.22, NNZs: 17527, Bias: -1.035712, T: 52815, Avg. loss: 0.082099\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 4.22, NNZs: 17853, Bias: -1.034344, T: 63378, Avg. loss: 0.081908\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 4.24, NNZs: 18256, Bias: -1.031181, T: 73941, Avg. loss: 0.081818\n",
      "Total training time: 0.02 seconds.\n",
      "Convergence after 7 epochs took 0.02 seconds\n",
      "-- Epoch 1\n",
      "Norm: 4.61, NNZs: 7911, Bias: -1.055240, T: 10563, Avg. loss: 0.060339\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 4.53, NNZs: 9149, Bias: -1.045981, T: 21126, Avg. loss: 0.055100\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 4.51, NNZs: 9737, Bias: -1.045107, T: 31689, Avg. loss: 0.054167\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 4.51, NNZs: 10298, Bias: -1.043518, T: 42252, Avg. loss: 0.053766\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 4.50, NNZs: 10571, Bias: -1.040675, T: 52815, Avg. loss: 0.053466\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 4.50, NNZs: 10830, Bias: -1.039966, T: 63378, Avg. loss: 0.053449\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 4.50, NNZs: 10952, Bias: -1.038281, T: 73941, Avg. loss: 0.053224\n",
      "Total training time: 0.01 seconds.\n",
      "Convergence after 7 epochs took 0.01 seconds\n",
      "-- Epoch 1\n",
      "Norm: 4.05, NNZs: 18585, Bias: -1.065900, T: 10563, Avg. loss: 0.118434\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 3.80, NNZs: 22857, Bias: -1.051114, T: 21126, Avg. loss: 0.110143\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 3.74, NNZs: 25109, Bias: -1.046826, T: 31689, Avg. loss: 0.108802\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 3.72, NNZs: 26350, Bias: -1.039193, T: 42252, Avg. loss: 0.108259\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 3.70, NNZs: 27273, Bias: -1.037737, T: 52815, Avg. loss: 0.107913\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 3.69, NNZs: 27807, Bias: -1.035462, T: 63378, Avg. loss: 0.107820\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 3.69, NNZs: 28204, Bias: -1.036323, T: 73941, Avg. loss: 0.107564\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 8\n",
      "Norm: 3.69, NNZs: 28446, Bias: -1.034452, T: 84504, Avg. loss: 0.107466\n",
      "Total training time: 0.02 seconds.\n",
      "Convergence after 8 epochs took 0.02 seconds\n",
      "-- Epoch 1\n",
      "Norm: 1.66, NNZs: 5238, Bias: -1.035770, T: 10563, Avg. loss: 0.028821\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 1.47, NNZs: 6983, Bias: -1.029881, T: 21126, Avg. loss: 0.026728\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 1.42, NNZs: 8544, Bias: -1.025414, T: 31689, Avg. loss: 0.026446\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 1.40, NNZs: 9299, Bias: -1.024291, T: 42252, Avg. loss: 0.026346\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 1.39, NNZs: 10152, Bias: -1.022947, T: 52815, Avg. loss: 0.026256\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 1.38, NNZs: 10768, Bias: -1.022901, T: 63378, Avg. loss: 0.026204\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 1.38, NNZs: 11338, Bias: -1.022438, T: 73941, Avg. loss: 0.026159\n",
      "Total training time: 0.01 seconds.\n",
      "Convergence after 7 epochs took 0.01 seconds\n",
      "-- Epoch 1\n",
      "Norm: 2.07, NNZs: 5463, Bias: -1.045906, T: 10563, Avg. loss: 0.047898\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 1.82, NNZs: 7085, Bias: -1.034258, T: 21126, Avg. loss: 0.044940\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 1.75, NNZs: 7952, Bias: -1.032497, T: 31689, Avg. loss: 0.044362\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 1.71, NNZs: 9099, Bias: -1.030060, T: 42252, Avg. loss: 0.044227\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 1.70, NNZs: 9758, Bias: -1.026302, T: 52815, Avg. loss: 0.044131\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 1.69, NNZs: 10637, Bias: -1.024690, T: 63378, Avg. loss: 0.044035\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 1.68, NNZs: 11028, Bias: -1.023719, T: 73941, Avg. loss: 0.044026\n",
      "Total training time: 0.01 seconds.\n",
      "Convergence after 7 epochs took 0.01 seconds\n",
      "-- Epoch 1\n",
      "Norm: 3.14, NNZs: 5591, Bias: -1.079108, T: 10563, Avg. loss: 0.030489\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 3.09, NNZs: 6718, Bias: -1.068343, T: 21126, Avg. loss: 0.027281\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 3.02, NNZs: 7390, Bias: -1.067924, T: 31689, Avg. loss: 0.026815\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 3.03, NNZs: 7781, Bias: -1.065956, T: 42252, Avg. loss: 0.026617\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 3.07, NNZs: 7933, Bias: -1.063068, T: 52815, Avg. loss: 0.026506\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 3.05, NNZs: 8090, Bias: -1.062494, T: 63378, Avg. loss: 0.026465\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 3.05, NNZs: 8183, Bias: -1.061455, T: 73941, Avg. loss: 0.026420\n",
      "Total training time: 0.01 seconds.\n",
      "Convergence after 7 epochs took 0.01 seconds\n",
      "-- Epoch 1\n",
      "Norm: 2.07, NNZs: 6969, Bias: -1.013036, T: 10563, Avg. loss: 0.031096\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 1.90, NNZs: 9280, Bias: -1.014099, T: 21126, Avg. loss: 0.028610\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 1.85, NNZs: 11513, Bias: -1.010217, T: 31689, Avg. loss: 0.028332\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 1.83, NNZs: 13140, Bias: -1.008042, T: 42252, Avg. loss: 0.028143\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 1.83, NNZs: 14620, Bias: -1.008711, T: 52815, Avg. loss: 0.028135\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 1.82, NNZs: 15530, Bias: -1.006746, T: 63378, Avg. loss: 0.028045\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 1.82, NNZs: 16613, Bias: -1.006003, T: 73941, Avg. loss: 0.028003\n",
      "Total training time: 0.02 seconds.\n",
      "Convergence after 7 epochs took 0.02 seconds\n",
      "-- Epoch 1\n",
      "Norm: 1.18, NNZs: 2207, Bias: -1.035654, T: 10563, Avg. loss: 0.016840\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 0.94, NNZs: 2489, Bias: -1.031162, T: 21126, Avg. loss: 0.015534\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 0.86, NNZs: 2657, Bias: -1.029804, T: 31689, Avg. loss: 0.015405\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 0.83, NNZs: 2737, Bias: -1.028972, T: 42252, Avg. loss: 0.015303\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 0.81, NNZs: 2843, Bias: -1.026948, T: 52815, Avg. loss: 0.015289\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 0.80, NNZs: 2940, Bias: -1.026783, T: 63378, Avg. loss: 0.015254\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 0.79, NNZs: 2992, Bias: -1.026164, T: 73941, Avg. loss: 0.015229\n",
      "Total training time: 0.01 seconds.\n",
      "Convergence after 7 epochs took 0.01 seconds\n",
      "-- Epoch 1\n",
      "Norm: 2.10, NNZs: 4526, Bias: -1.044465, T: 10563, Avg. loss: 0.020450\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 1.99, NNZs: 5310, Bias: -1.034990, T: 21126, Avg. loss: 0.018758\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 1.96, NNZs: 5849, Bias: -1.029229, T: 31689, Avg. loss: 0.018657\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 1.95, NNZs: 6447, Bias: -1.030797, T: 42252, Avg. loss: 0.018496\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 1.95, NNZs: 6743, Bias: -1.027794, T: 52815, Avg. loss: 0.018481\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 1.94, NNZs: 6975, Bias: -1.026958, T: 63378, Avg. loss: 0.018434\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 1.94, NNZs: 7153, Bias: -1.026505, T: 73941, Avg. loss: 0.018399\n",
      "Total training time: 0.01 seconds.\n",
      "Convergence after 7 epochs took 0.01 seconds\n",
      "-- Epoch 1\n",
      "Norm: 3.20, NNZs: 11550, Bias: -1.047906, T: 10563, Avg. loss: 0.065442\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 3.02, NNZs: 14880, Bias: -1.039437, T: 21126, Avg. loss: 0.061166\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 2.98, NNZs: 16638, Bias: -1.030426, T: 31689, Avg. loss: 0.060661\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 2.97, NNZs: 18165, Bias: -1.028650, T: 42252, Avg. loss: 0.060248\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 2.96, NNZs: 18996, Bias: -1.026938, T: 52815, Avg. loss: 0.060027\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 2.95, NNZs: 19711, Bias: -1.024481, T: 63378, Avg. loss: 0.059897\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 2.95, NNZs: 20667, Bias: -1.025438, T: 73941, Avg. loss: 0.059802\n",
      "Total training time: 0.01 seconds.\n",
      "Convergence after 7 epochs took 0.01 seconds\n",
      "-- Epoch 1\n",
      "Norm: 1.54, NNZs: 3832, Bias: -1.008739, T: 10563, Avg. loss: 0.021917\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 1.34, NNZs: 5126, Bias: -1.005352, T: 21126, Avg. loss: 0.020225\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 1.29, NNZs: 6453, Bias: -1.002922, T: 31689, Avg. loss: 0.020031\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 1.27, NNZs: 7185, Bias: -1.001046, T: 42252, Avg. loss: 0.019951\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 1.26, NNZs: 8091, Bias: -0.999366, T: 52815, Avg. loss: 0.019904\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 1.25, NNZs: 9102, Bias: -1.000443, T: 63378, Avg. loss: 0.019860\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 1.25, NNZs: 9814, Bias: -1.000487, T: 73941, Avg. loss: 0.019833\n",
      "Total training time: 0.01 seconds.\n",
      "Convergence after 7 epochs took 0.01 seconds\n",
      "-- Epoch 1\n",
      "Norm: 4.16, NNZs: 3406, Bias: -1.057388, T: 10563, Avg. loss: 0.023570\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 4.09, NNZs: 3951, Bias: -1.059391, T: 21126, Avg. loss: 0.019547\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 4.11, NNZs: 4253, Bias: -1.056776, T: 31689, Avg. loss: 0.019391\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 4.10, NNZs: 4636, Bias: -1.056909, T: 42252, Avg. loss: 0.019041\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 4.10, NNZs: 4813, Bias: -1.054757, T: 52815, Avg. loss: 0.018895\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 4.10, NNZs: 4892, Bias: -1.053423, T: 63378, Avg. loss: 0.018926\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 4.11, NNZs: 4954, Bias: -1.053448, T: 73941, Avg. loss: 0.018764\n",
      "Total training time: 0.01 seconds.\n",
      "Convergence after 7 epochs took 0.01 seconds\n",
      "-- Epoch 1\n",
      "Norm: 4.21, NNZs: 4888, Bias: -1.055577, T: 10563, Avg. loss: 0.021774\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 4.34, NNZs: 6843, Bias: -1.056873, T: 21126, Avg. loss: 0.018901\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 4.35, NNZs: 7422, Bias: -1.052122, T: 31689, Avg. loss: 0.018218\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 4.37, NNZs: 7993, Bias: -1.049768, T: 42252, Avg. loss: 0.018001\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 4.39, NNZs: 8417, Bias: -1.048599, T: 52815, Avg. loss: 0.017726\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 4.39, NNZs: 8547, Bias: -1.048425, T: 63378, Avg. loss: 0.017670\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 4.37, NNZs: 8698, Bias: -1.048303, T: 73941, Avg. loss: 0.017425\n",
      "Total training time: 0.01 seconds.\n",
      "Convergence after 7 epochs took 0.01 seconds\n",
      "-- Epoch 1\n",
      "Norm: 1.18, NNZs: 2455, Bias: -1.021879, T: 10563, Avg. loss: 0.011319\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 0.98, NNZs: 2872, Bias: -1.015037, T: 21126, Avg. loss: 0.010291\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 0.93, NNZs: 3665, Bias: -1.015747, T: 31689, Avg. loss: 0.010178\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 0.91, NNZs: 3873, Bias: -1.012805, T: 42252, Avg. loss: 0.010182\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 0.90, NNZs: 4105, Bias: -1.011529, T: 52815, Avg. loss: 0.010124\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 0.90, NNZs: 4422, Bias: -1.011535, T: 63378, Avg. loss: 0.010099\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 0.89, NNZs: 4664, Bias: -1.011064, T: 73941, Avg. loss: 0.010102\n",
      "Total training time: 0.01 seconds.\n",
      "Convergence after 7 epochs took 0.01 seconds\n",
      "-- Epoch 1\n",
      "Norm: 2.42, NNZs: 4272, Bias: -1.068354, T: 10563, Avg. loss: 0.032316\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 2.33, NNZs: 5108, Bias: -1.065444, T: 21126, Avg. loss: 0.029532\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 2.30, NNZs: 5408, Bias: -1.065891, T: 31689, Avg. loss: 0.029266\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 2.29, NNZs: 5599, Bias: -1.064150, T: 42252, Avg. loss: 0.029147\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 2.28, NNZs: 5635, Bias: -1.062896, T: 52815, Avg. loss: 0.029079\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 2.28, NNZs: 5659, Bias: -1.060353, T: 63378, Avg. loss: 0.029000\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 2.28, NNZs: 5683, Bias: -1.060193, T: 73941, Avg. loss: 0.028982\n",
      "Total training time: 0.01 seconds.\n",
      "Convergence after 7 epochs took 0.01 seconds\n",
      "-- Epoch 1\n",
      "Norm: 5.30, NNZs: 44012, Bias: -1.035231, T: 10563, Avg. loss: 0.229729\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 5.01, NNZs: 51038, Bias: -1.030371, T: 21126, Avg. loss: 0.215118\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 4.94, NNZs: 54016, Bias: -1.015598, T: 31689, Avg. loss: 0.212906\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 4.91, NNZs: 55356, Bias: -1.014301, T: 42252, Avg. loss: 0.211953\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 4.89, NNZs: 56399, Bias: -1.011247, T: 52815, Avg. loss: 0.211243\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 4.88, NNZs: 57152, Bias: -1.009706, T: 63378, Avg. loss: 0.210911\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 4.88, NNZs: 57561, Bias: -1.010512, T: 73941, Avg. loss: 0.210498\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 8\n",
      "Norm: 4.87, NNZs: 57976, Bias: -1.006950, T: 84504, Avg. loss: 0.210417\n",
      "Total training time: 0.02 seconds.\n",
      "Convergence after 8 epochs took 0.02 seconds\n",
      "-- Epoch 1\n",
      "Norm: 1.91, NNZs: 7434, Bias: -1.038149, T: 10563, Avg. loss: 0.038126\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 1.70, NNZs: 9899, Bias: -1.034196, T: 21126, Avg. loss: 0.035468\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 1.65, NNZs: 11530, Bias: -1.032482, T: 31689, Avg. loss: 0.035043\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 1.63, NNZs: 12587, Bias: -1.028179, T: 42252, Avg. loss: 0.034870\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 1.62, NNZs: 13665, Bias: -1.026939, T: 52815, Avg. loss: 0.034741\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 1.61, NNZs: 14148, Bias: -1.026524, T: 63378, Avg. loss: 0.034674\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 1.61, NNZs: 14820, Bias: -1.025273, T: 73941, Avg. loss: 0.034646\n",
      "Total training time: 0.01 seconds.\n",
      "Convergence after 7 epochs took 0.01 seconds\n",
      "-- Epoch 1\n",
      "Norm: 2.96, NNZs: 6811, Bias: -1.045678, T: 10563, Avg. loss: 0.034400\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 2.87, NNZs: 8506, Bias: -1.043488, T: 21126, Avg. loss: 0.031189\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 2.85, NNZs: 9219, Bias: -1.041850, T: 31689, Avg. loss: 0.030671\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 2.84, NNZs: 9620, Bias: -1.037174, T: 42252, Avg. loss: 0.030513\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 2.84, NNZs: 9815, Bias: -1.035113, T: 52815, Avg. loss: 0.030451\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 2.84, NNZs: 9948, Bias: -1.033187, T: 63378, Avg. loss: 0.030374\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 2.84, NNZs: 10084, Bias: -1.033139, T: 73941, Avg. loss: 0.030353\n",
      "Total training time: 0.02 seconds.\n",
      "Convergence after 7 epochs took 0.02 seconds\n",
      "-- Epoch 1\n",
      "Norm: 2.87, NNZs: 6754, Bias: -1.060116, T: 10563, Avg. loss: 0.071513\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 2.64, NNZs: 8078, Bias: -1.047889, T: 21126, Avg. loss: 0.066580\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 2.57, NNZs: 9081, Bias: -1.047338, T: 31689, Avg. loss: 0.065817\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 2.54, NNZs: 9558, Bias: -1.042276, T: 42252, Avg. loss: 0.065555\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 2.52, NNZs: 10236, Bias: -1.038892, T: 52815, Avg. loss: 0.065420\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 2.50, NNZs: 10782, Bias: -1.038960, T: 63378, Avg. loss: 0.065312\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 2.49, NNZs: 10992, Bias: -1.037920, T: 73941, Avg. loss: 0.065241\n",
      "Total training time: 0.01 seconds.\n",
      "Convergence after 7 epochs took 0.01 seconds\n",
      "-- Epoch 1\n",
      "Norm: 3.31, NNZs: 8704, Bias: -1.016682, T: 10563, Avg. loss: 0.061937\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 3.11, NNZs: 11296, Bias: -1.018077, T: 21126, Avg. loss: 0.057167\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 3.06, NNZs: 12903, Bias: -1.018363, T: 31689, Avg. loss: 0.056465\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 3.03, NNZs: 13842, Bias: -1.015332, T: 42252, Avg. loss: 0.056041\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 3.02, NNZs: 14509, Bias: -1.013987, T: 52815, Avg. loss: 0.055820\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 3.01, NNZs: 15081, Bias: -1.013199, T: 63378, Avg. loss: 0.055687\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 3.01, NNZs: 15941, Bias: -1.012458, T: 73941, Avg. loss: 0.055744\n",
      "Total training time: 0.01 seconds.\n",
      "Convergence after 7 epochs took 0.01 seconds\n",
      "-- Epoch 1\n",
      "Norm: 2.21, NNZs: 3621, Bias: -1.023225, T: 10563, Avg. loss: 0.020223\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 2.09, NNZs: 5285, Bias: -1.029204, T: 21126, Avg. loss: 0.017769\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 2.06, NNZs: 5821, Bias: -1.023885, T: 31689, Avg. loss: 0.017625\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 2.05, NNZs: 6301, Bias: -1.023616, T: 42252, Avg. loss: 0.017514\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 2.05, NNZs: 6676, Bias: -1.023315, T: 52815, Avg. loss: 0.017446\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 2.04, NNZs: 6946, Bias: -1.022152, T: 63378, Avg. loss: 0.017415\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 2.04, NNZs: 7224, Bias: -1.020695, T: 73941, Avg. loss: 0.017418\n",
      "Total training time: 0.01 seconds.\n",
      "Convergence after 7 epochs took 0.01 seconds\n",
      "-- Epoch 1\n",
      "Norm: 0.97, NNZs: 2303, Bias: -1.032170, T: 10563, Avg. loss: 0.011564\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 0.78, NNZs: 2681, Bias: -1.024950, T: 21126, Avg. loss: 0.010229\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 0.73, NNZs: 2990, Bias: -1.024006, T: 31689, Avg. loss: 0.010146\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 0.71, NNZs: 3421, Bias: -1.023024, T: 42252, Avg. loss: 0.010088\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 0.70, NNZs: 3743, Bias: -1.020833, T: 52815, Avg. loss: 0.010064\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 0.69, NNZs: 3994, Bias: -1.020349, T: 63378, Avg. loss: 0.010046\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 0.69, NNZs: 4249, Bias: -1.020136, T: 73941, Avg. loss: 0.010034\n",
      "Total training time: 0.01 seconds.\n",
      "Convergence after 7 epochs took 0.01 seconds\n",
      "-- Epoch 1\n",
      "Norm: 2.81, NNZs: 4708, Bias: -1.055850, T: 10563, Avg. loss: 0.024824\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 2.78, NNZs: 5831, Bias: -1.049276, T: 21126, Avg. loss: 0.022340\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 2.79, NNZs: 6387, Bias: -1.045127, T: 31689, Avg. loss: 0.022134\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 2.79, NNZs: 6560, Bias: -1.043550, T: 42252, Avg. loss: 0.022132\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 2.79, NNZs: 7082, Bias: -1.043552, T: 52815, Avg. loss: 0.021844\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 2.79, NNZs: 7166, Bias: -1.043350, T: 63378, Avg. loss: 0.021787\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 2.79, NNZs: 7244, Bias: -1.042265, T: 73941, Avg. loss: 0.021787\n",
      "Total training time: 0.01 seconds.\n",
      "Convergence after 7 epochs took 0.01 seconds\n",
      "-- Epoch 1\n",
      "Norm: 1.39, NNZs: 2846, Bias: -1.020680, T: 10563, Avg. loss: 0.015867\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 1.21, NNZs: 4254, Bias: -1.028264, T: 21126, Avg. loss: 0.014275\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 1.17, NNZs: 4861, Bias: -1.021572, T: 31689, Avg. loss: 0.014185\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 1.15, NNZs: 5517, Bias: -1.023871, T: 42252, Avg. loss: 0.014074\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 1.14, NNZs: 6133, Bias: -1.022267, T: 52815, Avg. loss: 0.014010\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 1.14, NNZs: 6421, Bias: -1.020150, T: 63378, Avg. loss: 0.014017\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 1.13, NNZs: 6626, Bias: -1.019579, T: 73941, Avg. loss: 0.013992\n",
      "Total training time: 0.01 seconds.\n",
      "Convergence after 7 epochs took 0.01 seconds\n",
      "-- Epoch 1\n",
      "Norm: 2.89, NNZs: 15737, Bias: -1.077303, T: 10563, Avg. loss: 0.066067\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 2.69, NNZs: 19567, Bias: -1.066028, T: 21126, Avg. loss: 0.061121\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 2.63, NNZs: 21061, Bias: -1.060956, T: 31689, Avg. loss: 0.060380\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 2.62, NNZs: 22119, Bias: -1.056383, T: 42252, Avg. loss: 0.060054\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 2.60, NNZs: 22667, Bias: -1.056043, T: 52815, Avg. loss: 0.059908\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 2.60, NNZs: 22786, Bias: -1.052160, T: 63378, Avg. loss: 0.059795\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 2.59, NNZs: 23001, Bias: -1.052646, T: 73941, Avg. loss: 0.059709\n",
      "Total training time: 0.01 seconds.\n",
      "Convergence after 7 epochs took 0.01 seconds\n",
      "-- Epoch 1\n",
      "Norm: 1.67, NNZs: 2213, Bias: -1.046629, T: 10563, Avg. loss: 0.004979\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 1.67, NNZs: 2623, Bias: -1.042789, T: 21126, Avg. loss: 0.004453\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 1.69, NNZs: 2988, Bias: -1.043190, T: 31689, Avg. loss: 0.004252\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 1.70, NNZs: 3226, Bias: -1.042636, T: 42252, Avg. loss: 0.004193\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 1.71, NNZs: 3251, Bias: -1.041242, T: 52815, Avg. loss: 0.004179\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 1.71, NNZs: 3540, Bias: -1.042387, T: 63378, Avg. loss: 0.004126\n",
      "Total training time: 0.01 seconds.\n",
      "Convergence after 6 epochs took 0.01 seconds\n",
      "-- Epoch 1\n",
      "Norm: 3.00, NNZs: 7701, Bias: -1.084842, T: 10563, Avg. loss: 0.054541\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 2.87, NNZs: 9359, Bias: -1.069469, T: 21126, Avg. loss: 0.050159\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 2.83, NNZs: 10320, Bias: -1.065967, T: 31689, Avg. loss: 0.049466\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 2.82, NNZs: 11105, Bias: -1.063290, T: 42252, Avg. loss: 0.049145\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 2.81, NNZs: 11371, Bias: -1.060851, T: 52815, Avg. loss: 0.049004\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 2.81, NNZs: 11924, Bias: -1.059206, T: 63378, Avg. loss: 0.048937\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 2.81, NNZs: 12074, Bias: -1.057979, T: 73941, Avg. loss: 0.048843\n",
      "Total training time: 0.01 seconds.\n",
      "Convergence after 7 epochs took 0.01 seconds\n",
      "-- Epoch 1\n",
      "Norm: 2.14, NNZs: 4404, Bias: -1.028897, T: 10563, Avg. loss: 0.018424\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 2.03, NNZs: 5325, Bias: -1.020352, T: 21126, Avg. loss: 0.016844\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 2.01, NNZs: 6198, Bias: -1.015852, T: 31689, Avg. loss: 0.016726\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 2.00, NNZs: 7000, Bias: -1.016454, T: 42252, Avg. loss: 0.016571\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 1.99, NNZs: 7448, Bias: -1.015225, T: 52815, Avg. loss: 0.016533\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 1.99, NNZs: 7939, Bias: -1.014226, T: 63378, Avg. loss: 0.016513\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 1.99, NNZs: 8370, Bias: -1.013836, T: 73941, Avg. loss: 0.016465\n",
      "Total training time: 0.01 seconds.\n",
      "Convergence after 7 epochs took 0.01 seconds\n",
      "-- Epoch 1\n",
      "Norm: 1.78, NNZs: 4152, Bias: -1.037372, T: 10563, Avg. loss: 0.020692\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 1.65, NNZs: 5226, Bias: -1.033707, T: 21126, Avg. loss: 0.018664\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 1.62, NNZs: 6014, Bias: -1.034637, T: 31689, Avg. loss: 0.018542\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 1.61, NNZs: 6559, Bias: -1.031918, T: 42252, Avg. loss: 0.018392\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 1.60, NNZs: 6805, Bias: -1.031879, T: 52815, Avg. loss: 0.018335\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 1.59, NNZs: 7251, Bias: -1.031567, T: 63378, Avg. loss: 0.018317\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 1.59, NNZs: 7617, Bias: -1.030596, T: 73941, Avg. loss: 0.018289\n",
      "Total training time: 0.01 seconds.\n",
      "Convergence after 7 epochs took 0.01 seconds\n",
      "-- Epoch 1\n",
      "Norm: 3.38, NNZs: 20292, Bias: -1.024004, T: 10563, Avg. loss: 0.125618\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 3.06, NNZs: 28061, Bias: -1.016689, T: 21126, Avg. loss: 0.117877\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 2.98, NNZs: 32769, Bias: -1.014036, T: 31689, Avg. loss: 0.116727\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 2.94, NNZs: 35518, Bias: -1.012034, T: 42252, Avg. loss: 0.116124\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 2.93, NNZs: 37361, Bias: -1.008897, T: 52815, Avg. loss: 0.115749\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 2.92, NNZs: 38650, Bias: -1.006415, T: 63378, Avg. loss: 0.115575\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 2.91, NNZs: 39771, Bias: -1.005757, T: 73941, Avg. loss: 0.115396\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 8\n",
      "Norm: 2.91, NNZs: 40633, Bias: -1.004698, T: 84504, Avg. loss: 0.115288\n",
      "Total training time: 0.02 seconds.\n",
      "Convergence after 8 epochs took 0.02 seconds\n",
      "-- Epoch 1\n",
      "Norm: 2.11, NNZs: 4334, Bias: -1.066041, T: 10563, Avg. loss: 0.042253\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 1.90, NNZs: 5537, Bias: -1.064114, T: 21126, Avg. loss: 0.038992\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 1.87, NNZs: 6020, Bias: -1.053118, T: 31689, Avg. loss: 0.038616\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 1.83, NNZs: 6673, Bias: -1.055783, T: 42252, Avg. loss: 0.038416\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 1.82, NNZs: 7134, Bias: -1.053818, T: 52815, Avg. loss: 0.038241\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 1.82, NNZs: 7329, Bias: -1.050760, T: 63378, Avg. loss: 0.038208\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 1.82, NNZs: 7474, Bias: -1.048711, T: 73941, Avg. loss: 0.038157\n",
      "Total training time: 0.01 seconds.\n",
      "Convergence after 7 epochs took 0.01 seconds\n",
      "-- Epoch 1\n",
      "Norm: 3.19, NNZs: 8307, Bias: -1.053195, T: 10563, Avg. loss: 0.052491\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 2.98, NNZs: 10402, Bias: -1.041689, T: 21126, Avg. loss: 0.048239\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 3.03, NNZs: 12243, Bias: -1.038010, T: 31689, Avg. loss: 0.047674\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 3.04, NNZs: 13242, Bias: -1.038447, T: 42252, Avg. loss: 0.047288\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 3.05, NNZs: 14191, Bias: -1.034752, T: 52815, Avg. loss: 0.047166\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 3.03, NNZs: 14445, Bias: -1.036223, T: 63378, Avg. loss: 0.046988\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 3.05, NNZs: 14895, Bias: -1.033994, T: 73941, Avg. loss: 0.046927\n",
      "Total training time: 0.02 seconds.\n",
      "Convergence after 7 epochs took 0.02 seconds\n",
      "-- Epoch 1\n",
      "Norm: 2.23, NNZs: 6750, Bias: -1.022326, T: 10563, Avg. loss: 0.032639\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 2.08, NNZs: 9100, Bias: -1.016557, T: 21126, Avg. loss: 0.030112\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 2.04, NNZs: 11343, Bias: -1.017595, T: 31689, Avg. loss: 0.029711\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 2.03, NNZs: 12589, Bias: -1.015291, T: 42252, Avg. loss: 0.029506\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 2.02, NNZs: 13717, Bias: -1.013316, T: 52815, Avg. loss: 0.029485\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 2.02, NNZs: 14634, Bias: -1.012588, T: 63378, Avg. loss: 0.029404\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 2.01, NNZs: 15286, Bias: -1.011996, T: 73941, Avg. loss: 0.029367\n",
      "Total training time: 0.01 seconds.\n",
      "Convergence after 7 epochs took 0.01 seconds\n",
      "-- Epoch 1\n",
      "Norm: 3.41, NNZs: 6696, Bias: -1.071478, T: 10563, Avg. loss: 0.044255\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 3.26, NNZs: 8576, Bias: -1.074518, T: 21126, Avg. loss: 0.040501\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 3.23, NNZs: 8975, Bias: -1.069693, T: 31689, Avg. loss: 0.039915\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 3.22, NNZs: 9219, Bias: -1.068257, T: 42252, Avg. loss: 0.039560\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 3.21, NNZs: 9400, Bias: -1.065527, T: 52815, Avg. loss: 0.039491\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 3.21, NNZs: 9451, Bias: -1.063369, T: 63378, Avg. loss: 0.039389\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 3.21, NNZs: 9517, Bias: -1.063443, T: 73941, Avg. loss: 0.039366\n",
      "Total training time: 0.01 seconds.\n",
      "Convergence after 7 epochs took 0.01 seconds\n",
      "-- Epoch 1\n",
      "Norm: 4.38, NNZs: 30810, Bias: -1.004169, T: 10563, Avg. loss: 0.181967\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 4.02, NNZs: 40229, Bias: -0.993095, T: 21126, Avg. loss: 0.170982\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 3.93, NNZs: 45130, Bias: -0.987295, T: 31689, Avg. loss: 0.169259\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 3.89, NNZs: 48123, Bias: -0.985806, T: 42252, Avg. loss: 0.168418\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 3.88, NNZs: 49857, Bias: -0.982825, T: 52815, Avg. loss: 0.167870\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 3.87, NNZs: 51674, Bias: -0.982267, T: 63378, Avg. loss: 0.167627\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 3.86, NNZs: 52300, Bias: -0.983225, T: 73941, Avg. loss: 0.167313\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 8\n",
      "Norm: 3.85, NNZs: 52784, Bias: -0.981762, T: 84504, Avg. loss: 0.167255\n",
      "Total training time: 0.02 seconds.\n",
      "Convergence after 8 epochs took 0.02 seconds\n",
      "-- Epoch 1\n",
      "Norm: 2.04, NNZs: 4293, Bias: -1.024768, T: 10563, Avg. loss: 0.016988\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 1.89, NNZs: 5647, Bias: -1.024385, T: 21126, Avg. loss: 0.014977\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 1.85, NNZs: 6303, Bias: -1.025622, T: 31689, Avg. loss: 0.014867\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 1.84, NNZs: 6889, Bias: -1.027031, T: 42252, Avg. loss: 0.014696\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 1.83, NNZs: 7152, Bias: -1.025058, T: 52815, Avg. loss: 0.014668\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 1.83, NNZs: 7515, Bias: -1.023857, T: 63378, Avg. loss: 0.014621\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 1.83, NNZs: 7717, Bias: -1.022489, T: 73941, Avg. loss: 0.014659\n",
      "Total training time: 0.01 seconds.\n",
      "Convergence after 7 epochs took 0.01 seconds\n",
      "-- Epoch 1\n",
      "Norm: 1.61, NNZs: 2542, Bias: -1.049971, T: 10563, Avg. loss: 0.013325\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 1.51, NNZs: 2956, Bias: -1.040719, T: 21126, Avg. loss: 0.011960\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 1.47, NNZs: 3639, Bias: -1.044055, T: 31689, Avg. loss: 0.011800\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 1.47, NNZs: 3897, Bias: -1.042263, T: 42252, Avg. loss: 0.011688\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 1.46, NNZs: 4067, Bias: -1.041422, T: 52815, Avg. loss: 0.011665\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 1.46, NNZs: 4100, Bias: -1.039555, T: 63378, Avg. loss: 0.011644\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 1.46, NNZs: 4282, Bias: -1.039544, T: 73941, Avg. loss: 0.011619\n",
      "Total training time: 0.01 seconds.\n",
      "Convergence after 7 epochs took 0.01 seconds\n",
      "-- Epoch 1\n",
      "Norm: 2.97, NNZs: 12749, Bias: -1.043994, T: 10563, Avg. loss: 0.081296\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 2.73, NNZs: 16351, Bias: -1.040917, T: 21126, Avg. loss: 0.076098\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 2.67, NNZs: 18038, Bias: -1.032771, T: 31689, Avg. loss: 0.075247\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 2.65, NNZs: 19563, Bias: -1.029024, T: 42252, Avg. loss: 0.074914\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 2.64, NNZs: 20336, Bias: -1.027051, T: 52815, Avg. loss: 0.074697\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 2.63, NNZs: 21330, Bias: -1.023122, T: 63378, Avg. loss: 0.074546\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 2.62, NNZs: 21851, Bias: -1.023467, T: 73941, Avg. loss: 0.074488\n",
      "Total training time: 0.01 seconds.\n",
      "Convergence after 7 epochs took 0.01 seconds\n",
      "-- Epoch 1\n",
      "Norm: 2.24, NNZs: 3218, Bias: -1.028880, T: 10563, Avg. loss: 0.024117\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 2.12, NNZs: 4108, Bias: -1.021332, T: 21126, Avg. loss: 0.021804\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 2.09, NNZs: 4620, Bias: -1.017754, T: 31689, Avg. loss: 0.021609\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 2.08, NNZs: 5119, Bias: -1.016427, T: 42252, Avg. loss: 0.021438\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 2.08, NNZs: 5443, Bias: -1.013512, T: 52815, Avg. loss: 0.021380\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 2.07, NNZs: 5862, Bias: -1.013996, T: 63378, Avg. loss: 0.021382\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 2.07, NNZs: 6420, Bias: -1.012392, T: 73941, Avg. loss: 0.021299\n",
      "Total training time: 0.01 seconds.\n",
      "Convergence after 7 epochs took 0.01 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  40 out of  40 | elapsed:    0.4s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SGDClassifier(alpha=0.001, random_state=7600, verbose=1)"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_tfidf_sgd.fit(X_train_tfidf_transformed, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6567263088137839"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_test, clf_tfidf_sgd.predict(X_test_tfidf_transformed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.50      0.23      0.32        13\n",
      "           1       0.61      0.72      0.66       232\n",
      "           2       0.65      0.90      0.75       157\n",
      "           3       0.67      0.70      0.68       273\n",
      "           4       0.67      0.41      0.50        64\n",
      "           5       0.60      0.52      0.56       111\n",
      "           6       0.82      0.78      0.80        96\n",
      "           7       0.72      0.42      0.53        67\n",
      "           8       0.67      0.33      0.44        36\n",
      "           9       0.68      0.84      0.75        56\n",
      "          10       0.66      0.63      0.64       141\n",
      "          11       0.72      0.43      0.53        54\n",
      "          12       0.67      0.94      0.78        79\n",
      "          13       0.63      0.95      0.75        81\n",
      "          14       0.73      0.55      0.63        29\n",
      "          15       0.69      0.68      0.69        79\n",
      "          16       0.60      0.71      0.65       544\n",
      "          17       0.72      0.64      0.68        75\n",
      "          18       0.75      0.83      0.79        93\n",
      "          19       0.73      0.71      0.72       154\n",
      "          20       0.62      0.66      0.64       146\n",
      "          21       0.77      0.88      0.82        50\n",
      "          22       0.50      0.04      0.08        23\n",
      "          23       0.77      0.91      0.83        75\n",
      "          24       0.77      0.74      0.75        27\n",
      "          25       0.70      0.72      0.71       173\n",
      "          26       0.67      0.46      0.55        13\n",
      "          27       0.65      0.83      0.73       118\n",
      "          28       0.83      1.00      0.91        39\n",
      "          29       0.74      0.39      0.51        36\n",
      "          30       0.65      0.49      0.56       290\n",
      "          31       0.78      0.63      0.70        95\n",
      "          32       0.63      0.53      0.58       134\n",
      "          33       0.73      0.68      0.70        66\n",
      "          34       0.54      0.77      0.64        81\n",
      "          35       0.55      0.45      0.50       408\n",
      "          36       0.59      0.90      0.71        41\n",
      "          37       0.83      0.57      0.68        35\n",
      "          38       0.62      0.51      0.56       170\n",
      "          39       0.88      0.77      0.82        73\n",
      "\n",
      "    accuracy                           0.66      4527\n",
      "   macro avg       0.68      0.65      0.65      4527\n",
      "weighted avg       0.66      0.66      0.65      4527\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, clf_tfidf_sgd.predict(X_test_tfidf_transformed)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_tfidf_lr = LogisticRegression(\n",
    "    max_iter=200,\n",
    "    random_state=7600,\n",
    "    verbose=1,\n",
    "    n_jobs=cpu_count()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=16)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=16)]: Done   1 out of   1 | elapsed:   51.4s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(max_iter=200, n_jobs=16, random_state=7600, verbose=1)"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_tfidf_lr.fit(X_train_tfidf_transformed, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6114424563728739"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_test, clf_tfidf_lr.predict(X_test_tfidf_transformed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.08      0.14        13\n",
      "           1       0.70      0.70      0.70       232\n",
      "           2       0.79      0.84      0.81       157\n",
      "           3       0.61      0.67      0.64       273\n",
      "           4       0.62      0.36      0.46        64\n",
      "           5       0.68      0.50      0.57       111\n",
      "           6       0.83      0.67      0.74        96\n",
      "           7       1.00      0.24      0.39        67\n",
      "           8       0.90      0.25      0.39        36\n",
      "           9       0.77      0.43      0.55        56\n",
      "          10       0.78      0.52      0.63       141\n",
      "          11       1.00      0.09      0.17        54\n",
      "          12       0.85      0.87      0.86        79\n",
      "          13       0.90      0.79      0.84        81\n",
      "          14       1.00      0.17      0.29        29\n",
      "          15       0.77      0.65      0.70        79\n",
      "          16       0.45      0.82      0.58       544\n",
      "          17       0.78      0.39      0.52        75\n",
      "          18       0.87      0.65      0.74        93\n",
      "          19       0.61      0.78      0.68       154\n",
      "          20       0.73      0.59      0.65       146\n",
      "          21       0.92      0.46      0.61        50\n",
      "          22       0.00      0.00      0.00        23\n",
      "          23       0.94      0.63      0.75        75\n",
      "          24       1.00      0.37      0.54        27\n",
      "          25       0.74      0.64      0.69       173\n",
      "          26       0.80      0.31      0.44        13\n",
      "          27       0.80      0.75      0.77       118\n",
      "          28       0.93      0.72      0.81        39\n",
      "          29       1.00      0.44      0.62        36\n",
      "          30       0.52      0.53      0.53       290\n",
      "          31       0.77      0.65      0.70        95\n",
      "          32       0.66      0.45      0.53       134\n",
      "          33       0.89      0.48      0.63        66\n",
      "          34       0.65      0.72      0.68        81\n",
      "          35       0.36      0.60      0.45       408\n",
      "          36       0.70      0.51      0.59        41\n",
      "          37       0.93      0.37      0.53        35\n",
      "          38       0.74      0.43      0.54       170\n",
      "          39       0.98      0.66      0.79        73\n",
      "\n",
      "    accuracy                           0.61      4527\n",
      "   macro avg       0.77      0.52      0.58      4527\n",
      "weighted avg       0.68      0.61      0.61      4527\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Kackie\\anaconda3\\envs\\DataScience\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\Kackie\\anaconda3\\envs\\DataScience\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\Kackie\\anaconda3\\envs\\DataScience\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, clf_tfidf_lr.predict(X_test_tfidf_transformed)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Kackie\\anaconda3\\envs\\DataScience\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import scipy\n",
    "\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1 GPU(s) available.\n",
      "Device name:  NVIDIA GeForce RTX 3080 Ti\n"
     ]
    }
   ],
   "source": [
    "# set up gpu for training\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    print(f'There are {torch.cuda.device_count()} GPU(s) available.')\n",
    "    print('Device name: ', torch.cuda.get_device_name(0))\n",
    "\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    print('No GPU available, using the CPU instead.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert X tfidf vectors to tensor\n",
    "X_train_tfidf_tensor = torch.tensor(scipy.sparse.csr_matrix(X_train_tfidf_transformed).todense()).float().to(device)\n",
    "X_test_tfidf_tensor = torch.tensor(scipy.sparse.csr_matrix(X_test_tfidf_transformed).todense()).float()\n",
    "\n",
    "# convert y vectors to tensor\n",
    "y_train_tensor = torch.tensor(y_train.values).to(device)\n",
    "y_test_tensor = torch.tensor(y_test.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Linear(in_features=104384, out_features=64, bias=True)\n",
       "  (1): ReLU()\n",
       "  (2): Linear(in_features=64, out_features=40, bias=True)\n",
       "  (3): LogSoftmax(dim=1)\n",
       ")"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define cnn model\n",
    "clf_tfidf_cnn = nn.Sequential(\n",
    "    nn.Linear(X_train_tfidf_tensor.shape[1], 64),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(64, df_job_postings['cluster'].nunique()),\n",
    "    nn.LogSoftmax(dim=1)\n",
    ")\n",
    "clf_tfidf_cnn.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define loss\n",
    "criterion = nn.NLLLoss()\n",
    "logps = clf_tfidf_cnn(X_train_tfidf_tensor)\n",
    "loss = criterion(logps, y_train_tensor)\n",
    "\n",
    "loss.backward()\n",
    "\n",
    "# define optimizer\n",
    "optimizer = torch.optim.Adam(clf_tfidf_cnn.parameters(), lr=2e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define parameters for model training\n",
    "EPOCHS = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n",
      "Epoch: 1\n",
      "Epoch: 2\n",
      "Epoch: 3\n",
      "Epoch: 4\n",
      "Epoch: 5\n",
      "Epoch: 6\n",
      "Epoch: 7\n",
      "Epoch: 8\n",
      "Epoch: 9\n",
      "Epoch: 10\n",
      "Epoch: 11\n",
      "Epoch: 12\n",
      "Epoch: 13\n",
      "Epoch: 14\n",
      "Epoch: 15\n",
      "Epoch: 16\n",
      "Epoch: 17\n",
      "Epoch: 18\n",
      "Epoch: 19\n",
      "Epoch: 20\n",
      "Epoch: 21\n",
      "Epoch: 22\n",
      "Epoch: 23\n",
      "Epoch: 24\n",
      "Epoch: 25\n",
      "Epoch: 26\n",
      "Epoch: 27\n",
      "Epoch: 28\n",
      "Epoch: 29\n",
      "Epoch: 30\n",
      "Epoch: 31\n",
      "Epoch: 32\n",
      "Epoch: 33\n",
      "Epoch: 34\n",
      "Epoch: 35\n",
      "Epoch: 36\n",
      "Epoch: 37\n",
      "Epoch: 38\n",
      "Epoch: 39\n",
      "Epoch: 40\n",
      "Epoch: 41\n",
      "Epoch: 42\n",
      "Epoch: 43\n",
      "Epoch: 44\n",
      "Epoch: 45\n",
      "Epoch: 46\n",
      "Epoch: 47\n",
      "Epoch: 48\n",
      "Epoch: 49\n"
     ]
    }
   ],
   "source": [
    "# train the model\n",
    "for epoch in range(EPOCHS):\n",
    "    # zero the paramater gradients\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # forward + backward + optimize\n",
    "    outputs = clf_tfidf_cnn.forward(X_train_tfidf_tensor)\n",
    "    loss = criterion(outputs, y_train_tensor)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # print statistics\n",
    "    print(f'Epoch: {epoch}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate our model\n",
    "with torch.no_grad():\n",
    "    clf_tfidf_cnn.eval()\n",
    "    log_ps = clf_tfidf_cnn(X_test_tfidf_tensor)\n",
    "    test_loss = criterion(log_ps, y_test_tensor)\n",
    "\n",
    "    ps = torch.exp(log_ps)\n",
    "    top_p, top_class = ps.topk(1, dim=1)\n",
    "    equals = top_class == y_test_tensor.view(*top_class.shape)\n",
    "    test_accuracy = torch.mean(equals.float())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.4632, device='cuda:0')"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LSTM (fixed length) - memory error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FixedLengthLSTM(nn.Module):\n",
    "    def __init__(self, embedding_dim, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)\n",
    "        self.linear = nn.Linear(hidden_dim, df_job_postings['cluster'].nunique())\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.dropout(x)\n",
    "        lstm_out, (ht, ct) = self.lstm(x)\n",
    "        return self.linear(ht[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, epochs=10, lr=0.001):\n",
    "    parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
    "    optimizer = torch.optim.Adam(parameters, lr=lr)\n",
    "    for i in range(epochs):\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model.forward(X_train_tfidf_tensor)\n",
    "        loss = F.cross_entropy(outputs, y_train_tensor)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "def validation_metrics(model, X_test, y_test):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    sum_loss = 0.0\n",
    "\n",
    "    for x in X_test:\n",
    "        y_hat = model(x)\n",
    "        loss = F.cross_entropy(y_hat, y_test)\n",
    "        pred = torch.max(y_hat, 1)[1]\n",
    "        correct += (pred == y_test).float().sum()\n",
    "        total += y_test.shape[0]\n",
    "        sum_loss += loss.item()*y_test.shape[0]\n",
    "    return sum_loss/total, correct/total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FixedLengthLSTM(\n",
       "  (lstm): LSTM(104384, 16, batch_first=True)\n",
       "  (linear): Linear(in_features=16, out_features=40, bias=True)\n",
       "  (dropout): Dropout(p=0.2, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_tfidf_lstm_fixed = FixedLengthLSTM(X_train_tfidf_tensor.shape[1], 16)\n",
    "clf_tfidf_lstm_fixed.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 4.11 GiB (GPU 0; 12.00 GiB total capacity; 5.16 GiB already allocated; 197.57 MiB free; 8.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_10976\\3642861150.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtrain_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclf_tfidf_lstm_fixed\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_10976\\1245342384.py\u001b[0m in \u001b[0;36mtrain_model\u001b[1;34m(model, epochs, lr)\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m         \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train_tfidf_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m         \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcross_entropy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m         \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_10976\\1266911706.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m         \u001b[0mlstm_out\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mht\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mct\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlstm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mht\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Kackie\\anaconda3\\envs\\DataScience\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1110\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1111\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Kackie\\anaconda3\\envs\\DataScience\\lib\\site-packages\\torch\\nn\\modules\\dropout.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m     56\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 58\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtraining\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minplace\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     59\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     60\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Kackie\\anaconda3\\envs\\DataScience\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36mdropout\u001b[1;34m(input, p, training, inplace)\u001b[0m\n\u001b[0;32m   1277\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mp\u001b[0m \u001b[1;33m<\u001b[0m \u001b[1;36m0.0\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mp\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1.0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1278\u001b[0m         \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"dropout probability has to be between 0 and 1, \"\u001b[0m \u001b[1;34m\"but got {}\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1279\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_VF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdropout_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0minplace\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0m_VF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1280\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1281\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 4.11 GiB (GPU 0; 12.00 GiB total capacity; 5.16 GiB already allocated; 197.57 MiB free; 8.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "train_model(clf_tfidf_lstm_fixed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 2: Word2Vec (mean of word vectors) + Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split into training and testing datasets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, shuffle=True, random_state=7600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1106    We are seeking a Brand Manager to work for a t...\n",
       "3113    Deep Ray Ltd is seeking candidates for the pos...\n",
       "6990    \"Tonus-Les\" Ltd. is looking for enthusiastic,\\...\n",
       "8086    The incumbent will be responsible for vehicle ...\n",
       "5735    The Director will head Investment Banking Depa...\n",
       "Name: JobDescription, dtype: object"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1106    27\n",
       "3113    30\n",
       "6990    13\n",
       "8086    30\n",
       "5735     4\n",
       "Name: cluster, dtype: int64"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Kackie\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Kackie\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Kackie\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\Kackie\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "from nltk import word_tokenize\n",
    "from nltk import sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem import PorterStemmer\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def w2v_preprocess(text):\n",
    "    text = text.lower()\n",
    "    words = word_tokenize(text)\n",
    "    words = [word for word in words if word.isalpha()]\n",
    "    \n",
    "    # stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words = [word for word in words if not word in stop_words]\n",
    "\n",
    "    # stemming\n",
    "    stemmer = PorterStemmer()\n",
    "    words = [stemmer.stem(word) for word in words]\n",
    "\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from os import cpu_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        [ameria, invest, consult, compani, seek, chief...\n",
       "1        [public, outreach, strengthen, grow, network, ...\n",
       "2        [lead, local, enhanc, develop, health, bcc, sp...\n",
       "3                  [saleswoman, sell, menswear, accessori]\n",
       "4        [armenian, branch, offic, open, societi, insti...\n",
       "                               ...                        \n",
       "15085    [incumb, develop, softwar, applic, work, distr...\n",
       "15086    [incumb, respons, support, director, organ, ac...\n",
       "15087    [tech, startup, technolinguist, base, new, yor...\n",
       "15088    [san, lazzaro, llc, look, individu, work, head...\n",
       "15089    [kamurj, uco, cjsc, look, lawyer, legal, depar...\n",
       "Name: JobDescription, Length: 15090, dtype: object"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# preprocess the X texts\n",
    "X_tokenized = X.apply(w2v_preprocess)\n",
    "X_tokenized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Train Word2Vec**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train our own word2vec model using the career corpus\n",
    "w2v = Word2Vec(\n",
    "    sentences=X_tokenized, \n",
    "    vector_size=300, \n",
    "    window=5,\n",
    "    min_count=1,\n",
    "    workers=cpu_count()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7171"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(w2v.wv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the w2v model \n",
    "w2v.save('models/career_w2v.model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Convert train & test datasets to Word2Vec**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Kackie\\anaconda3\\envs\\DataScience\\lib\\site-packages\\ipykernel_launcher.py:3: RuntimeWarning: Mean of empty slice\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "# get sentence vectors for each row of the training and testing X datasets\n",
    "def get_w2v_sentence_vector(text, model):\n",
    "    sentence_vector = np.nanmean(np.array([model[i] for i in w2v_preprocess(text)]), axis=0).tolist()\n",
    "    if not isinstance(sentence_vector, list):\n",
    "        sentence_vector = [0]*300\n",
    "    return sentence_vector\n",
    "\n",
    "X_train_w2v_transformed = []\n",
    "for text in X_train:\n",
    "    sentence_vector = get_w2v_sentence_vector(text, w2v.wv)\n",
    "    X_train_w2v_transformed.append(sentence_vector)\n",
    "\n",
    "X_test_w2v_transformed = []\n",
    "for text in X_test:\n",
    "    sentence_vector = get_w2v_sentence_vector(text, w2v.wv)\n",
    "    X_test_w2v_transformed.append(sentence_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_w2v_transformed = np.array(X_train_w2v_transformed)\n",
    "X_test_w2v_transformed = np.array(X_test_w2v_transformed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10563, 300)"
      ]
     },
     "execution_count": 276,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_w2v_transformed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4527, 300)"
      ]
     },
     "execution_count": 277,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_w2v_transformed.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Naive Bayes (baseline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [],
   "source": [
    "# since MNB can only do positive values, we need to normalize the word embeddings\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "X_train_w2v_transformed_scaled = MinMaxScaler().fit_transform(X_train_w2v_transformed)\n",
    "X_test_w2v_transformed_scaled = MinMaxScaler().fit_transform(X_test_w2v_transformed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_w2v_mnb = MultinomialNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB()"
      ]
     },
     "execution_count": 287,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_w2v_mnb.fit(X_train_w2v_transformed_scaled, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2701568367572344"
      ]
     },
     "execution_count": 288,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_test, clf_w2v_mnb.predict(X_test_w2v_transformed_scaled))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        13\n",
      "           1       0.30      0.51      0.38       232\n",
      "           2       0.41      0.68      0.51       157\n",
      "           3       0.26      0.29      0.28       273\n",
      "           4       0.25      0.05      0.08        64\n",
      "           5       0.50      0.09      0.15       111\n",
      "           6       0.44      0.44      0.44        96\n",
      "           7       0.00      0.00      0.00        67\n",
      "           8       0.06      0.08      0.07        36\n",
      "           9       0.38      0.36      0.37        56\n",
      "          10       0.59      0.23      0.34       141\n",
      "          11       0.05      0.07      0.06        54\n",
      "          12       0.19      0.52      0.28        79\n",
      "          13       0.69      0.62      0.65        81\n",
      "          14       0.00      0.00      0.00        29\n",
      "          15       0.29      0.42      0.34        79\n",
      "          16       0.27      0.62      0.37       544\n",
      "          17       0.00      0.00      0.00        75\n",
      "          18       0.51      0.23      0.31        93\n",
      "          19       0.17      0.28      0.21       154\n",
      "          20       0.35      0.11      0.17       146\n",
      "          21       0.25      0.04      0.07        50\n",
      "          22       0.00      0.00      0.00        23\n",
      "          23       0.52      0.16      0.24        75\n",
      "          24       0.00      0.00      0.00        27\n",
      "          25       0.27      0.17      0.21       173\n",
      "          26       0.00      0.00      0.00        13\n",
      "          27       0.40      0.27      0.32       118\n",
      "          28       0.00      0.00      0.00        39\n",
      "          29       0.00      0.00      0.00        36\n",
      "          30       0.22      0.14      0.17       290\n",
      "          31       0.00      0.00      0.00        95\n",
      "          32       0.20      0.08      0.12       134\n",
      "          33       0.00      0.00      0.00        66\n",
      "          34       0.36      0.47      0.40        81\n",
      "          35       0.16      0.22      0.18       408\n",
      "          36       0.50      0.05      0.09        41\n",
      "          37       0.29      0.06      0.10        35\n",
      "          38       0.12      0.04      0.05       170\n",
      "          39       0.00      0.00      0.00        73\n",
      "\n",
      "    accuracy                           0.27      4527\n",
      "   macro avg       0.22      0.18      0.17      4527\n",
      "weighted avg       0.26      0.27      0.23      4527\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Kackie\\anaconda3\\envs\\DataScience\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\Kackie\\anaconda3\\envs\\DataScience\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\Kackie\\anaconda3\\envs\\DataScience\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, clf_w2v_mnb.predict(X_test_w2v_transformed_scaled)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Linear SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_w2v_sgd = SGDClassifier(\n",
    "    loss='hinge',\n",
    "    penalty='l2',\n",
    "    alpha=1e-3,\n",
    "    random_state=7600,\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Epoch 1\n",
      "Norm: 0.84, NNZs: 300, Bias: -3.994332, T: 10563, Avg. loss: 0.011362\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 0.90, NNZs: 300, Bias: -3.556771, T: 21126, Avg. loss: 0.005836\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 0.81, NNZs: 300, Bias: -3.344102, T: 31689, Avg. loss: 0.005314\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 0.72, NNZs: 300, Bias: -3.208036, T: 42252, Avg. loss: 0.004855\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 0.75, NNZs: 300, Bias: -3.061649, T: 52815, Avg. loss: 0.004722\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 0.69, NNZs: 300, Bias: -2.976538, T: 63378, Avg. loss: 0.004517\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 0.64, NNZs: 300, Bias: -2.904785, T: 73941, Avg. loss: 0.004506\n",
      "Total training time: 0.03 seconds.\n",
      "Convergence after 7 epochs took 0.03 seconds\n",
      "-- Epoch 1\n",
      "Norm: 4.04, NNZs: 300, Bias: -2.815307, T: 10563, Avg. loss: 0.332334\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 3.24, NNZs: 300, Bias: -1.832887, T: 21126, Avg. loss: 0.140863\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 2.86, NNZs: 300, Bias: -1.371076, T: 31689, Avg. loss: 0.122711\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 2.56, NNZs: 300, Bias: -1.159761, T: 42252, Avg. loss: 0.115686\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 2.49, NNZs: 300, Bias: -1.061128, T: 52815, Avg. loss: 0.111002\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 2.38, NNZs: 300, Bias: -1.046893, T: 63378, Avg. loss: 0.109066\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 2.32, NNZs: 300, Bias: -1.029295, T: 73941, Avg. loss: 0.106557\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 8\n",
      "Norm: 2.36, NNZs: 300, Bias: -0.967936, T: 84504, Avg. loss: 0.105658\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 9\n",
      "Norm: 2.33, NNZs: 300, Bias: -0.990361, T: 95067, Avg. loss: 0.104806\n",
      "Total training time: 0.04 seconds.\n",
      "-- Epoch 10\n",
      "Norm: 2.27, NNZs: 300, Bias: -0.998981, T: 105630, Avg. loss: 0.104100\n",
      "Total training time: 0.04 seconds.\n",
      "-- Epoch 11\n",
      "Norm: 2.26, NNZs: 300, Bias: -0.998780, T: 116193, Avg. loss: 0.103607\n",
      "Total training time: 0.04 seconds.\n",
      "-- Epoch 12\n",
      "Norm: 2.25, NNZs: 300, Bias: -0.998951, T: 126756, Avg. loss: 0.103191\n",
      "Total training time: 0.05 seconds.\n",
      "Convergence after 12 epochs took 0.05 seconds\n",
      "-- Epoch 1\n",
      "Norm: 4.73, NNZs: 300, Bias: -2.145209, T: 10563, Avg. loss: 0.233437\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 3.89, NNZs: 300, Bias: -1.315618, T: 21126, Avg. loss: 0.087715\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 3.43, NNZs: 300, Bias: -1.253754, T: 31689, Avg. loss: 0.081469\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 3.29, NNZs: 300, Bias: -1.140668, T: 42252, Avg. loss: 0.074549\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 3.19, NNZs: 300, Bias: -0.949988, T: 52815, Avg. loss: 0.072840\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 3.14, NNZs: 300, Bias: -1.020306, T: 63378, Avg. loss: 0.071913\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 3.11, NNZs: 300, Bias: -1.035808, T: 73941, Avg. loss: 0.069382\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 8\n",
      "Norm: 3.02, NNZs: 300, Bias: -1.004118, T: 84504, Avg. loss: 0.069834\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 9\n",
      "Norm: 2.99, NNZs: 300, Bias: -1.007092, T: 95067, Avg. loss: 0.068363\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 10\n",
      "Norm: 2.95, NNZs: 300, Bias: -0.930262, T: 105630, Avg. loss: 0.067868\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 11\n",
      "Norm: 2.89, NNZs: 300, Bias: -0.961987, T: 116193, Avg. loss: 0.068843\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 12\n",
      "Norm: 2.87, NNZs: 300, Bias: -0.986595, T: 126756, Avg. loss: 0.067767\n",
      "Total training time: 0.04 seconds.\n",
      "-- Epoch 13\n",
      "Norm: 2.85, NNZs: 300, Bias: -0.964113, T: 137319, Avg. loss: 0.067878\n",
      "Total training time: 0.04 seconds.\n",
      "-- Epoch 14\n",
      "Norm: 2.84, NNZs: 300, Bias: -0.993275, T: 147882, Avg. loss: 0.067140\n",
      "Total training time: 0.04 seconds.\n",
      "Convergence after 14 epochs took 0.04 seconds\n",
      "-- Epoch 1\n",
      "Norm: 4.45, NNZs: 300, Bias: -1.440896, T: 10563, Avg. loss: 0.445184\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 3.39, NNZs: 300, Bias: -1.043539, T: 21126, Avg. loss: 0.172635\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 2.83, NNZs: 300, Bias: -0.934783, T: 31689, Avg. loss: 0.151800\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 2.52, NNZs: 300, Bias: -0.915554, T: 42252, Avg. loss: 0.142698\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 2.31, NNZs: 300, Bias: -0.905740, T: 52815, Avg. loss: 0.137345\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 2.17, NNZs: 300, Bias: -1.022514, T: 63378, Avg. loss: 0.133830\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 1.99, NNZs: 300, Bias: -0.947819, T: 73941, Avg. loss: 0.132372\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 8\n",
      "Norm: 1.85, NNZs: 300, Bias: -0.962895, T: 84504, Avg. loss: 0.130948\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 9\n",
      "Norm: 1.76, NNZs: 300, Bias: -0.952803, T: 95067, Avg. loss: 0.129280\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 10\n",
      "Norm: 1.67, NNZs: 300, Bias: -1.001559, T: 105630, Avg. loss: 0.128508\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 11\n",
      "Norm: 1.60, NNZs: 300, Bias: -0.992156, T: 116193, Avg. loss: 0.127451\n",
      "Total training time: 0.04 seconds.\n",
      "-- Epoch 12\n",
      "Norm: 1.53, NNZs: 300, Bias: -1.016387, T: 126756, Avg. loss: 0.127038\n",
      "Total training time: 0.04 seconds.\n",
      "-- Epoch 13\n",
      "Norm: 1.48, NNZs: 300, Bias: -0.992722, T: 137319, Avg. loss: 0.126238\n",
      "Total training time: 0.04 seconds.\n",
      "-- Epoch 14\n",
      "Norm: 1.43, NNZs: 300, Bias: -1.005914, T: 147882, Avg. loss: 0.126063\n",
      "Total training time: 0.05 seconds.\n",
      "-- Epoch 15\n",
      "Norm: 1.38, NNZs: 300, Bias: -0.986735, T: 158445, Avg. loss: 0.125473\n",
      "Total training time: 0.05 seconds.\n",
      "-- Epoch 16\n",
      "Norm: 1.34, NNZs: 300, Bias: -0.999448, T: 169008, Avg. loss: 0.125154\n",
      "Total training time: 0.05 seconds.\n",
      "Convergence after 16 epochs took 0.05 seconds\n",
      "-- Epoch 1\n",
      "Norm: 2.17, NNZs: 300, Bias: -4.030275, T: 10563, Avg. loss: 0.101464\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 1.40, NNZs: 300, Bias: -3.396725, T: 21126, Avg. loss: 0.046121\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 1.14, NNZs: 300, Bias: -2.994558, T: 31689, Avg. loss: 0.039049\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 1.07, NNZs: 300, Bias: -2.695297, T: 42252, Avg. loss: 0.035816\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 0.96, NNZs: 300, Bias: -2.485018, T: 52815, Avg. loss: 0.034304\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 0.87, NNZs: 300, Bias: -2.278408, T: 63378, Avg. loss: 0.033557\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 0.83, NNZs: 300, Bias: -2.118587, T: 73941, Avg. loss: 0.032413\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 8\n",
      "Norm: 0.79, NNZs: 300, Bias: -1.981033, T: 84504, Avg. loss: 0.031816\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 9\n",
      "Norm: 0.70, NNZs: 300, Bias: -1.892183, T: 95067, Avg. loss: 0.031477\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 10\n",
      "Norm: 0.66, NNZs: 300, Bias: -1.792290, T: 105630, Avg. loss: 0.031023\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 11\n",
      "Norm: 0.62, NNZs: 300, Bias: -1.710825, T: 116193, Avg. loss: 0.030718\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 12\n",
      "Norm: 0.59, NNZs: 300, Bias: -1.636954, T: 126756, Avg. loss: 0.030416\n",
      "Total training time: 0.04 seconds.\n",
      "Convergence after 12 epochs took 0.04 seconds\n",
      "-- Epoch 1\n",
      "Norm: 2.49, NNZs: 300, Bias: -3.890197, T: 10563, Avg. loss: 0.221597\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 1.90, NNZs: 300, Bias: -2.904584, T: 21126, Avg. loss: 0.076906\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 1.53, NNZs: 300, Bias: -2.398751, T: 31689, Avg. loss: 0.064903\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 1.35, NNZs: 300, Bias: -2.061292, T: 42252, Avg. loss: 0.058973\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 1.21, NNZs: 300, Bias: -1.787019, T: 52815, Avg. loss: 0.056456\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 1.07, NNZs: 300, Bias: -1.667323, T: 63378, Avg. loss: 0.054376\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 0.95, NNZs: 300, Bias: -1.520716, T: 73941, Avg. loss: 0.053405\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 8\n",
      "Norm: 0.88, NNZs: 300, Bias: -1.395873, T: 84504, Avg. loss: 0.052282\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 9\n",
      "Norm: 0.82, NNZs: 300, Bias: -1.307712, T: 95067, Avg. loss: 0.051518\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 10\n",
      "Norm: 0.76, NNZs: 300, Bias: -1.257651, T: 105630, Avg. loss: 0.050959\n",
      "Total training time: 0.04 seconds.\n",
      "-- Epoch 11\n",
      "Norm: 0.71, NNZs: 300, Bias: -1.212929, T: 116193, Avg. loss: 0.050517\n",
      "Total training time: 0.04 seconds.\n",
      "-- Epoch 12\n",
      "Norm: 0.69, NNZs: 300, Bias: -1.147038, T: 126756, Avg. loss: 0.049913\n",
      "Total training time: 0.04 seconds.\n",
      "-- Epoch 13\n",
      "Norm: 0.65, NNZs: 300, Bias: -1.109546, T: 137319, Avg. loss: 0.049876\n",
      "Total training time: 0.05 seconds.\n",
      "Convergence after 13 epochs took 0.05 seconds\n",
      "-- Epoch 1\n",
      "Norm: 2.97, NNZs: 300, Bias: -6.446979, T: 10563, Avg. loss: 0.159762\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 2.65, NNZs: 300, Bias: -5.424645, T: 21126, Avg. loss: 0.062741\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 2.35, NNZs: 300, Bias: -4.771770, T: 31689, Avg. loss: 0.054167\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 2.29, NNZs: 300, Bias: -4.366290, T: 42252, Avg. loss: 0.048685\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 2.27, NNZs: 300, Bias: -4.030987, T: 52815, Avg. loss: 0.046547\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 2.21, NNZs: 300, Bias: -3.792235, T: 63378, Avg. loss: 0.045204\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 2.13, NNZs: 300, Bias: -3.590675, T: 73941, Avg. loss: 0.044814\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 8\n",
      "Norm: 2.09, NNZs: 300, Bias: -3.401780, T: 84504, Avg. loss: 0.043506\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 9\n",
      "Norm: 2.04, NNZs: 300, Bias: -3.246345, T: 95067, Avg. loss: 0.043057\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 10\n",
      "Norm: 2.01, NNZs: 300, Bias: -3.107060, T: 105630, Avg. loss: 0.042286\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 11\n",
      "Norm: 1.98, NNZs: 300, Bias: -2.963936, T: 116193, Avg. loss: 0.041819\n",
      "Total training time: 0.04 seconds.\n",
      "-- Epoch 12\n",
      "Norm: 1.93, NNZs: 300, Bias: -2.874526, T: 126756, Avg. loss: 0.041376\n",
      "Total training time: 0.04 seconds.\n",
      "-- Epoch 13\n",
      "Norm: 1.90, NNZs: 300, Bias: -2.776306, T: 137319, Avg. loss: 0.041313\n",
      "Total training time: 0.04 seconds.\n",
      "Convergence after 13 epochs took 0.04 seconds\n",
      "-- Epoch 1\n",
      "Norm: 1.64, NNZs: 300, Bias: -1.782296, T: 10563, Avg. loss: 0.099463\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 1.21, NNZs: 300, Bias: -1.251333, T: 21126, Avg. loss: 0.044468\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 0.89, NNZs: 300, Bias: -1.068415, T: 31689, Avg. loss: 0.038980\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 0.75, NNZs: 300, Bias: -0.985171, T: 42252, Avg. loss: 0.036699\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 0.66, NNZs: 300, Bias: -1.004207, T: 52815, Avg. loss: 0.035317\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 0.60, NNZs: 300, Bias: -0.954196, T: 63378, Avg. loss: 0.034569\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 0.55, NNZs: 300, Bias: -0.971990, T: 73941, Avg. loss: 0.034140\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 8\n",
      "Norm: 0.52, NNZs: 300, Bias: -0.997286, T: 84504, Avg. loss: 0.033573\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 9\n",
      "Norm: 0.51, NNZs: 300, Bias: -1.008809, T: 95067, Avg. loss: 0.033245\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 10\n",
      "Norm: 0.49, NNZs: 300, Bias: -1.017860, T: 105630, Avg. loss: 0.033032\n",
      "Total training time: 0.03 seconds.\n",
      "Convergence after 10 epochs took 0.03 seconds\n",
      "-- Epoch 1\n",
      "Norm: 1.93, NNZs: 300, Bias: -4.962764, T: 10563, Avg. loss: 0.060797\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 1.66, NNZs: 300, Bias: -4.309183, T: 21126, Avg. loss: 0.028429\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 1.53, NNZs: 300, Bias: -3.923129, T: 31689, Avg. loss: 0.023769\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 1.38, NNZs: 300, Bias: -3.730751, T: 42252, Avg. loss: 0.021461\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 1.32, NNZs: 300, Bias: -3.537863, T: 52815, Avg. loss: 0.020395\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 1.27, NNZs: 300, Bias: -3.365589, T: 63378, Avg. loss: 0.019937\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 1.19, NNZs: 300, Bias: -3.246908, T: 73941, Avg. loss: 0.019351\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 8\n",
      "Norm: 1.17, NNZs: 300, Bias: -3.121669, T: 84504, Avg. loss: 0.018981\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 9\n",
      "Norm: 1.14, NNZs: 300, Bias: -3.010835, T: 95067, Avg. loss: 0.018806\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 10\n",
      "Norm: 1.10, NNZs: 300, Bias: -2.931095, T: 105630, Avg. loss: 0.018290\n",
      "Total training time: 0.03 seconds.\n",
      "Convergence after 10 epochs took 0.03 seconds\n",
      "-- Epoch 1\n",
      "Norm: 2.27, NNZs: 300, Bias: -4.490622, T: 10563, Avg. loss: 0.058791\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 1.88, NNZs: 300, Bias: -3.861064, T: 21126, Avg. loss: 0.028957\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 1.62, NNZs: 300, Bias: -3.646968, T: 31689, Avg. loss: 0.025694\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 1.50, NNZs: 300, Bias: -3.476219, T: 42252, Avg. loss: 0.023912\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 1.56, NNZs: 300, Bias: -3.258646, T: 52815, Avg. loss: 0.023469\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 1.47, NNZs: 300, Bias: -3.140199, T: 63378, Avg. loss: 0.023115\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 1.39, NNZs: 300, Bias: -3.051093, T: 73941, Avg. loss: 0.023141\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 8\n",
      "Norm: 1.43, NNZs: 300, Bias: -2.961652, T: 84504, Avg. loss: 0.022205\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 9\n",
      "Norm: 1.40, NNZs: 300, Bias: -2.915638, T: 95067, Avg. loss: 0.022160\n",
      "Total training time: 0.03 seconds.\n",
      "Convergence after 9 epochs took 0.03 seconds\n",
      "-- Epoch 1\n",
      "Norm: 2.88, NNZs: 300, Bias: -3.160078, T: 10563, Avg. loss: 0.217578\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 2.26, NNZs: 300, Bias: -2.669277, T: 21126, Avg. loss: 0.101954\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 1.91, NNZs: 300, Bias: -2.289514, T: 31689, Avg. loss: 0.088786\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 1.62, NNZs: 300, Bias: -2.019806, T: 42252, Avg. loss: 0.083426\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 1.49, NNZs: 300, Bias: -1.869649, T: 52815, Avg. loss: 0.079977\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 1.42, NNZs: 300, Bias: -1.679591, T: 63378, Avg. loss: 0.077869\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 1.31, NNZs: 300, Bias: -1.661705, T: 73941, Avg. loss: 0.076119\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 8\n",
      "Norm: 1.26, NNZs: 300, Bias: -1.520731, T: 84504, Avg. loss: 0.075135\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 9\n",
      "Norm: 1.16, NNZs: 300, Bias: -1.465811, T: 95067, Avg. loss: 0.074636\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 10\n",
      "Norm: 1.11, NNZs: 300, Bias: -1.386062, T: 105630, Avg. loss: 0.073623\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 11\n",
      "Norm: 1.05, NNZs: 300, Bias: -1.357945, T: 116193, Avg. loss: 0.073058\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 12\n",
      "Norm: 1.00, NNZs: 300, Bias: -1.292794, T: 126756, Avg. loss: 0.072759\n",
      "Total training time: 0.04 seconds.\n",
      "-- Epoch 13\n",
      "Norm: 0.97, NNZs: 300, Bias: -1.270748, T: 137319, Avg. loss: 0.072126\n",
      "Total training time: 0.04 seconds.\n",
      "-- Epoch 14\n",
      "Norm: 0.93, NNZs: 300, Bias: -1.235883, T: 147882, Avg. loss: 0.071941\n",
      "Total training time: 0.04 seconds.\n",
      "-- Epoch 15\n",
      "Norm: 0.89, NNZs: 300, Bias: -1.222780, T: 158445, Avg. loss: 0.071580\n",
      "Total training time: 0.05 seconds.\n",
      "Convergence after 15 epochs took 0.05 seconds\n",
      "-- Epoch 1\n",
      "Norm: 1.60, NNZs: 300, Bias: -0.695571, T: 10563, Avg. loss: 0.071532\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 0.90, NNZs: 300, Bias: -0.543314, T: 21126, Avg. loss: 0.031076\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 0.72, NNZs: 300, Bias: -0.520218, T: 31689, Avg. loss: 0.026788\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 0.64, NNZs: 300, Bias: -0.611948, T: 42252, Avg. loss: 0.025113\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 0.56, NNZs: 300, Bias: -0.623764, T: 52815, Avg. loss: 0.024313\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 0.48, NNZs: 300, Bias: -0.708058, T: 63378, Avg. loss: 0.023918\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 0.44, NNZs: 300, Bias: -0.749256, T: 73941, Avg. loss: 0.023381\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 8\n",
      "Norm: 0.40, NNZs: 300, Bias: -0.785992, T: 84504, Avg. loss: 0.023129\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 9\n",
      "Norm: 0.39, NNZs: 300, Bias: -0.808145, T: 95067, Avg. loss: 0.022755\n",
      "Total training time: 0.03 seconds.\n",
      "Convergence after 9 epochs took 0.03 seconds\n",
      "-- Epoch 1\n",
      "Norm: 3.19, NNZs: 300, Bias: -5.843240, T: 10563, Avg. loss: 0.134103\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 2.87, NNZs: 300, Bias: -5.532485, T: 21126, Avg. loss: 0.056023\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 2.57, NNZs: 300, Bias: -5.358566, T: 31689, Avg. loss: 0.047806\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 2.46, NNZs: 300, Bias: -5.218425, T: 42252, Avg. loss: 0.044015\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 2.43, NNZs: 300, Bias: -5.045896, T: 52815, Avg. loss: 0.042287\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 2.34, NNZs: 300, Bias: -4.946029, T: 63378, Avg. loss: 0.041614\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 2.30, NNZs: 300, Bias: -4.832428, T: 73941, Avg. loss: 0.040210\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 8\n",
      "Norm: 2.22, NNZs: 300, Bias: -4.770764, T: 84504, Avg. loss: 0.039502\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 9\n",
      "Norm: 2.15, NNZs: 300, Bias: -4.703063, T: 95067, Avg. loss: 0.039416\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 10\n",
      "Norm: 2.15, NNZs: 300, Bias: -4.623543, T: 105630, Avg. loss: 0.038607\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 11\n",
      "Norm: 2.09, NNZs: 300, Bias: -4.577217, T: 116193, Avg. loss: 0.038618\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 12\n",
      "Norm: 2.05, NNZs: 300, Bias: -4.535081, T: 126756, Avg. loss: 0.038073\n",
      "Total training time: 0.03 seconds.\n",
      "Convergence after 12 epochs took 0.03 seconds\n",
      "-- Epoch 1\n",
      "Norm: 3.65, NNZs: 300, Bias: -4.628685, T: 10563, Avg. loss: 0.068211\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 3.03, NNZs: 300, Bias: -4.258746, T: 21126, Avg. loss: 0.025333\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 3.02, NNZs: 300, Bias: -3.878409, T: 31689, Avg. loss: 0.022641\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 2.82, NNZs: 300, Bias: -3.746716, T: 42252, Avg. loss: 0.021039\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 2.70, NNZs: 300, Bias: -3.537900, T: 52815, Avg. loss: 0.020408\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 2.69, NNZs: 300, Bias: -3.456807, T: 63378, Avg. loss: 0.020269\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 2.56, NNZs: 300, Bias: -3.427888, T: 73941, Avg. loss: 0.019969\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 8\n",
      "Norm: 2.57, NNZs: 300, Bias: -3.352259, T: 84504, Avg. loss: 0.019688\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 9\n",
      "Norm: 2.58, NNZs: 300, Bias: -3.284738, T: 95067, Avg. loss: 0.019620\n",
      "Total training time: 0.03 seconds.\n",
      "Convergence after 9 epochs took 0.03 seconds\n",
      "-- Epoch 1\n",
      "Norm: 1.40, NNZs: 300, Bias: -5.501356, T: 10563, Avg. loss: 0.041245\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 1.45, NNZs: 300, Bias: -4.661715, T: 21126, Avg. loss: 0.020113\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 1.24, NNZs: 300, Bias: -4.328713, T: 31689, Avg. loss: 0.016759\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 1.16, NNZs: 300, Bias: -4.063384, T: 42252, Avg. loss: 0.015522\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 1.08, NNZs: 300, Bias: -3.850456, T: 52815, Avg. loss: 0.014923\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 1.05, NNZs: 300, Bias: -3.661297, T: 63378, Avg. loss: 0.014505\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 0.98, NNZs: 300, Bias: -3.530553, T: 73941, Avg. loss: 0.013931\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 8\n",
      "Norm: 0.96, NNZs: 300, Bias: -3.391484, T: 84504, Avg. loss: 0.013925\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 9\n",
      "Norm: 0.95, NNZs: 300, Bias: -3.268336, T: 95067, Avg. loss: 0.013527\n",
      "Total training time: 0.03 seconds.\n",
      "Convergence after 9 epochs took 0.03 seconds\n",
      "-- Epoch 1\n",
      "Norm: 3.22, NNZs: 300, Bias: -8.389079, T: 10563, Avg. loss: 0.135761\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 2.85, NNZs: 300, Bias: -7.466108, T: 21126, Avg. loss: 0.055444\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 2.76, NNZs: 300, Bias: -6.930877, T: 31689, Avg. loss: 0.047247\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 2.83, NNZs: 300, Bias: -6.492577, T: 42252, Avg. loss: 0.043956\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 2.69, NNZs: 300, Bias: -6.176840, T: 52815, Avg. loss: 0.042313\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 2.62, NNZs: 300, Bias: -5.948726, T: 63378, Avg. loss: 0.040253\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 2.60, NNZs: 300, Bias: -5.701213, T: 73941, Avg. loss: 0.039839\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 8\n",
      "Norm: 2.47, NNZs: 300, Bias: -5.574534, T: 84504, Avg. loss: 0.038566\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 9\n",
      "Norm: 2.46, NNZs: 300, Bias: -5.395179, T: 95067, Avg. loss: 0.038645\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 10\n",
      "Norm: 2.42, NNZs: 300, Bias: -5.264959, T: 105630, Avg. loss: 0.037417\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 11\n",
      "Norm: 2.45, NNZs: 300, Bias: -5.111851, T: 116193, Avg. loss: 0.037505\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 12\n",
      "Norm: 2.42, NNZs: 300, Bias: -4.998059, T: 126756, Avg. loss: 0.037099\n",
      "Total training time: 0.04 seconds.\n",
      "-- Epoch 13\n",
      "Norm: 2.36, NNZs: 300, Bias: -4.915109, T: 137319, Avg. loss: 0.037213\n",
      "Total training time: 0.04 seconds.\n",
      "-- Epoch 14\n",
      "Norm: 2.31, NNZs: 300, Bias: -4.844440, T: 147882, Avg. loss: 0.036339\n",
      "Total training time: 0.04 seconds.\n",
      "-- Epoch 15\n",
      "Norm: 2.32, NNZs: 300, Bias: -4.739568, T: 158445, Avg. loss: 0.036475\n",
      "Total training time: 0.05 seconds.\n",
      "Convergence after 15 epochs took 0.05 seconds\n",
      "-- Epoch 1\n",
      "Norm: 5.29, NNZs: 300, Bias: -1.469069, T: 10563, Avg. loss: 0.710626\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 3.80, NNZs: 300, Bias: -1.646127, T: 21126, Avg. loss: 0.319120\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 3.42, NNZs: 300, Bias: -1.537201, T: 31689, Avg. loss: 0.281343\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 3.24, NNZs: 300, Bias: -1.487301, T: 42252, Avg. loss: 0.267941\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 2.94, NNZs: 300, Bias: -1.464580, T: 52815, Avg. loss: 0.262582\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 2.78, NNZs: 300, Bias: -1.491541, T: 63378, Avg. loss: 0.255540\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 2.65, NNZs: 300, Bias: -1.387416, T: 73941, Avg. loss: 0.252207\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 8\n",
      "Norm: 2.49, NNZs: 300, Bias: -1.387597, T: 84504, Avg. loss: 0.250117\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 9\n",
      "Norm: 2.39, NNZs: 300, Bias: -1.367134, T: 95067, Avg. loss: 0.247628\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 10\n",
      "Norm: 2.30, NNZs: 300, Bias: -1.346113, T: 105630, Avg. loss: 0.245902\n",
      "Total training time: 0.04 seconds.\n",
      "-- Epoch 11\n",
      "Norm: 2.19, NNZs: 300, Bias: -1.317812, T: 116193, Avg. loss: 0.245482\n",
      "Total training time: 0.04 seconds.\n",
      "-- Epoch 12\n",
      "Norm: 2.12, NNZs: 300, Bias: -1.317344, T: 126756, Avg. loss: 0.243494\n",
      "Total training time: 0.04 seconds.\n",
      "-- Epoch 13\n",
      "Norm: 2.05, NNZs: 300, Bias: -1.307954, T: 137319, Avg. loss: 0.242755\n",
      "Total training time: 0.05 seconds.\n",
      "-- Epoch 14\n",
      "Norm: 1.97, NNZs: 300, Bias: -1.286101, T: 147882, Avg. loss: 0.242600\n",
      "Total training time: 0.05 seconds.\n",
      "-- Epoch 15\n",
      "Norm: 1.93, NNZs: 300, Bias: -1.252680, T: 158445, Avg. loss: 0.240867\n",
      "Total training time: 0.06 seconds.\n",
      "-- Epoch 16\n",
      "Norm: 1.89, NNZs: 300, Bias: -1.252345, T: 169008, Avg. loss: 0.240386\n",
      "Total training time: 0.06 seconds.\n",
      "-- Epoch 17\n",
      "Norm: 1.83, NNZs: 300, Bias: -1.235244, T: 179571, Avg. loss: 0.240581\n",
      "Total training time: 0.06 seconds.\n",
      "-- Epoch 18\n",
      "Norm: 1.78, NNZs: 300, Bias: -1.230289, T: 190134, Avg. loss: 0.239789\n",
      "Total training time: 0.07 seconds.\n",
      "-- Epoch 19\n",
      "Norm: 1.74, NNZs: 300, Bias: -1.225030, T: 200697, Avg. loss: 0.239395\n",
      "Total training time: 0.07 seconds.\n",
      "-- Epoch 20\n",
      "Norm: 1.71, NNZs: 300, Bias: -1.200844, T: 211260, Avg. loss: 0.239099\n",
      "Total training time: 0.08 seconds.\n",
      "Convergence after 20 epochs took 0.08 seconds\n",
      "-- Epoch 1\n",
      "Norm: 2.96, NNZs: 300, Bias: -5.550136, T: 10563, Avg. loss: 0.140527\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 2.18, NNZs: 300, Bias: -4.624824, T: 21126, Avg. loss: 0.061692\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 1.87, NNZs: 300, Bias: -3.906067, T: 31689, Avg. loss: 0.053547\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 1.77, NNZs: 300, Bias: -3.377633, T: 42252, Avg. loss: 0.048777\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 1.52, NNZs: 300, Bias: -3.112719, T: 52815, Avg. loss: 0.046402\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 1.39, NNZs: 300, Bias: -2.856486, T: 63378, Avg. loss: 0.044672\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 1.32, NNZs: 300, Bias: -2.608517, T: 73941, Avg. loss: 0.043584\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 8\n",
      "Norm: 1.23, NNZs: 300, Bias: -2.419099, T: 84504, Avg. loss: 0.042818\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 9\n",
      "Norm: 1.16, NNZs: 300, Bias: -2.252276, T: 95067, Avg. loss: 0.041948\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 10\n",
      "Norm: 1.11, NNZs: 300, Bias: -2.103308, T: 105630, Avg. loss: 0.041326\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 11\n",
      "Norm: 1.06, NNZs: 300, Bias: -1.976283, T: 116193, Avg. loss: 0.040842\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 12\n",
      "Norm: 1.01, NNZs: 300, Bias: -1.884970, T: 126756, Avg. loss: 0.040248\n",
      "Total training time: 0.04 seconds.\n",
      "Convergence after 12 epochs took 0.04 seconds\n",
      "-- Epoch 1\n",
      "Norm: 4.57, NNZs: 300, Bias: -2.922543, T: 10563, Avg. loss: 0.125722\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 3.79, NNZs: 300, Bias: -1.977541, T: 21126, Avg. loss: 0.047204\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 3.45, NNZs: 300, Bias: -1.632059, T: 31689, Avg. loss: 0.039646\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 3.27, NNZs: 300, Bias: -1.353817, T: 42252, Avg. loss: 0.037116\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 3.13, NNZs: 300, Bias: -1.246379, T: 52815, Avg. loss: 0.035745\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 3.00, NNZs: 300, Bias: -1.093166, T: 63378, Avg. loss: 0.035644\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 2.95, NNZs: 300, Bias: -1.047638, T: 73941, Avg. loss: 0.034709\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 8\n",
      "Norm: 2.88, NNZs: 300, Bias: -0.983977, T: 84504, Avg. loss: 0.034461\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 9\n",
      "Norm: 2.84, NNZs: 300, Bias: -0.973323, T: 95067, Avg. loss: 0.034268\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 10\n",
      "Norm: 2.81, NNZs: 300, Bias: -1.011672, T: 105630, Avg. loss: 0.033951\n",
      "Total training time: 0.03 seconds.\n",
      "Convergence after 10 epochs took 0.03 seconds\n",
      "-- Epoch 1\n",
      "Norm: 3.44, NNZs: 300, Bias: -6.436885, T: 10563, Avg. loss: 0.311520\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 2.62, NNZs: 300, Bias: -5.539461, T: 21126, Avg. loss: 0.122403\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 2.28, NNZs: 300, Bias: -4.904100, T: 31689, Avg. loss: 0.103886\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 2.11, NNZs: 300, Bias: -4.415689, T: 42252, Avg. loss: 0.095771\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 1.89, NNZs: 300, Bias: -4.102699, T: 52815, Avg. loss: 0.090705\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 1.78, NNZs: 300, Bias: -3.788513, T: 63378, Avg. loss: 0.087805\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 1.65, NNZs: 300, Bias: -3.538315, T: 73941, Avg. loss: 0.085578\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 8\n",
      "Norm: 1.62, NNZs: 300, Bias: -3.285683, T: 84504, Avg. loss: 0.083368\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 9\n",
      "Norm: 1.51, NNZs: 300, Bias: -3.108021, T: 95067, Avg. loss: 0.082491\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 10\n",
      "Norm: 1.41, NNZs: 300, Bias: -2.965989, T: 105630, Avg. loss: 0.080966\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 11\n",
      "Norm: 1.37, NNZs: 300, Bias: -2.810347, T: 116193, Avg. loss: 0.079969\n",
      "Total training time: 0.04 seconds.\n",
      "-- Epoch 12\n",
      "Norm: 1.32, NNZs: 300, Bias: -2.669285, T: 126756, Avg. loss: 0.079302\n",
      "Total training time: 0.04 seconds.\n",
      "-- Epoch 13\n",
      "Norm: 1.26, NNZs: 300, Bias: -2.532256, T: 137319, Avg. loss: 0.078920\n",
      "Total training time: 0.04 seconds.\n",
      "-- Epoch 14\n",
      "Norm: 1.23, NNZs: 300, Bias: -2.419543, T: 147882, Avg. loss: 0.077862\n",
      "Total training time: 0.05 seconds.\n",
      "-- Epoch 15\n",
      "Norm: 1.19, NNZs: 300, Bias: -2.322375, T: 158445, Avg. loss: 0.077462\n",
      "Total training time: 0.05 seconds.\n",
      "-- Epoch 16\n",
      "Norm: 1.16, NNZs: 300, Bias: -2.230487, T: 169008, Avg. loss: 0.076837\n",
      "Total training time: 0.05 seconds.\n",
      "-- Epoch 17\n",
      "Norm: 1.12, NNZs: 300, Bias: -2.149944, T: 179571, Avg. loss: 0.076528\n",
      "Total training time: 0.06 seconds.\n",
      "-- Epoch 18\n",
      "Norm: 1.08, NNZs: 300, Bias: -2.073869, T: 190134, Avg. loss: 0.076217\n",
      "Total training time: 0.06 seconds.\n",
      "-- Epoch 19\n",
      "Norm: 1.06, NNZs: 300, Bias: -1.986522, T: 200697, Avg. loss: 0.075931\n",
      "Total training time: 0.06 seconds.\n",
      "Convergence after 19 epochs took 0.06 seconds\n",
      "-- Epoch 1\n",
      "Norm: 4.26, NNZs: 300, Bias: -2.322987, T: 10563, Avg. loss: 0.246691\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 3.49, NNZs: 300, Bias: -1.664272, T: 21126, Avg. loss: 0.092868\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 2.86, NNZs: 300, Bias: -1.384351, T: 31689, Avg. loss: 0.081556\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 2.58, NNZs: 300, Bias: -1.161919, T: 42252, Avg. loss: 0.075681\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 2.35, NNZs: 300, Bias: -1.062012, T: 52815, Avg. loss: 0.073505\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 2.19, NNZs: 300, Bias: -1.026428, T: 63378, Avg. loss: 0.071322\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 2.08, NNZs: 300, Bias: -1.024431, T: 73941, Avg. loss: 0.069892\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 8\n",
      "Norm: 1.99, NNZs: 300, Bias: -1.011894, T: 84504, Avg. loss: 0.069242\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 9\n",
      "Norm: 1.91, NNZs: 300, Bias: -1.011688, T: 95067, Avg. loss: 0.068408\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 10\n",
      "Norm: 1.84, NNZs: 300, Bias: -1.001531, T: 105630, Avg. loss: 0.068015\n",
      "Total training time: 0.04 seconds.\n",
      "-- Epoch 11\n",
      "Norm: 1.78, NNZs: 300, Bias: -1.016908, T: 116193, Avg. loss: 0.067503\n",
      "Total training time: 0.04 seconds.\n",
      "-- Epoch 12\n",
      "Norm: 1.73, NNZs: 300, Bias: -1.008009, T: 126756, Avg. loss: 0.067041\n",
      "Total training time: 0.04 seconds.\n",
      "Convergence after 12 epochs took 0.04 seconds\n",
      "-- Epoch 1\n",
      "Norm: 3.18, NNZs: 300, Bias: -4.214548, T: 10563, Avg. loss: 0.072249\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 2.48, NNZs: 300, Bias: -3.660604, T: 21126, Avg. loss: 0.030460\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 2.18, NNZs: 300, Bias: -3.171871, T: 31689, Avg. loss: 0.027650\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 1.90, NNZs: 300, Bias: -2.986036, T: 42252, Avg. loss: 0.025562\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 1.71, NNZs: 300, Bias: -2.792264, T: 52815, Avg. loss: 0.024945\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 1.62, NNZs: 300, Bias: -2.615143, T: 63378, Avg. loss: 0.024180\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 1.55, NNZs: 300, Bias: -2.454383, T: 73941, Avg. loss: 0.023718\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 8\n",
      "Norm: 1.44, NNZs: 300, Bias: -2.356102, T: 84504, Avg. loss: 0.023641\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 9\n",
      "Norm: 1.37, NNZs: 300, Bias: -2.255485, T: 95067, Avg. loss: 0.023338\n",
      "Total training time: 0.03 seconds.\n",
      "Convergence after 9 epochs took 0.03 seconds\n",
      "-- Epoch 1\n",
      "Norm: 1.28, NNZs: 300, Bias: -3.798570, T: 10563, Avg. loss: 0.052050\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 1.04, NNZs: 300, Bias: -3.295657, T: 21126, Avg. loss: 0.017691\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 0.94, NNZs: 300, Bias: -2.950201, T: 31689, Avg. loss: 0.015040\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 0.88, NNZs: 300, Bias: -2.732956, T: 42252, Avg. loss: 0.013906\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 0.84, NNZs: 300, Bias: -2.544003, T: 52815, Avg. loss: 0.013355\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 0.75, NNZs: 300, Bias: -2.423876, T: 63378, Avg. loss: 0.012860\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 0.73, NNZs: 300, Bias: -2.292541, T: 73941, Avg. loss: 0.012621\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 8\n",
      "Norm: 0.66, NNZs: 300, Bias: -2.204279, T: 84504, Avg. loss: 0.012369\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 9\n",
      "Norm: 0.64, NNZs: 300, Bias: -2.104114, T: 95067, Avg. loss: 0.012151\n",
      "Total training time: 0.03 seconds.\n",
      "Convergence after 9 epochs took 0.03 seconds\n",
      "-- Epoch 1\n",
      "Norm: 3.08, NNZs: 300, Bias: -5.385210, T: 10563, Avg. loss: 0.126549\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 2.63, NNZs: 300, Bias: -4.734495, T: 21126, Avg. loss: 0.042039\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 2.59, NNZs: 300, Bias: -4.279589, T: 31689, Avg. loss: 0.036569\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 2.44, NNZs: 300, Bias: -3.964440, T: 42252, Avg. loss: 0.034062\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 2.31, NNZs: 300, Bias: -3.758685, T: 52815, Avg. loss: 0.032531\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 2.23, NNZs: 300, Bias: -3.581697, T: 63378, Avg. loss: 0.031793\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 2.23, NNZs: 300, Bias: -3.404670, T: 73941, Avg. loss: 0.030704\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 8\n",
      "Norm: 2.15, NNZs: 300, Bias: -3.277668, T: 84504, Avg. loss: 0.030728\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 9\n",
      "Norm: 2.15, NNZs: 300, Bias: -3.155124, T: 95067, Avg. loss: 0.030019\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 10\n",
      "Norm: 2.12, NNZs: 300, Bias: -3.064001, T: 105630, Avg. loss: 0.029874\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 11\n",
      "Norm: 2.09, NNZs: 300, Bias: -2.972637, T: 116193, Avg. loss: 0.029851\n",
      "Total training time: 0.04 seconds.\n",
      "-- Epoch 12\n",
      "Norm: 2.08, NNZs: 300, Bias: -2.890120, T: 126756, Avg. loss: 0.029292\n",
      "Total training time: 0.04 seconds.\n",
      "Convergence after 12 epochs took 0.04 seconds\n",
      "-- Epoch 1\n",
      "Norm: 2.06, NNZs: 300, Bias: -3.909274, T: 10563, Avg. loss: 0.071023\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 1.54, NNZs: 300, Bias: -3.421047, T: 21126, Avg. loss: 0.025763\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 1.43, NNZs: 300, Bias: -3.012397, T: 31689, Avg. loss: 0.021406\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 1.29, NNZs: 300, Bias: -2.766161, T: 42252, Avg. loss: 0.019833\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 1.21, NNZs: 300, Bias: -2.574760, T: 52815, Avg. loss: 0.018680\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 1.14, NNZs: 300, Bias: -2.422164, T: 63378, Avg. loss: 0.018196\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 1.08, NNZs: 300, Bias: -2.305436, T: 73941, Avg. loss: 0.017601\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 8\n",
      "Norm: 1.06, NNZs: 300, Bias: -2.166936, T: 84504, Avg. loss: 0.017441\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 9\n",
      "Norm: 1.01, NNZs: 300, Bias: -2.078105, T: 95067, Avg. loss: 0.017118\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 10\n",
      "Norm: 0.97, NNZs: 300, Bias: -1.998712, T: 105630, Avg. loss: 0.016872\n",
      "Total training time: 0.03 seconds.\n",
      "Convergence after 10 epochs took 0.03 seconds\n",
      "-- Epoch 1\n",
      "Norm: 3.59, NNZs: 300, Bias: -2.418763, T: 10563, Avg. loss: 0.218712\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 2.46, NNZs: 300, Bias: -1.831669, T: 21126, Avg. loss: 0.096342\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 2.18, NNZs: 300, Bias: -1.536186, T: 31689, Avg. loss: 0.083159\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 1.93, NNZs: 300, Bias: -1.409176, T: 42252, Avg. loss: 0.078264\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 1.74, NNZs: 300, Bias: -1.260363, T: 52815, Avg. loss: 0.075559\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 1.60, NNZs: 300, Bias: -1.175462, T: 63378, Avg. loss: 0.073605\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 1.47, NNZs: 300, Bias: -1.115726, T: 73941, Avg. loss: 0.072636\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 8\n",
      "Norm: 1.37, NNZs: 300, Bias: -1.076594, T: 84504, Avg. loss: 0.071530\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 9\n",
      "Norm: 1.31, NNZs: 300, Bias: -1.053865, T: 95067, Avg. loss: 0.070705\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 10\n",
      "Norm: 1.27, NNZs: 300, Bias: -1.034920, T: 105630, Avg. loss: 0.069977\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 11\n",
      "Norm: 1.21, NNZs: 300, Bias: -1.016960, T: 116193, Avg. loss: 0.069883\n",
      "Total training time: 0.04 seconds.\n",
      "-- Epoch 12\n",
      "Norm: 1.15, NNZs: 300, Bias: -1.016101, T: 126756, Avg. loss: 0.069612\n",
      "Total training time: 0.04 seconds.\n",
      "-- Epoch 13\n",
      "Norm: 1.11, NNZs: 300, Bias: -0.993642, T: 137319, Avg. loss: 0.069152\n",
      "Total training time: 0.04 seconds.\n",
      "Convergence after 13 epochs took 0.04 seconds\n",
      "-- Epoch 1\n",
      "Norm: 1.65, NNZs: 300, Bias: -5.183393, T: 10563, Avg. loss: 0.028285\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 1.62, NNZs: 300, Bias: -4.760055, T: 21126, Avg. loss: 0.011594\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 1.48, NNZs: 300, Bias: -4.539535, T: 31689, Avg. loss: 0.009889\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 1.44, NNZs: 300, Bias: -4.351754, T: 42252, Avg. loss: 0.009356\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 1.35, NNZs: 300, Bias: -4.248820, T: 52815, Avg. loss: 0.008720\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 1.28, NNZs: 300, Bias: -4.143544, T: 63378, Avg. loss: 0.008803\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 1.28, NNZs: 300, Bias: -4.040109, T: 73941, Avg. loss: 0.008452\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 8\n",
      "Norm: 1.26, NNZs: 300, Bias: -3.952195, T: 84504, Avg. loss: 0.008386\n",
      "Total training time: 0.03 seconds.\n",
      "Convergence after 8 epochs took 0.03 seconds\n",
      "-- Epoch 1\n",
      "Norm: 3.46, NNZs: 300, Bias: -2.638665, T: 10563, Avg. loss: 0.234008\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 2.67, NNZs: 300, Bias: -2.137620, T: 21126, Avg. loss: 0.083844\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 2.39, NNZs: 300, Bias: -1.920808, T: 31689, Avg. loss: 0.071848\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 2.15, NNZs: 300, Bias: -1.685748, T: 42252, Avg. loss: 0.067797\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 1.95, NNZs: 300, Bias: -1.647843, T: 52815, Avg. loss: 0.065012\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 1.83, NNZs: 300, Bias: -1.539166, T: 63378, Avg. loss: 0.063190\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 1.73, NNZs: 300, Bias: -1.454246, T: 73941, Avg. loss: 0.061992\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 8\n",
      "Norm: 1.63, NNZs: 300, Bias: -1.394315, T: 84504, Avg. loss: 0.061405\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 9\n",
      "Norm: 1.56, NNZs: 300, Bias: -1.340578, T: 95067, Avg. loss: 0.060622\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 10\n",
      "Norm: 1.49, NNZs: 300, Bias: -1.329848, T: 105630, Avg. loss: 0.060143\n",
      "Total training time: 0.04 seconds.\n",
      "-- Epoch 11\n",
      "Norm: 1.43, NNZs: 300, Bias: -1.284820, T: 116193, Avg. loss: 0.059858\n",
      "Total training time: 0.04 seconds.\n",
      "-- Epoch 12\n",
      "Norm: 1.38, NNZs: 300, Bias: -1.275799, T: 126756, Avg. loss: 0.059453\n",
      "Total training time: 0.04 seconds.\n",
      "Convergence after 12 epochs took 0.04 seconds\n",
      "-- Epoch 1\n",
      "Norm: 2.31, NNZs: 300, Bias: -2.860000, T: 10563, Avg. loss: 0.072645\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 1.69, NNZs: 300, Bias: -2.158745, T: 21126, Avg. loss: 0.031222\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 1.53, NNZs: 300, Bias: -1.780417, T: 31689, Avg. loss: 0.025955\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 1.33, NNZs: 300, Bias: -1.569602, T: 42252, Avg. loss: 0.024446\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 1.20, NNZs: 300, Bias: -1.444090, T: 52815, Avg. loss: 0.023319\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 1.13, NNZs: 300, Bias: -1.306857, T: 63378, Avg. loss: 0.022506\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 1.07, NNZs: 300, Bias: -1.221744, T: 73941, Avg. loss: 0.022148\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 8\n",
      "Norm: 1.02, NNZs: 300, Bias: -1.172284, T: 84504, Avg. loss: 0.021758\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 9\n",
      "Norm: 0.96, NNZs: 300, Bias: -1.138672, T: 95067, Avg. loss: 0.021751\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 10\n",
      "Norm: 0.92, NNZs: 300, Bias: -1.069403, T: 105630, Avg. loss: 0.021441\n",
      "Total training time: 0.03 seconds.\n",
      "Convergence after 10 epochs took 0.03 seconds\n",
      "-- Epoch 1\n",
      "Norm: 1.83, NNZs: 300, Bias: -4.620670, T: 10563, Avg. loss: 0.106124\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 1.29, NNZs: 300, Bias: -4.044520, T: 21126, Avg. loss: 0.037572\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 1.21, NNZs: 300, Bias: -3.580206, T: 31689, Avg. loss: 0.031059\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 1.08, NNZs: 300, Bias: -3.280576, T: 42252, Avg. loss: 0.028719\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 0.98, NNZs: 300, Bias: -3.071352, T: 52815, Avg. loss: 0.026935\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 0.92, NNZs: 300, Bias: -2.883087, T: 63378, Avg. loss: 0.026198\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 0.89, NNZs: 300, Bias: -2.710161, T: 73941, Avg. loss: 0.025529\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 8\n",
      "Norm: 0.82, NNZs: 300, Bias: -2.585312, T: 84504, Avg. loss: 0.024865\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 9\n",
      "Norm: 0.78, NNZs: 300, Bias: -2.474190, T: 95067, Avg. loss: 0.024391\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 10\n",
      "Norm: 0.76, NNZs: 300, Bias: -2.354779, T: 105630, Avg. loss: 0.024170\n",
      "Total training time: 0.03 seconds.\n",
      "Convergence after 10 epochs took 0.03 seconds\n",
      "-- Epoch 1\n",
      "Norm: 2.85, NNZs: 300, Bias: -1.030113, T: 10563, Avg. loss: 0.431839\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 2.00, NNZs: 300, Bias: -0.907764, T: 21126, Avg. loss: 0.182008\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 1.74, NNZs: 300, Bias: -0.922353, T: 31689, Avg. loss: 0.157037\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 1.46, NNZs: 300, Bias: -1.003043, T: 42252, Avg. loss: 0.147690\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 1.32, NNZs: 300, Bias: -0.952199, T: 52815, Avg. loss: 0.142229\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 1.16, NNZs: 300, Bias: -1.005608, T: 63378, Avg. loss: 0.138790\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 1.12, NNZs: 300, Bias: -1.018965, T: 73941, Avg. loss: 0.135908\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 8\n",
      "Norm: 1.02, NNZs: 300, Bias: -1.005196, T: 84504, Avg. loss: 0.134613\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 9\n",
      "Norm: 0.98, NNZs: 300, Bias: -0.992552, T: 95067, Avg. loss: 0.132951\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 10\n",
      "Norm: 0.90, NNZs: 300, Bias: -1.021549, T: 105630, Avg. loss: 0.132166\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 11\n",
      "Norm: 0.87, NNZs: 300, Bias: -1.020218, T: 116193, Avg. loss: 0.131064\n",
      "Total training time: 0.04 seconds.\n",
      "-- Epoch 12\n",
      "Norm: 0.83, NNZs: 300, Bias: -0.978920, T: 126756, Avg. loss: 0.130409\n",
      "Total training time: 0.04 seconds.\n",
      "-- Epoch 13\n",
      "Norm: 0.79, NNZs: 300, Bias: -1.001542, T: 137319, Avg. loss: 0.129873\n",
      "Total training time: 0.04 seconds.\n",
      "-- Epoch 14\n",
      "Norm: 0.76, NNZs: 300, Bias: -0.987296, T: 147882, Avg. loss: 0.129311\n",
      "Total training time: 0.05 seconds.\n",
      "-- Epoch 15\n",
      "Norm: 0.74, NNZs: 300, Bias: -0.993867, T: 158445, Avg. loss: 0.128837\n",
      "Total training time: 0.05 seconds.\n",
      "-- Epoch 16\n",
      "Norm: 0.72, NNZs: 300, Bias: -1.006547, T: 169008, Avg. loss: 0.128348\n",
      "Total training time: 0.05 seconds.\n",
      "Convergence after 16 epochs took 0.05 seconds\n",
      "-- Epoch 1\n",
      "Norm: 2.84, NNZs: 300, Bias: -4.508263, T: 10563, Avg. loss: 0.183090\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 2.07, NNZs: 300, Bias: -3.497409, T: 21126, Avg. loss: 0.070529\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 1.86, NNZs: 300, Bias: -2.952309, T: 31689, Avg. loss: 0.057611\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 1.56, NNZs: 300, Bias: -2.618396, T: 42252, Avg. loss: 0.053712\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 1.49, NNZs: 300, Bias: -2.299664, T: 52815, Avg. loss: 0.050491\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 1.36, NNZs: 300, Bias: -2.077462, T: 63378, Avg. loss: 0.049023\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 1.28, NNZs: 300, Bias: -1.903902, T: 73941, Avg. loss: 0.047483\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 8\n",
      "Norm: 1.19, NNZs: 300, Bias: -1.790220, T: 84504, Avg. loss: 0.046588\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 9\n",
      "Norm: 1.12, NNZs: 300, Bias: -1.678318, T: 95067, Avg. loss: 0.046000\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 10\n",
      "Norm: 1.06, NNZs: 300, Bias: -1.578686, T: 105630, Avg. loss: 0.045367\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 11\n",
      "Norm: 1.01, NNZs: 300, Bias: -1.488347, T: 116193, Avg. loss: 0.044854\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 12\n",
      "Norm: 0.98, NNZs: 300, Bias: -1.423137, T: 126756, Avg. loss: 0.044435\n",
      "Total training time: 0.04 seconds.\n",
      "Convergence after 12 epochs took 0.04 seconds\n",
      "-- Epoch 1\n",
      "Norm: 2.69, NNZs: 300, Bias: -4.696125, T: 10563, Avg. loss: 0.218413\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 2.08, NNZs: 300, Bias: -3.400584, T: 21126, Avg. loss: 0.092708\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 1.82, NNZs: 300, Bias: -2.648314, T: 31689, Avg. loss: 0.077771\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 1.63, NNZs: 300, Bias: -2.190328, T: 42252, Avg. loss: 0.070993\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 1.58, NNZs: 300, Bias: -1.877302, T: 52815, Avg. loss: 0.066586\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 1.45, NNZs: 300, Bias: -1.638457, T: 63378, Avg. loss: 0.064945\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 1.32, NNZs: 300, Bias: -1.462834, T: 73941, Avg. loss: 0.063639\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 8\n",
      "Norm: 1.22, NNZs: 300, Bias: -1.336665, T: 84504, Avg. loss: 0.062498\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 9\n",
      "Norm: 1.17, NNZs: 300, Bias: -1.225191, T: 95067, Avg. loss: 0.061224\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 10\n",
      "Norm: 1.09, NNZs: 300, Bias: -1.156172, T: 105630, Avg. loss: 0.060972\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 11\n",
      "Norm: 1.04, NNZs: 300, Bias: -1.120155, T: 116193, Avg. loss: 0.060250\n",
      "Total training time: 0.04 seconds.\n",
      "-- Epoch 12\n",
      "Norm: 1.00, NNZs: 300, Bias: -1.046138, T: 126756, Avg. loss: 0.059828\n",
      "Total training time: 0.04 seconds.\n",
      "-- Epoch 13\n",
      "Norm: 0.96, NNZs: 300, Bias: -1.016138, T: 137319, Avg. loss: 0.059545\n",
      "Total training time: 0.04 seconds.\n",
      "-- Epoch 14\n",
      "Norm: 0.92, NNZs: 300, Bias: -1.009235, T: 147882, Avg. loss: 0.059328\n",
      "Total training time: 0.05 seconds.\n",
      "Convergence after 14 epochs took 0.05 seconds\n",
      "-- Epoch 1\n",
      "Norm: 2.27, NNZs: 300, Bias: -2.091641, T: 10563, Avg. loss: 0.121401\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 1.49, NNZs: 300, Bias: -1.684344, T: 21126, Avg. loss: 0.049697\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 1.15, NNZs: 300, Bias: -1.276562, T: 31689, Avg. loss: 0.043333\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 0.94, NNZs: 300, Bias: -1.092746, T: 42252, Avg. loss: 0.040296\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 0.85, NNZs: 300, Bias: -1.026263, T: 52815, Avg. loss: 0.038336\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 0.76, NNZs: 300, Bias: -0.975210, T: 63378, Avg. loss: 0.037468\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 0.71, NNZs: 300, Bias: -0.976519, T: 73941, Avg. loss: 0.036704\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 8\n",
      "Norm: 0.70, NNZs: 300, Bias: -0.976401, T: 84504, Avg. loss: 0.035998\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 9\n",
      "Norm: 0.64, NNZs: 300, Bias: -0.987193, T: 95067, Avg. loss: 0.035965\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 10\n",
      "Norm: 0.63, NNZs: 300, Bias: -0.987930, T: 105630, Avg. loss: 0.035482\n",
      "Total training time: 0.03 seconds.\n",
      "Convergence after 10 epochs took 0.03 seconds\n",
      "-- Epoch 1\n",
      "Norm: 3.49, NNZs: 300, Bias: -6.888889, T: 10563, Avg. loss: 0.186076\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 2.92, NNZs: 300, Bias: -5.923741, T: 21126, Avg. loss: 0.073489\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 2.82, NNZs: 300, Bias: -5.327620, T: 31689, Avg. loss: 0.064567\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 2.54, NNZs: 300, Bias: -4.995769, T: 42252, Avg. loss: 0.060664\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 2.48, NNZs: 300, Bias: -4.657914, T: 52815, Avg. loss: 0.057273\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 2.40, NNZs: 300, Bias: -4.449480, T: 63378, Avg. loss: 0.056300\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 2.35, NNZs: 300, Bias: -4.259161, T: 73941, Avg. loss: 0.054885\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 8\n",
      "Norm: 2.29, NNZs: 300, Bias: -4.093517, T: 84504, Avg. loss: 0.054159\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 9\n",
      "Norm: 2.29, NNZs: 300, Bias: -3.926825, T: 95067, Avg. loss: 0.053171\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 10\n",
      "Norm: 2.25, NNZs: 300, Bias: -3.815856, T: 105630, Avg. loss: 0.052624\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 11\n",
      "Norm: 2.24, NNZs: 300, Bias: -3.681033, T: 116193, Avg. loss: 0.052495\n",
      "Total training time: 0.04 seconds.\n",
      "-- Epoch 12\n",
      "Norm: 2.22, NNZs: 300, Bias: -3.576459, T: 126756, Avg. loss: 0.051693\n",
      "Total training time: 0.04 seconds.\n",
      "Convergence after 12 epochs took 0.04 seconds\n",
      "-- Epoch 1\n",
      "Norm: 3.14, NNZs: 300, Bias: 0.392626, T: 10563, Avg. loss: 0.620587\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 2.02, NNZs: 300, Bias: -0.167211, T: 21126, Avg. loss: 0.259675\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 1.72, NNZs: 300, Bias: -0.567679, T: 31689, Avg. loss: 0.227498\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 1.52, NNZs: 300, Bias: -0.718396, T: 42252, Avg. loss: 0.214315\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 1.32, NNZs: 300, Bias: -0.775436, T: 52815, Avg. loss: 0.206988\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 1.15, NNZs: 300, Bias: -0.843733, T: 63378, Avg. loss: 0.202508\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 1.04, NNZs: 300, Bias: -0.840684, T: 73941, Avg. loss: 0.198918\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 8\n",
      "Norm: 1.01, NNZs: 300, Bias: -0.877031, T: 84504, Avg. loss: 0.196250\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 9\n",
      "Norm: 0.92, NNZs: 300, Bias: -0.865070, T: 95067, Avg. loss: 0.194773\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 10\n",
      "Norm: 0.88, NNZs: 300, Bias: -0.894949, T: 105630, Avg. loss: 0.193045\n",
      "Total training time: 0.04 seconds.\n",
      "-- Epoch 11\n",
      "Norm: 0.82, NNZs: 300, Bias: -0.905911, T: 116193, Avg. loss: 0.192143\n",
      "Total training time: 0.04 seconds.\n",
      "-- Epoch 12\n",
      "Norm: 0.78, NNZs: 300, Bias: -0.898608, T: 126756, Avg. loss: 0.191035\n",
      "Total training time: 0.04 seconds.\n",
      "-- Epoch 13\n",
      "Norm: 0.74, NNZs: 300, Bias: -0.929523, T: 137319, Avg. loss: 0.190302\n",
      "Total training time: 0.05 seconds.\n",
      "-- Epoch 14\n",
      "Norm: 0.72, NNZs: 300, Bias: -0.928962, T: 147882, Avg. loss: 0.189448\n",
      "Total training time: 0.05 seconds.\n",
      "-- Epoch 15\n",
      "Norm: 0.71, NNZs: 300, Bias: -0.928488, T: 158445, Avg. loss: 0.188759\n",
      "Total training time: 0.06 seconds.\n",
      "-- Epoch 16\n",
      "Norm: 0.68, NNZs: 300, Bias: -0.916556, T: 169008, Avg. loss: 0.188507\n",
      "Total training time: 0.06 seconds.\n",
      "-- Epoch 17\n",
      "Norm: 0.66, NNZs: 300, Bias: -0.911697, T: 179571, Avg. loss: 0.187915\n",
      "Total training time: 0.06 seconds.\n",
      "Convergence after 17 epochs took 0.06 seconds\n",
      "-- Epoch 1\n",
      "Norm: 2.41, NNZs: 300, Bias: -4.478858, T: 10563, Avg. loss: 0.053012\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 1.94, NNZs: 300, Bias: -3.905089, T: 21126, Avg. loss: 0.028441\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 1.65, NNZs: 300, Bias: -3.640218, T: 31689, Avg. loss: 0.024188\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 1.51, NNZs: 300, Bias: -3.426116, T: 42252, Avg. loss: 0.022563\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 1.42, NNZs: 300, Bias: -3.239033, T: 52815, Avg. loss: 0.021795\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 1.30, NNZs: 300, Bias: -3.120251, T: 63378, Avg. loss: 0.021206\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 1.22, NNZs: 300, Bias: -3.004523, T: 73941, Avg. loss: 0.020938\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 8\n",
      "Norm: 1.15, NNZs: 300, Bias: -2.902901, T: 84504, Avg. loss: 0.020637\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 9\n",
      "Norm: 1.10, NNZs: 300, Bias: -2.823383, T: 95067, Avg. loss: 0.020094\n",
      "Total training time: 0.03 seconds.\n",
      "Convergence after 9 epochs took 0.03 seconds\n",
      "-- Epoch 1\n",
      "Norm: 1.94, NNZs: 300, Bias: -4.774374, T: 10563, Avg. loss: 0.054344\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 1.61, NNZs: 300, Bias: -4.119813, T: 21126, Avg. loss: 0.024145\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 1.37, NNZs: 300, Bias: -3.850028, T: 31689, Avg. loss: 0.020280\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 1.34, NNZs: 300, Bias: -3.578472, T: 42252, Avg. loss: 0.018928\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 1.20, NNZs: 300, Bias: -3.412234, T: 52815, Avg. loss: 0.018235\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 1.20, NNZs: 300, Bias: -3.243194, T: 63378, Avg. loss: 0.017280\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 1.15, NNZs: 300, Bias: -3.128220, T: 73941, Avg. loss: 0.016755\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 8\n",
      "Norm: 1.09, NNZs: 300, Bias: -3.026093, T: 84504, Avg. loss: 0.016514\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 9\n",
      "Norm: 1.08, NNZs: 300, Bias: -2.916391, T: 95067, Avg. loss: 0.016335\n",
      "Total training time: 0.03 seconds.\n",
      "Convergence after 9 epochs took 0.03 seconds\n",
      "-- Epoch 1\n",
      "Norm: 3.34, NNZs: 300, Bias: -1.047389, T: 10563, Avg. loss: 0.306914\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 2.44, NNZs: 300, Bias: -0.831446, T: 21126, Avg. loss: 0.120141\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 2.02, NNZs: 300, Bias: -0.780079, T: 31689, Avg. loss: 0.104093\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 1.83, NNZs: 300, Bias: -0.790346, T: 42252, Avg. loss: 0.096545\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 1.66, NNZs: 300, Bias: -0.823454, T: 52815, Avg. loss: 0.093302\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 1.55, NNZs: 300, Bias: -0.850621, T: 63378, Avg. loss: 0.090854\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 1.43, NNZs: 300, Bias: -0.853814, T: 73941, Avg. loss: 0.089392\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 8\n",
      "Norm: 1.38, NNZs: 300, Bias: -0.905180, T: 84504, Avg. loss: 0.087880\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 9\n",
      "Norm: 1.33, NNZs: 300, Bias: -0.925233, T: 95067, Avg. loss: 0.087092\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 10\n",
      "Norm: 1.27, NNZs: 300, Bias: -0.935418, T: 105630, Avg. loss: 0.086438\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 11\n",
      "Norm: 1.21, NNZs: 300, Bias: -0.936546, T: 116193, Avg. loss: 0.085979\n",
      "Total training time: 0.04 seconds.\n",
      "-- Epoch 12\n",
      "Norm: 1.18, NNZs: 300, Bias: -0.929378, T: 126756, Avg. loss: 0.085236\n",
      "Total training time: 0.04 seconds.\n",
      "-- Epoch 13\n",
      "Norm: 1.15, NNZs: 300, Bias: -0.951369, T: 137319, Avg. loss: 0.084884\n",
      "Total training time: 0.04 seconds.\n",
      "Convergence after 13 epochs took 0.04 seconds\n",
      "-- Epoch 1\n",
      "Norm: 1.22, NNZs: 300, Bias: -3.772727, T: 10563, Avg. loss: 0.123545\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 1.27, NNZs: 300, Bias: -2.857602, T: 21126, Avg. loss: 0.045297\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 0.94, NNZs: 300, Bias: -2.563856, T: 31689, Avg. loss: 0.037239\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 0.77, NNZs: 300, Bias: -2.264219, T: 42252, Avg. loss: 0.034358\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 0.78, NNZs: 300, Bias: -2.035885, T: 52815, Avg. loss: 0.031878\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 0.67, NNZs: 300, Bias: -1.916243, T: 63378, Avg. loss: 0.030797\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 0.63, NNZs: 300, Bias: -1.755431, T: 73941, Avg. loss: 0.030102\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 8\n",
      "Norm: 0.62, NNZs: 300, Bias: -1.654516, T: 84504, Avg. loss: 0.029142\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 9\n",
      "Norm: 0.56, NNZs: 300, Bias: -1.577261, T: 95067, Avg. loss: 0.028867\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 10\n",
      "Norm: 0.53, NNZs: 300, Bias: -1.487966, T: 105630, Avg. loss: 0.028451\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 11\n",
      "Norm: 0.48, NNZs: 300, Bias: -1.433257, T: 116193, Avg. loss: 0.028155\n",
      "Total training time: 0.04 seconds.\n",
      "Convergence after 11 epochs took 0.04 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  40 out of  40 | elapsed:    1.5s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SGDClassifier(alpha=0.001, random_state=7600, verbose=1)"
      ]
     },
     "execution_count": 292,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_w2v_sgd.fit(X_train_w2v_transformed, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.35012149326264635"
      ]
     },
     "execution_count": 293,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_test, clf_w2v_sgd.predict(X_test_w2v_transformed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        13\n",
      "           1       0.46      0.50      0.48       232\n",
      "           2       0.53      0.78      0.63       157\n",
      "           3       0.62      0.30      0.40       273\n",
      "           4       0.00      0.00      0.00        64\n",
      "           5       0.20      0.01      0.02       111\n",
      "           6       0.63      0.28      0.39        96\n",
      "           7       0.40      0.03      0.06        67\n",
      "           8       0.00      0.00      0.00        36\n",
      "           9       0.51      0.48      0.50        56\n",
      "          10       0.48      0.41      0.44       141\n",
      "          11       0.00      0.00      0.00        54\n",
      "          12       0.69      0.65      0.67        79\n",
      "          13       0.66      0.84      0.74        81\n",
      "          14       0.00      0.00      0.00        29\n",
      "          15       0.89      0.20      0.33        79\n",
      "          16       0.37      0.63      0.47       544\n",
      "          17       0.00      0.00      0.00        75\n",
      "          18       0.61      0.73      0.67        93\n",
      "          19       0.67      0.01      0.03       154\n",
      "          20       0.60      0.25      0.35       146\n",
      "          21       0.71      0.10      0.18        50\n",
      "          22       0.00      0.00      0.00        23\n",
      "          23       0.79      0.25      0.38        75\n",
      "          24       0.42      0.30      0.35        27\n",
      "          25       0.45      0.41      0.43       173\n",
      "          26       0.57      0.31      0.40        13\n",
      "          27       0.76      0.21      0.33       118\n",
      "          28       0.48      0.54      0.51        39\n",
      "          29       0.10      0.19      0.14        36\n",
      "          30       0.14      0.47      0.22       290\n",
      "          31       0.24      0.61      0.35        95\n",
      "          32       0.45      0.07      0.13       134\n",
      "          33       1.00      0.06      0.11        66\n",
      "          34       0.29      0.83      0.43        81\n",
      "          35       0.23      0.30      0.26       408\n",
      "          36       0.40      0.10      0.16        41\n",
      "          37       0.22      0.06      0.09        35\n",
      "          38       0.46      0.04      0.07       170\n",
      "          39       0.00      0.00      0.00        73\n",
      "\n",
      "    accuracy                           0.35      4527\n",
      "   macro avg       0.40      0.27      0.27      4527\n",
      "weighted avg       0.42      0.35      0.32      4527\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Kackie\\anaconda3\\envs\\DataScience\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\Kackie\\anaconda3\\envs\\DataScience\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\Kackie\\anaconda3\\envs\\DataScience\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, clf_w2v_sgd.predict(X_test_w2v_transformed)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_w2v_lr = LogisticRegression(\n",
    "    max_iter=200,\n",
    "    random_state=7600,\n",
    "    verbose=1,\n",
    "    n_jobs=cpu_count()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=16)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=16)]: Done   1 out of   1 | elapsed:    9.1s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(max_iter=200, n_jobs=16, random_state=7600, verbose=1)"
      ]
     },
     "execution_count": 297,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_w2v_lr.fit(X_train_w2v_transformed, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.46277888226198366"
      ]
     },
     "execution_count": 298,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_test, clf_w2v_lr.predict(X_test_w2v_transformed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        13\n",
      "           1       0.46      0.58      0.51       232\n",
      "           2       0.59      0.78      0.67       157\n",
      "           3       0.48      0.52      0.50       273\n",
      "           4       0.47      0.27      0.34        64\n",
      "           5       0.38      0.36      0.37       111\n",
      "           6       0.57      0.48      0.52        96\n",
      "           7       0.41      0.13      0.20        67\n",
      "           8       0.33      0.08      0.13        36\n",
      "           9       0.60      0.46      0.53        56\n",
      "          10       0.53      0.44      0.48       141\n",
      "          11       0.23      0.11      0.15        54\n",
      "          12       0.59      0.78      0.67        79\n",
      "          13       0.82      0.84      0.83        81\n",
      "          14       0.33      0.10      0.16        29\n",
      "          15       0.75      0.56      0.64        79\n",
      "          16       0.43      0.66      0.52       544\n",
      "          17       0.50      0.27      0.35        75\n",
      "          18       0.70      0.72      0.71        93\n",
      "          19       0.44      0.63      0.52       154\n",
      "          20       0.51      0.45      0.48       146\n",
      "          21       0.67      0.48      0.56        50\n",
      "          22       0.00      0.00      0.00        23\n",
      "          23       0.71      0.49      0.58        75\n",
      "          24       0.73      0.30      0.42        27\n",
      "          25       0.54      0.47      0.50       173\n",
      "          26       0.56      0.38      0.45        13\n",
      "          27       0.47      0.53      0.50       118\n",
      "          28       0.56      0.49      0.52        39\n",
      "          29       0.59      0.28      0.38        36\n",
      "          30       0.33      0.31      0.32       290\n",
      "          31       0.44      0.40      0.42        95\n",
      "          32       0.40      0.31      0.35       134\n",
      "          33       0.50      0.18      0.27        66\n",
      "          34       0.52      0.72      0.60        81\n",
      "          35       0.28      0.36      0.31       408\n",
      "          36       0.52      0.32      0.39        41\n",
      "          37       0.55      0.34      0.42        35\n",
      "          38       0.49      0.24      0.32       170\n",
      "          39       0.14      0.01      0.03        73\n",
      "\n",
      "    accuracy                           0.46      4527\n",
      "   macro avg       0.48      0.40      0.42      4527\n",
      "weighted avg       0.46      0.46      0.45      4527\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Kackie\\anaconda3\\envs\\DataScience\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\Kackie\\anaconda3\\envs\\DataScience\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\Kackie\\anaconda3\\envs\\DataScience\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, clf_w2v_lr.predict(X_test_w2v_transformed)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 3: Doc2Vec + Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.doc2vec import TaggedDocument\n",
    "from gensim.models.doc2vec import Doc2Vec\n",
    "import logging\n",
    "logging.basicConfig(level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15079\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0        34\n",
       "1        10\n",
       "2        38\n",
       "3        11\n",
       "4        32\n",
       "         ..\n",
       "15085     8\n",
       "15086    17\n",
       "15087    18\n",
       "15088    30\n",
       "15089    16\n",
       "Name: cluster, Length: 15079, dtype: int64"
      ]
     },
     "execution_count": 383,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_tokenized_nonempty_indices = X_tokenized[X_tokenized.apply(lambda x: len(x)>0)].index\n",
    "y_nonempty = y[y.index.isin(X_tokenized_nonempty_indices)]\n",
    "print(len(y_nonempty))\n",
    "y_nonempty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15079\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0        [ameria, invest, consult, compani, seek, chief...\n",
       "1        [public, outreach, strengthen, grow, network, ...\n",
       "2        [lead, local, enhanc, develop, health, bcc, sp...\n",
       "3                  [saleswoman, sell, menswear, accessori]\n",
       "4        [armenian, branch, offic, open, societi, insti...\n",
       "                               ...                        \n",
       "15085    [incumb, develop, softwar, applic, work, distr...\n",
       "15086    [incumb, respons, support, director, organ, ac...\n",
       "15087    [tech, startup, technolinguist, base, new, yor...\n",
       "15088    [san, lazzaro, llc, look, individu, work, head...\n",
       "15089    [kamurj, uco, cjsc, look, lawyer, legal, depar...\n",
       "Name: JobDescription, Length: 15079, dtype: object"
      ]
     },
     "execution_count": 384,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# remove the documents that have no tokenized words from the X_tokenized list\n",
    "X_tokenized_nonempty = X_tokenized[X_tokenized.apply(lambda x: len(x)>0)]\n",
    "print(len(X_tokenized_nonempty))\n",
    "X_tokenized_nonempty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TaggedDocument<['ameria', 'invest', 'consult', 'compani', 'seek', 'chief', 'financi', 'offic', 'posit', 'manag', 'compani', 'fiscal', 'administr', 'function', 'provid', 'highli', 'respons', 'technic', 'complex', 'staff', 'assist', 'execut', 'director', 'work', 'perform', 'requir', 'high', 'level', 'technic', 'profici', 'financi', 'manag', 'invest', 'manag', 'well', 'manag', 'supervisori', 'administr', 'skill'], [34]>\n"
     ]
    }
   ],
   "source": [
    "# convert the texts into tagged document format as required by doc2vec\n",
    "tagged = []\n",
    "for i in range(len(X_tokenized_nonempty)):\n",
    "    try:\n",
    "        tagged_document = TaggedDocument(\n",
    "            words = X_tokenized_nonempty[i],\n",
    "            tags = [y[i]]\n",
    "        )\n",
    "        tagged.append(tagged_document)\n",
    "    except:\n",
    "        continue\n",
    "\n",
    "print(tagged[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:gensim.utils:Doc2Vec lifecycle event {'params': 'Doc2Vec<dbow,d300,n5,t16>', 'datetime': '2022-09-27T11:27:16.197366', 'gensim': '4.2.0', 'python': '3.7.13 (default, Mar 28 2022, 08:03:21) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19041-SP0', 'event': 'created'}\n",
      "INFO:gensim.models.doc2vec:collecting all words and their counts\n",
      "INFO:gensim.models.doc2vec:PROGRESS: at example #0, processed 0 words (0 words/s), 0 word types, 0 tags\n",
      "INFO:gensim.models.doc2vec:PROGRESS: at example #10000, processed 228741 words (7898842 words/s), 5953 word types, 0 tags\n",
      "INFO:gensim.models.doc2vec:collected 7165 word types and 40 unique tags from a corpus of 15068 examples and 355957 words\n",
      "INFO:gensim.models.word2vec:Creating a fresh vocabulary\n",
      "INFO:gensim.utils:Doc2Vec lifecycle event {'msg': 'effective_min_count=1 retains 7165 unique words (100.00% of original 7165, drops 0)', 'datetime': '2022-09-27T11:27:16.263931', 'gensim': '4.2.0', 'python': '3.7.13 (default, Mar 28 2022, 08:03:21) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19041-SP0', 'event': 'prepare_vocab'}\n",
      "INFO:gensim.utils:Doc2Vec lifecycle event {'msg': 'effective_min_count=1 leaves 355957 word corpus (100.00% of original 355957, drops 0)', 'datetime': '2022-09-27T11:27:16.264932', 'gensim': '4.2.0', 'python': '3.7.13 (default, Mar 28 2022, 08:03:21) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19041-SP0', 'event': 'prepare_vocab'}\n",
      "INFO:gensim.models.word2vec:deleting the raw counts dictionary of 7165 items\n",
      "INFO:gensim.models.word2vec:sample=0 downsamples 0 most-common words\n",
      "INFO:gensim.utils:Doc2Vec lifecycle event {'msg': 'downsampling leaves estimated 355957 word corpus (100.0%% of prior 355957)', 'datetime': '2022-09-27T11:27:16.291957', 'gensim': '4.2.0', 'python': '3.7.13 (default, Mar 28 2022, 08:03:21) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19041-SP0', 'event': 'prepare_vocab'}\n",
      "INFO:gensim.models.word2vec:estimated required memory for 7165 words and 300 dimensions: 20834500 bytes\n",
      "INFO:gensim.models.word2vec:resetting layer weights\n",
      "INFO:gensim.utils:Doc2Vec lifecycle event {'msg': 'training model with 16 workers on 7165 vocabulary and 300 features, using sg=1 hs=0 sample=0 negative=5 window=5 shrink_windows=True', 'datetime': '2022-09-27T11:27:16.342002', 'gensim': '4.2.0', 'python': '3.7.13 (default, Mar 28 2022, 08:03:21) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19041-SP0', 'event': 'train'}\n",
      "INFO:gensim.models.word2vec:EPOCH 0: training on 355957 raw words (371025 effective words) took 0.4s, 987178 effective words/s\n",
      "INFO:gensim.models.word2vec:EPOCH 1: training on 355957 raw words (371025 effective words) took 0.4s, 980252 effective words/s\n",
      "INFO:gensim.models.word2vec:EPOCH 2: training on 355957 raw words (371025 effective words) took 0.4s, 971678 effective words/s\n",
      "INFO:gensim.models.word2vec:EPOCH 3: training on 355957 raw words (371025 effective words) took 0.4s, 975588 effective words/s\n",
      "INFO:gensim.models.word2vec:EPOCH 4: training on 355957 raw words (371025 effective words) took 0.4s, 980530 effective words/s\n",
      "INFO:gensim.models.word2vec:EPOCH 5: training on 355957 raw words (371025 effective words) took 0.4s, 982260 effective words/s\n",
      "INFO:gensim.models.word2vec:EPOCH 6: training on 355957 raw words (371025 effective words) took 0.4s, 980672 effective words/s\n",
      "INFO:gensim.models.word2vec:EPOCH 7: training on 355957 raw words (371025 effective words) took 0.4s, 968823 effective words/s\n",
      "INFO:gensim.models.word2vec:EPOCH 8: training on 355957 raw words (371025 effective words) took 0.4s, 978409 effective words/s\n",
      "INFO:gensim.models.word2vec:EPOCH 9: training on 355957 raw words (371025 effective words) took 0.4s, 961944 effective words/s\n",
      "INFO:gensim.models.word2vec:EPOCH 10: training on 355957 raw words (371025 effective words) took 0.4s, 968729 effective words/s\n",
      "INFO:gensim.models.word2vec:EPOCH 11: training on 355957 raw words (371025 effective words) took 0.4s, 971436 effective words/s\n",
      "INFO:gensim.models.word2vec:EPOCH 12: training on 355957 raw words (371025 effective words) took 0.4s, 974502 effective words/s\n",
      "INFO:gensim.models.word2vec:EPOCH 13: training on 355957 raw words (371025 effective words) took 0.4s, 933855 effective words/s\n",
      "INFO:gensim.models.word2vec:EPOCH 14: training on 355957 raw words (371025 effective words) took 0.4s, 953120 effective words/s\n",
      "INFO:gensim.models.word2vec:EPOCH 15: training on 355957 raw words (371025 effective words) took 0.4s, 956294 effective words/s\n",
      "INFO:gensim.models.word2vec:EPOCH 16: training on 355957 raw words (371025 effective words) took 0.4s, 911679 effective words/s\n",
      "INFO:gensim.models.word2vec:EPOCH 17: training on 355957 raw words (371025 effective words) took 0.4s, 951936 effective words/s\n",
      "INFO:gensim.models.word2vec:EPOCH 18: training on 355957 raw words (371025 effective words) took 0.4s, 958160 effective words/s\n",
      "INFO:gensim.models.word2vec:EPOCH 19: training on 355957 raw words (371025 effective words) took 0.4s, 965009 effective words/s\n",
      "INFO:gensim.models.word2vec:EPOCH 20: training on 355957 raw words (371025 effective words) took 0.4s, 976765 effective words/s\n",
      "INFO:gensim.models.word2vec:EPOCH 21: training on 355957 raw words (371025 effective words) took 0.4s, 983854 effective words/s\n",
      "INFO:gensim.models.word2vec:EPOCH 22: training on 355957 raw words (371025 effective words) took 0.4s, 980712 effective words/s\n",
      "INFO:gensim.models.word2vec:EPOCH 23: training on 355957 raw words (371025 effective words) took 0.4s, 971654 effective words/s\n",
      "INFO:gensim.models.word2vec:EPOCH 24: training on 355957 raw words (371025 effective words) took 0.4s, 964334 effective words/s\n",
      "INFO:gensim.models.word2vec:EPOCH 25: training on 355957 raw words (371025 effective words) took 0.4s, 950371 effective words/s\n",
      "INFO:gensim.models.word2vec:EPOCH 26: training on 355957 raw words (371025 effective words) took 0.4s, 967367 effective words/s\n",
      "INFO:gensim.models.word2vec:EPOCH 27: training on 355957 raw words (371025 effective words) took 0.4s, 950555 effective words/s\n",
      "INFO:gensim.models.word2vec:EPOCH 28: training on 355957 raw words (371025 effective words) took 0.4s, 951046 effective words/s\n",
      "INFO:gensim.models.word2vec:EPOCH 29: training on 355957 raw words (371025 effective words) took 0.4s, 972696 effective words/s\n",
      "INFO:gensim.utils:Doc2Vec lifecycle event {'msg': 'training on 10678710 raw words (11130750 effective words) took 12.0s, 931386 effective words/s', 'datetime': '2022-09-27T11:27:28.293226', 'gensim': '4.2.0', 'python': '3.7.13 (default, Mar 28 2022, 08:03:21) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19041-SP0', 'event': 'train'}\n"
     ]
    }
   ],
   "source": [
    "# train the doc2vec model\n",
    "d2v = Doc2Vec(\n",
    "    dm=0,\n",
    "    vector_size=300,\n",
    "    negative=5,\n",
    "    hs=0,\n",
    "    min_count=1,\n",
    "    sample=0,\n",
    "    epochs=30,\n",
    "    workers=cpu_count()\n",
    ")\n",
    "d2v.build_vocab(tagged)\n",
    "d2v.train(tagged, total_examples=d2v.corpus_count, epochs=d2v.epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 499,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:gensim.utils:Doc2Vec lifecycle event {'fname_or_handle': 'models/d2v_online_job_descriptions.model', 'separately': 'None', 'sep_limit': 10485760, 'ignore': frozenset(), 'datetime': '2022-09-27T12:14:34.851723', 'gensim': '4.2.0', 'python': '3.7.13 (default, Mar 28 2022, 08:03:21) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19041-SP0', 'event': 'saving'}\n",
      "INFO:gensim.utils:not storing attribute cum_table\n",
      "INFO:gensim.utils:saved models/d2v_online_job_descriptions.model\n"
     ]
    }
   ],
   "source": [
    "# save the d2v model\n",
    "d2v.save('models/d2v_online_job_descriptions.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 500,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:gensim.utils:loading Doc2Vec object from models/d2v_online_job_descriptions.model\n",
      "INFO:gensim.utils:loading dv recursively from models/d2v_online_job_descriptions.model.dv.* with mmap=None\n",
      "INFO:gensim.utils:loading wv recursively from models/d2v_online_job_descriptions.model.wv.* with mmap=None\n",
      "INFO:gensim.utils:setting ignored attribute cum_table to None\n",
      "INFO:gensim.utils:Doc2Vec lifecycle event {'fname': 'models/d2v_online_job_descriptions.model', 'datetime': '2022-09-27T12:15:02.305732', 'gensim': '4.2.0', 'python': '3.7.13 (default, Mar 28 2022, 08:03:21) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19041-SP0', 'event': 'loaded'}\n"
     ]
    }
   ],
   "source": [
    "# load the saved model\n",
    "temp = Doc2Vec.load('models/d2v_online_job_descriptions.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 428,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split nonempty dataset into train and test datasets\n",
    "X_train_tokenized_nonempty, X_test_tokenized_nonempty, y_train_nonempty, y_test_nonempty = train_test_split(\n",
    "    X_tokenized_nonempty, y_nonempty, test_size=0.3, shuffle=True, random_state=7600\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 429,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_d2v = X_train_tokenized_nonempty.apply(lambda x: d2v.infer_vector(x))\n",
    "X_test_d2v = X_test_tokenized_nonempty.apply(lambda x: d2v.infer_vector(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 430,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_series_to_array(series):\n",
    "    temp = []\n",
    "    for x in series:\n",
    "        temp.append(x.tolist())\n",
    "\n",
    "    return np.array(temp)\n",
    "\n",
    "X_train_d2v = convert_series_to_array(X_train_d2v)\n",
    "X_test_d2v = convert_series_to_array(X_test_d2v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 432,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10555, 300)"
      ]
     },
     "execution_count": 432,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_d2v.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Naive Bayes (baseline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 433,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 438,
   "metadata": {},
   "outputs": [],
   "source": [
    "# since MNB can only do positive values, we need to normalize the word embeddings\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "X_train_d2v_transformed_scaled = MinMaxScaler().fit_transform(X_train_d2v)\n",
    "X_test_d2v_transformed_scaled = MinMaxScaler().fit_transform(X_test_d2v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 439,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_d2v_mnb = MultinomialNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 440,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB()"
      ]
     },
     "execution_count": 440,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_d2v_mnb.fit(X_train_d2v_transformed_scaled, y_train_nonempty)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 441,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4363395225464191"
      ]
     },
     "execution_count": 441,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_test_nonempty, clf_d2v_mnb.predict(X_test_d2v_transformed_scaled))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 442,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         9\n",
      "           1       0.55      0.74      0.63       237\n",
      "           2       0.83      0.57      0.68       172\n",
      "           3       0.43      0.48      0.46       251\n",
      "           4       1.00      0.14      0.25        57\n",
      "           5       0.91      0.17      0.29       115\n",
      "           6       0.80      0.39      0.53        71\n",
      "           7       0.70      0.12      0.20        60\n",
      "           8       0.00      0.00      0.00        39\n",
      "           9       0.73      0.17      0.28        47\n",
      "          10       0.84      0.32      0.46       147\n",
      "          11       1.00      0.13      0.23        46\n",
      "          12       0.91      0.54      0.68        76\n",
      "          13       0.88      0.55      0.68        80\n",
      "          14       0.86      0.26      0.40        23\n",
      "          15       0.75      0.08      0.14        75\n",
      "          16       0.29      0.92      0.45       529\n",
      "          17       1.00      0.31      0.47        81\n",
      "          18       1.00      0.33      0.49        95\n",
      "          19       0.33      0.56      0.42       157\n",
      "          20       0.87      0.44      0.59       152\n",
      "          21       1.00      0.09      0.17        43\n",
      "          22       0.00      0.00      0.00        28\n",
      "          23       1.00      0.25      0.40        73\n",
      "          24       0.00      0.00      0.00        26\n",
      "          25       0.81      0.31      0.45       175\n",
      "          26       1.00      0.36      0.53        14\n",
      "          27       0.88      0.33      0.48       127\n",
      "          28       1.00      0.31      0.48        32\n",
      "          29       1.00      0.13      0.23        46\n",
      "          30       0.77      0.35      0.48       298\n",
      "          31       0.80      0.22      0.34        92\n",
      "          32       0.92      0.26      0.40       140\n",
      "          33       0.90      0.13      0.23        70\n",
      "          34       0.60      0.54      0.57        90\n",
      "          35       0.26      0.62      0.37       427\n",
      "          36       1.00      0.35      0.52        48\n",
      "          37       1.00      0.05      0.10        37\n",
      "          38       1.00      0.09      0.16       167\n",
      "          39       1.00      0.10      0.18        72\n",
      "\n",
      "    accuracy                           0.44      4524\n",
      "   macro avg       0.74      0.29      0.36      4524\n",
      "weighted avg       0.67      0.44      0.42      4524\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Kackie\\anaconda3\\envs\\DataScience\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\Kackie\\anaconda3\\envs\\DataScience\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\Kackie\\anaconda3\\envs\\DataScience\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test_nonempty, clf_d2v_mnb.predict(X_test_d2v_transformed_scaled)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Linear SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 443,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 444,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_d2v_sgd = SGDClassifier(\n",
    "    loss='hinge',\n",
    "    penalty='l2',\n",
    "    alpha=1e-3,\n",
    "    random_state=7600,\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 446,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Epoch 1\n",
      "Norm: 1.59, NNZs: 300, Bias: -4.439741, T: 10555, Avg. loss: 0.014464\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 1.51, NNZs: 300, Bias: -3.980765, T: 21110, Avg. loss: 0.005580\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 1.49, NNZs: 300, Bias: -3.691271, T: 31665, Avg. loss: 0.005033\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 1.36, NNZs: 300, Bias: -3.558167, T: 42220, Avg. loss: 0.004710\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 1.35, NNZs: 300, Bias: -3.406475, T: 52775, Avg. loss: 0.004558\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 1.30, NNZs: 300, Bias: -3.303278, T: 63330, Avg. loss: 0.004508\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 1.30, NNZs: 300, Bias: -3.204145, T: 73885, Avg. loss: 0.004393\n",
      "Total training time: 0.03 seconds.\n",
      "Convergence after 7 epochs took 0.03 seconds\n",
      "-- Epoch 1\n",
      "Norm: 4.72, NNZs: 300, Bias: -0.387570, T: 10555, Avg. loss: 0.147571\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 4.00, NNZs: 300, Bias: 0.174749, T: 21110, Avg. loss: 0.075677\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 3.72, NNZs: 300, Bias: 0.218925, T: 31665, Avg. loss: 0.070666\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 3.64, NNZs: 300, Bias: 0.201827, T: 42220, Avg. loss: 0.069033\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 3.44, NNZs: 300, Bias: 0.203163, T: 52775, Avg. loss: 0.068118\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 3.41, NNZs: 300, Bias: 0.163479, T: 63330, Avg. loss: 0.067287\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 3.34, NNZs: 300, Bias: 0.241643, T: 73885, Avg. loss: 0.067408\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 8\n",
      "Norm: 3.33, NNZs: 300, Bias: 0.239768, T: 84440, Avg. loss: 0.066532\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 9\n",
      "Norm: 3.31, NNZs: 300, Bias: 0.271136, T: 94995, Avg. loss: 0.066321\n",
      "Total training time: 0.03 seconds.\n",
      "Convergence after 9 epochs took 0.03 seconds\n",
      "-- Epoch 1\n",
      "Norm: 4.86, NNZs: 300, Bias: 0.276735, T: 10555, Avg. loss: 0.091933\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 4.17, NNZs: 300, Bias: 0.397855, T: 21110, Avg. loss: 0.049552\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 4.13, NNZs: 300, Bias: 0.357693, T: 31665, Avg. loss: 0.045152\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 3.90, NNZs: 300, Bias: 0.399240, T: 42220, Avg. loss: 0.044805\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 3.86, NNZs: 300, Bias: 0.333524, T: 52775, Avg. loss: 0.044542\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 3.82, NNZs: 300, Bias: 0.304664, T: 63330, Avg. loss: 0.044149\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 3.79, NNZs: 300, Bias: 0.334174, T: 73885, Avg. loss: 0.043665\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 8\n",
      "Norm: 3.80, NNZs: 300, Bias: 0.335454, T: 84440, Avg. loss: 0.043519\n",
      "Total training time: 0.03 seconds.\n",
      "Convergence after 8 epochs took 0.03 seconds\n",
      "-- Epoch 1\n",
      "Norm: 5.19, NNZs: 300, Bias: 0.094848, T: 10555, Avg. loss: 0.205529\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 4.21, NNZs: 300, Bias: 0.578599, T: 21110, Avg. loss: 0.107478\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 4.00, NNZs: 300, Bias: 0.520719, T: 31665, Avg. loss: 0.101704\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 3.74, NNZs: 300, Bias: 0.457063, T: 42220, Avg. loss: 0.099228\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 3.62, NNZs: 300, Bias: 0.418169, T: 52775, Avg. loss: 0.097752\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 3.55, NNZs: 300, Bias: 0.393106, T: 63330, Avg. loss: 0.096680\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 3.55, NNZs: 300, Bias: 0.370573, T: 73885, Avg. loss: 0.096498\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 8\n",
      "Norm: 3.48, NNZs: 300, Bias: 0.411100, T: 84440, Avg. loss: 0.095580\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 9\n",
      "Norm: 3.51, NNZs: 300, Bias: 0.357679, T: 94995, Avg. loss: 0.094657\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 10\n",
      "Norm: 3.43, NNZs: 300, Bias: 0.388952, T: 105550, Avg. loss: 0.094817\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 11\n",
      "Norm: 3.39, NNZs: 300, Bias: 0.361839, T: 116105, Avg. loss: 0.094855\n",
      "Total training time: 0.03 seconds.\n",
      "Convergence after 11 epochs took 0.03 seconds\n",
      "-- Epoch 1\n",
      "Norm: 3.22, NNZs: 300, Bias: -3.020407, T: 10555, Avg. loss: 0.064045\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 2.70, NNZs: 300, Bias: -2.237762, T: 21110, Avg. loss: 0.031199\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 2.45, NNZs: 300, Bias: -1.719478, T: 31665, Avg. loss: 0.028170\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 2.28, NNZs: 300, Bias: -1.495649, T: 42220, Avg. loss: 0.026822\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 2.18, NNZs: 300, Bias: -1.320238, T: 52775, Avg. loss: 0.026447\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 2.12, NNZs: 300, Bias: -1.144389, T: 63330, Avg. loss: 0.025904\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 2.11, NNZs: 300, Bias: -1.016730, T: 73885, Avg. loss: 0.025534\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 8\n",
      "Norm: 2.08, NNZs: 300, Bias: -0.930579, T: 84440, Avg. loss: 0.025399\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 9\n",
      "Norm: 2.05, NNZs: 300, Bias: -0.907689, T: 94995, Avg. loss: 0.025389\n",
      "Total training time: 0.03 seconds.\n",
      "Convergence after 9 epochs took 0.03 seconds\n",
      "-- Epoch 1\n",
      "Norm: 3.61, NNZs: 300, Bias: -2.549367, T: 10555, Avg. loss: 0.092800\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 3.05, NNZs: 300, Bias: -1.644605, T: 21110, Avg. loss: 0.045813\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 2.73, NNZs: 300, Bias: -1.346911, T: 31665, Avg. loss: 0.041240\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 2.54, NNZs: 300, Bias: -1.170002, T: 42220, Avg. loss: 0.039680\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 2.44, NNZs: 300, Bias: -1.123160, T: 52775, Avg. loss: 0.039317\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 2.42, NNZs: 300, Bias: -1.014830, T: 63330, Avg. loss: 0.039065\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 2.41, NNZs: 300, Bias: -0.965046, T: 73885, Avg. loss: 0.038358\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 8\n",
      "Norm: 2.34, NNZs: 300, Bias: -0.863342, T: 84440, Avg. loss: 0.038585\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 9\n",
      "Norm: 2.33, NNZs: 300, Bias: -0.872352, T: 94995, Avg. loss: 0.037978\n",
      "Total training time: 0.03 seconds.\n",
      "Convergence after 9 epochs took 0.03 seconds\n",
      "-- Epoch 1\n",
      "Norm: 4.26, NNZs: 300, Bias: -2.262560, T: 10555, Avg. loss: 0.069689\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 3.43, NNZs: 300, Bias: -1.506584, T: 21110, Avg. loss: 0.035187\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 3.24, NNZs: 300, Bias: -1.223200, T: 31665, Avg. loss: 0.032415\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 3.08, NNZs: 300, Bias: -1.032430, T: 42220, Avg. loss: 0.030573\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 3.06, NNZs: 300, Bias: -0.924861, T: 52775, Avg. loss: 0.030836\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 2.99, NNZs: 300, Bias: -0.805460, T: 63330, Avg. loss: 0.029674\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 2.98, NNZs: 300, Bias: -0.719119, T: 73885, Avg. loss: 0.029826\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 8\n",
      "Norm: 2.96, NNZs: 300, Bias: -0.668772, T: 84440, Avg. loss: 0.029384\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 9\n",
      "Norm: 2.96, NNZs: 300, Bias: -0.648407, T: 94995, Avg. loss: 0.029325\n",
      "Total training time: 0.03 seconds.\n",
      "Convergence after 9 epochs took 0.03 seconds\n",
      "-- Epoch 1\n",
      "Norm: 3.93, NNZs: 300, Bias: -0.657311, T: 10555, Avg. loss: 0.052925\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 3.19, NNZs: 300, Bias: -0.326863, T: 21110, Avg. loss: 0.022470\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 2.91, NNZs: 300, Bias: -0.310016, T: 31665, Avg. loss: 0.021423\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 2.78, NNZs: 300, Bias: -0.250570, T: 42220, Avg. loss: 0.020912\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 2.78, NNZs: 300, Bias: -0.266257, T: 52775, Avg. loss: 0.020685\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 2.70, NNZs: 300, Bias: -0.280727, T: 63330, Avg. loss: 0.020349\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 2.65, NNZs: 300, Bias: -0.223786, T: 73885, Avg. loss: 0.020136\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 8\n",
      "Norm: 2.62, NNZs: 300, Bias: -0.251251, T: 84440, Avg. loss: 0.020163\n",
      "Total training time: 0.03 seconds.\n",
      "Convergence after 8 epochs took 0.03 seconds\n",
      "-- Epoch 1\n",
      "Norm: 2.29, NNZs: 300, Bias: -2.927900, T: 10555, Avg. loss: 0.033070\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 1.50, NNZs: 300, Bias: -2.366401, T: 21110, Avg. loss: 0.019927\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 1.39, NNZs: 300, Bias: -1.979955, T: 31665, Avg. loss: 0.017702\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 1.24, NNZs: 300, Bias: -1.715245, T: 42220, Avg. loss: 0.017043\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 1.10, NNZs: 300, Bias: -1.611217, T: 52775, Avg. loss: 0.016522\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 1.04, NNZs: 300, Bias: -1.422014, T: 63330, Avg. loss: 0.016265\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 0.96, NNZs: 300, Bias: -1.381416, T: 73885, Avg. loss: 0.015956\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 8\n",
      "Norm: 0.89, NNZs: 300, Bias: -1.316792, T: 84440, Avg. loss: 0.015935\n",
      "Total training time: 0.03 seconds.\n",
      "Convergence after 8 epochs took 0.03 seconds\n",
      "-- Epoch 1\n",
      "Norm: 3.08, NNZs: 300, Bias: -2.423987, T: 10555, Avg. loss: 0.038267\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 2.84, NNZs: 300, Bias: -1.645020, T: 21110, Avg. loss: 0.017993\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 2.64, NNZs: 300, Bias: -1.412184, T: 31665, Avg. loss: 0.016475\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 2.58, NNZs: 300, Bias: -1.110983, T: 42220, Avg. loss: 0.015364\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 2.58, NNZs: 300, Bias: -1.005730, T: 52775, Avg. loss: 0.014488\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 2.54, NNZs: 300, Bias: -0.889201, T: 63330, Avg. loss: 0.014492\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 2.49, NNZs: 300, Bias: -0.891234, T: 73885, Avg. loss: 0.014167\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 8\n",
      "Norm: 2.49, NNZs: 300, Bias: -0.812021, T: 84440, Avg. loss: 0.014455\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 9\n",
      "Norm: 2.48, NNZs: 300, Bias: -0.765654, T: 94995, Avg. loss: 0.014083\n",
      "Total training time: 0.03 seconds.\n",
      "Convergence after 9 epochs took 0.03 seconds\n",
      "-- Epoch 1\n",
      "Norm: 3.89, NNZs: 300, Bias: -0.092531, T: 10555, Avg. loss: 0.110364\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 3.66, NNZs: 300, Bias: -0.129193, T: 21110, Avg. loss: 0.054493\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 3.40, NNZs: 300, Bias: -0.136892, T: 31665, Avg. loss: 0.052179\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 3.28, NNZs: 300, Bias: -0.032337, T: 42220, Avg. loss: 0.049706\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 3.23, NNZs: 300, Bias: -0.112592, T: 52775, Avg. loss: 0.049695\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 3.13, NNZs: 300, Bias: 0.024373, T: 63330, Avg. loss: 0.049364\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 3.11, NNZs: 300, Bias: -0.053494, T: 73885, Avg. loss: 0.048950\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 8\n",
      "Norm: 3.09, NNZs: 300, Bias: -0.065759, T: 84440, Avg. loss: 0.048754\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 9\n",
      "Norm: 3.04, NNZs: 300, Bias: -0.100216, T: 94995, Avg. loss: 0.048105\n",
      "Total training time: 0.03 seconds.\n",
      "Convergence after 9 epochs took 0.03 seconds\n",
      "-- Epoch 1\n",
      "Norm: 2.92, NNZs: 300, Bias: -1.692784, T: 10555, Avg. loss: 0.032451\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 2.58, NNZs: 300, Bias: -1.039714, T: 21110, Avg. loss: 0.019391\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 2.33, NNZs: 300, Bias: -0.624032, T: 31665, Avg. loss: 0.017992\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 2.23, NNZs: 300, Bias: -0.496576, T: 42220, Avg. loss: 0.017218\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 2.15, NNZs: 300, Bias: -0.419548, T: 52775, Avg. loss: 0.016731\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 2.13, NNZs: 300, Bias: -0.370934, T: 63330, Avg. loss: 0.016572\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 2.13, NNZs: 300, Bias: -0.345931, T: 73885, Avg. loss: 0.016407\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 8\n",
      "Norm: 2.13, NNZs: 300, Bias: -0.307038, T: 84440, Avg. loss: 0.016514\n",
      "Total training time: 0.03 seconds.\n",
      "Convergence after 8 epochs took 0.03 seconds\n",
      "-- Epoch 1\n",
      "Norm: 4.17, NNZs: 300, Bias: -3.254297, T: 10555, Avg. loss: 0.051183\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 3.49, NNZs: 300, Bias: -2.575203, T: 21110, Avg. loss: 0.027164\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 3.32, NNZs: 300, Bias: -2.115329, T: 31665, Avg. loss: 0.024392\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 3.21, NNZs: 300, Bias: -1.858573, T: 42220, Avg. loss: 0.023353\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 3.15, NNZs: 300, Bias: -1.667614, T: 52775, Avg. loss: 0.023483\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 3.12, NNZs: 300, Bias: -1.484674, T: 63330, Avg. loss: 0.022734\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 3.08, NNZs: 300, Bias: -1.446611, T: 73885, Avg. loss: 0.022458\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 8\n",
      "Norm: 3.05, NNZs: 300, Bias: -1.395392, T: 84440, Avg. loss: 0.022111\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 9\n",
      "Norm: 3.04, NNZs: 300, Bias: -1.295509, T: 94995, Avg. loss: 0.022251\n",
      "Total training time: 0.03 seconds.\n",
      "Convergence after 9 epochs took 0.03 seconds\n",
      "-- Epoch 1\n",
      "Norm: 3.57, NNZs: 300, Bias: -3.859702, T: 10555, Avg. loss: 0.031388\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 2.97, NNZs: 300, Bias: -3.231168, T: 21110, Avg. loss: 0.015317\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 2.94, NNZs: 300, Bias: -2.759578, T: 31665, Avg. loss: 0.014670\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 2.78, NNZs: 300, Bias: -2.510472, T: 42220, Avg. loss: 0.013869\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 2.70, NNZs: 300, Bias: -2.334689, T: 52775, Avg. loss: 0.013523\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 2.72, NNZs: 300, Bias: -2.179998, T: 63330, Avg. loss: 0.013395\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 2.69, NNZs: 300, Bias: -2.078080, T: 73885, Avg. loss: 0.013098\n",
      "Total training time: 0.02 seconds.\n",
      "Convergence after 7 epochs took 0.02 seconds\n",
      "-- Epoch 1\n",
      "Norm: 2.11, NNZs: 300, Bias: -3.402381, T: 10555, Avg. loss: 0.022311\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 1.72, NNZs: 300, Bias: -2.749022, T: 21110, Avg. loss: 0.013566\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 1.77, NNZs: 300, Bias: -2.264422, T: 31665, Avg. loss: 0.011816\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 1.55, NNZs: 300, Bias: -1.996286, T: 42220, Avg. loss: 0.011081\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 1.52, NNZs: 300, Bias: -1.803912, T: 52775, Avg. loss: 0.010669\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 1.48, NNZs: 300, Bias: -1.680567, T: 63330, Avg. loss: 0.010338\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 1.47, NNZs: 300, Bias: -1.577119, T: 73885, Avg. loss: 0.010309\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 8\n",
      "Norm: 1.49, NNZs: 300, Bias: -1.461920, T: 84440, Avg. loss: 0.010146\n",
      "Total training time: 0.02 seconds.\n",
      "Convergence after 8 epochs took 0.02 seconds\n",
      "-- Epoch 1\n",
      "Norm: 3.61, NNZs: 300, Bias: -1.427511, T: 10555, Avg. loss: 0.071988\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 2.78, NNZs: 300, Bias: -0.827405, T: 21110, Avg. loss: 0.033943\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 2.68, NNZs: 300, Bias: -0.566227, T: 31665, Avg. loss: 0.031453\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 2.64, NNZs: 300, Bias: -0.409570, T: 42220, Avg. loss: 0.030299\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 2.55, NNZs: 300, Bias: -0.352843, T: 52775, Avg. loss: 0.029838\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 2.56, NNZs: 300, Bias: -0.326739, T: 63330, Avg. loss: 0.029265\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 2.51, NNZs: 300, Bias: -0.224533, T: 73885, Avg. loss: 0.029104\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 8\n",
      "Norm: 2.48, NNZs: 300, Bias: -0.187891, T: 84440, Avg. loss: 0.029319\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 9\n",
      "Norm: 2.49, NNZs: 300, Bias: -0.189434, T: 94995, Avg. loss: 0.028748\n",
      "Total training time: 0.03 seconds.\n",
      "Convergence after 9 epochs took 0.03 seconds\n",
      "-- Epoch 1\n",
      "Norm: 6.03, NNZs: 300, Bias: 1.558698, T: 10555, Avg. loss: 0.412876\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 4.69, NNZs: 300, Bias: 1.069377, T: 21110, Avg. loss: 0.197701\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 4.14, NNZs: 300, Bias: 0.886942, T: 31665, Avg. loss: 0.189258\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 4.01, NNZs: 300, Bias: 0.614222, T: 42220, Avg. loss: 0.183087\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 3.93, NNZs: 300, Bias: 0.515380, T: 52775, Avg. loss: 0.180287\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 3.80, NNZs: 300, Bias: 0.430067, T: 63330, Avg. loss: 0.178159\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 3.73, NNZs: 300, Bias: 0.445671, T: 73885, Avg. loss: 0.177582\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 8\n",
      "Norm: 3.70, NNZs: 300, Bias: 0.397362, T: 84440, Avg. loss: 0.175171\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 9\n",
      "Norm: 3.67, NNZs: 300, Bias: 0.375782, T: 94995, Avg. loss: 0.174985\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 10\n",
      "Norm: 3.63, NNZs: 300, Bias: 0.405294, T: 105550, Avg. loss: 0.174413\n",
      "Total training time: 0.04 seconds.\n",
      "-- Epoch 11\n",
      "Norm: 3.61, NNZs: 300, Bias: 0.343728, T: 116105, Avg. loss: 0.173469\n",
      "Total training time: 0.04 seconds.\n",
      "-- Epoch 12\n",
      "Norm: 3.60, NNZs: 300, Bias: 0.336852, T: 126660, Avg. loss: 0.173544\n",
      "Total training time: 0.04 seconds.\n",
      "-- Epoch 13\n",
      "Norm: 3.53, NNZs: 300, Bias: 0.389789, T: 137215, Avg. loss: 0.173083\n",
      "Total training time: 0.04 seconds.\n",
      "Convergence after 13 epochs took 0.04 seconds\n",
      "-- Epoch 1\n",
      "Norm: 3.76, NNZs: 300, Bias: -2.340729, T: 10555, Avg. loss: 0.069178\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 2.82, NNZs: 300, Bias: -1.700797, T: 21110, Avg. loss: 0.031941\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 2.52, NNZs: 300, Bias: -1.440438, T: 31665, Avg. loss: 0.029742\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 2.48, NNZs: 300, Bias: -1.253553, T: 42220, Avg. loss: 0.028627\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 2.46, NNZs: 300, Bias: -1.126543, T: 52775, Avg. loss: 0.028040\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 2.41, NNZs: 300, Bias: -1.039709, T: 63330, Avg. loss: 0.027286\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 2.35, NNZs: 300, Bias: -1.021378, T: 73885, Avg. loss: 0.027063\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 8\n",
      "Norm: 2.33, NNZs: 300, Bias: -0.941671, T: 84440, Avg. loss: 0.027262\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 9\n",
      "Norm: 2.34, NNZs: 300, Bias: -0.886132, T: 94995, Avg. loss: 0.027011\n",
      "Total training time: 0.03 seconds.\n",
      "Convergence after 9 epochs took 0.03 seconds\n",
      "-- Epoch 1\n",
      "Norm: 3.91, NNZs: 300, Bias: -1.807491, T: 10555, Avg. loss: 0.049967\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 3.14, NNZs: 300, Bias: -1.271552, T: 21110, Avg. loss: 0.024715\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 2.93, NNZs: 300, Bias: -1.075759, T: 31665, Avg. loss: 0.022547\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 2.90, NNZs: 300, Bias: -0.857412, T: 42220, Avg. loss: 0.021633\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 2.81, NNZs: 300, Bias: -0.713290, T: 52775, Avg. loss: 0.021096\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 2.78, NNZs: 300, Bias: -0.679631, T: 63330, Avg. loss: 0.021387\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 2.74, NNZs: 300, Bias: -0.630755, T: 73885, Avg. loss: 0.020902\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 8\n",
      "Norm: 2.72, NNZs: 300, Bias: -0.552550, T: 84440, Avg. loss: 0.021080\n",
      "Total training time: 0.03 seconds.\n",
      "Convergence after 8 epochs took 0.03 seconds\n",
      "-- Epoch 1\n",
      "Norm: 4.67, NNZs: 300, Bias: -0.719487, T: 10555, Avg. loss: 0.124468\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 3.65, NNZs: 300, Bias: -0.469202, T: 21110, Avg. loss: 0.070121\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 3.34, NNZs: 300, Bias: -0.087622, T: 31665, Avg. loss: 0.064859\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 3.14, NNZs: 300, Bias: -0.084553, T: 42220, Avg. loss: 0.063655\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 3.09, NNZs: 300, Bias: -0.083593, T: 52775, Avg. loss: 0.061967\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 3.06, NNZs: 300, Bias: -0.095658, T: 63330, Avg. loss: 0.061271\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 2.99, NNZs: 300, Bias: -0.080117, T: 73885, Avg. loss: 0.060859\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 8\n",
      "Norm: 2.96, NNZs: 300, Bias: -0.101419, T: 84440, Avg. loss: 0.060757\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 9\n",
      "Norm: 2.94, NNZs: 300, Bias: -0.121605, T: 94995, Avg. loss: 0.060688\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 10\n",
      "Norm: 2.91, NNZs: 300, Bias: -0.121472, T: 105550, Avg. loss: 0.060263\n",
      "Total training time: 0.03 seconds.\n",
      "Convergence after 10 epochs took 0.03 seconds\n",
      "-- Epoch 1\n",
      "Norm: 4.85, NNZs: 300, Bias: 0.482680, T: 10555, Avg. loss: 0.093114\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 3.92, NNZs: 300, Bias: 1.018653, T: 21110, Avg. loss: 0.049344\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 3.71, NNZs: 300, Bias: 0.999708, T: 31665, Avg. loss: 0.045835\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 3.68, NNZs: 300, Bias: 1.053806, T: 42220, Avg. loss: 0.044679\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 3.53, NNZs: 300, Bias: 1.142174, T: 52775, Avg. loss: 0.044090\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 3.54, NNZs: 300, Bias: 1.089051, T: 63330, Avg. loss: 0.043787\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 3.47, NNZs: 300, Bias: 1.102277, T: 73885, Avg. loss: 0.043693\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 8\n",
      "Norm: 3.43, NNZs: 300, Bias: 1.078737, T: 84440, Avg. loss: 0.043285\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 9\n",
      "Norm: 3.39, NNZs: 300, Bias: 1.068412, T: 94995, Avg. loss: 0.043535\n",
      "Total training time: 0.03 seconds.\n",
      "Convergence after 9 epochs took 0.03 seconds\n",
      "-- Epoch 1\n",
      "Norm: 3.56, NNZs: 300, Bias: -3.588017, T: 10555, Avg. loss: 0.041602\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 2.93, NNZs: 300, Bias: -2.721067, T: 21110, Avg. loss: 0.017365\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 2.70, NNZs: 300, Bias: -2.216274, T: 31665, Avg. loss: 0.015294\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 2.58, NNZs: 300, Bias: -1.972369, T: 42220, Avg. loss: 0.015100\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 2.53, NNZs: 300, Bias: -1.726488, T: 52775, Avg. loss: 0.014513\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 2.48, NNZs: 300, Bias: -1.631682, T: 63330, Avg. loss: 0.014365\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 2.44, NNZs: 300, Bias: -1.519191, T: 73885, Avg. loss: 0.014136\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 8\n",
      "Norm: 2.42, NNZs: 300, Bias: -1.434740, T: 84440, Avg. loss: 0.014045\n",
      "Total training time: 0.02 seconds.\n",
      "Convergence after 8 epochs took 0.02 seconds\n",
      "-- Epoch 1\n",
      "Norm: 2.52, NNZs: 300, Bias: -3.571959, T: 10555, Avg. loss: 0.017498\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 1.98, NNZs: 300, Bias: -2.974172, T: 21110, Avg. loss: 0.010287\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 1.76, NNZs: 300, Bias: -2.738932, T: 31665, Avg. loss: 0.008993\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 1.64, NNZs: 300, Bias: -2.540070, T: 42220, Avg. loss: 0.008875\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 1.64, NNZs: 300, Bias: -2.348522, T: 52775, Avg. loss: 0.008482\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 1.54, NNZs: 300, Bias: -2.283007, T: 63330, Avg. loss: 0.008394\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 1.54, NNZs: 300, Bias: -2.166728, T: 73885, Avg. loss: 0.008254\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 8\n",
      "Norm: 1.50, NNZs: 300, Bias: -2.079788, T: 84440, Avg. loss: 0.008121\n",
      "Total training time: 0.03 seconds.\n",
      "Convergence after 8 epochs took 0.03 seconds\n",
      "-- Epoch 1\n",
      "Norm: 3.42, NNZs: 300, Bias: -1.937190, T: 10555, Avg. loss: 0.045238\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 3.08, NNZs: 300, Bias: -1.326886, T: 21110, Avg. loss: 0.020552\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 2.86, NNZs: 300, Bias: -0.956290, T: 31665, Avg. loss: 0.019001\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 2.73, NNZs: 300, Bias: -0.881049, T: 42220, Avg. loss: 0.018337\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 2.67, NNZs: 300, Bias: -0.792872, T: 52775, Avg. loss: 0.018126\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 2.64, NNZs: 300, Bias: -0.650144, T: 63330, Avg. loss: 0.017995\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 2.66, NNZs: 300, Bias: -0.619978, T: 73885, Avg. loss: 0.017644\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 8\n",
      "Norm: 2.65, NNZs: 300, Bias: -0.569023, T: 84440, Avg. loss: 0.017354\n",
      "Total training time: 0.03 seconds.\n",
      "Convergence after 8 epochs took 0.03 seconds\n",
      "-- Epoch 1\n",
      "Norm: 2.90, NNZs: 300, Bias: -2.184555, T: 10555, Avg. loss: 0.025894\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 2.25, NNZs: 300, Bias: -1.724998, T: 21110, Avg. loss: 0.014932\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 2.07, NNZs: 300, Bias: -1.319007, T: 31665, Avg. loss: 0.013771\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 1.96, NNZs: 300, Bias: -1.175101, T: 42220, Avg. loss: 0.013037\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 1.88, NNZs: 300, Bias: -0.960989, T: 52775, Avg. loss: 0.012850\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 1.81, NNZs: 300, Bias: -0.925012, T: 63330, Avg. loss: 0.012727\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 1.77, NNZs: 300, Bias: -0.834278, T: 73885, Avg. loss: 0.012455\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 8\n",
      "Norm: 1.76, NNZs: 300, Bias: -0.769598, T: 84440, Avg. loss: 0.012488\n",
      "Total training time: 0.02 seconds.\n",
      "Convergence after 8 epochs took 0.02 seconds\n",
      "-- Epoch 1\n",
      "Norm: 4.28, NNZs: 300, Bias: -1.630046, T: 10555, Avg. loss: 0.122564\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 3.50, NNZs: 300, Bias: -0.690765, T: 21110, Avg. loss: 0.058131\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 3.07, NNZs: 300, Bias: -0.340287, T: 31665, Avg. loss: 0.052547\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 2.96, NNZs: 300, Bias: -0.097665, T: 42220, Avg. loss: 0.051333\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 2.85, NNZs: 300, Bias: -0.042124, T: 52775, Avg. loss: 0.050783\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 2.78, NNZs: 300, Bias: 0.038642, T: 63330, Avg. loss: 0.049994\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 2.74, NNZs: 300, Bias: 0.008925, T: 73885, Avg. loss: 0.049972\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 8\n",
      "Norm: 2.75, NNZs: 300, Bias: 0.060064, T: 84440, Avg. loss: 0.048987\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 9\n",
      "Norm: 2.73, NNZs: 300, Bias: 0.047813, T: 94995, Avg. loss: 0.048916\n",
      "Total training time: 0.03 seconds.\n",
      "Convergence after 9 epochs took 0.03 seconds\n",
      "-- Epoch 1\n",
      "Norm: 2.45, NNZs: 300, Bias: -4.289507, T: 10555, Avg. loss: 0.015001\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 2.06, NNZs: 300, Bias: -3.797527, T: 21110, Avg. loss: 0.008474\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 1.90, NNZs: 300, Bias: -3.449195, T: 31665, Avg. loss: 0.007344\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 1.65, NNZs: 300, Bias: -3.309024, T: 42220, Avg. loss: 0.007198\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 1.62, NNZs: 300, Bias: -3.095922, T: 52775, Avg. loss: 0.006853\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 1.54, NNZs: 300, Bias: -2.956495, T: 63330, Avg. loss: 0.006904\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 1.48, NNZs: 300, Bias: -2.841369, T: 73885, Avg. loss: 0.006832\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 8\n",
      "Norm: 1.46, NNZs: 300, Bias: -2.740734, T: 84440, Avg. loss: 0.006625\n",
      "Total training time: 0.02 seconds.\n",
      "Convergence after 8 epochs took 0.02 seconds\n",
      "-- Epoch 1\n",
      "Norm: 3.83, NNZs: 300, Bias: -2.770149, T: 10555, Avg. loss: 0.091774\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 3.29, NNZs: 300, Bias: -1.945495, T: 21110, Avg. loss: 0.048304\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 2.88, NNZs: 300, Bias: -1.481454, T: 31665, Avg. loss: 0.046279\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 2.79, NNZs: 300, Bias: -1.204747, T: 42220, Avg. loss: 0.043733\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 2.75, NNZs: 300, Bias: -1.068983, T: 52775, Avg. loss: 0.042881\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 2.72, NNZs: 300, Bias: -0.877708, T: 63330, Avg. loss: 0.043135\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 2.73, NNZs: 300, Bias: -0.835714, T: 73885, Avg. loss: 0.042346\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 8\n",
      "Norm: 2.73, NNZs: 300, Bias: -0.773176, T: 84440, Avg. loss: 0.041718\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 9\n",
      "Norm: 2.71, NNZs: 300, Bias: -0.663838, T: 94995, Avg. loss: 0.041629\n",
      "Total training time: 0.03 seconds.\n",
      "Convergence after 9 epochs took 0.03 seconds\n",
      "-- Epoch 1\n",
      "Norm: 3.41, NNZs: 300, Bias: -3.214540, T: 10555, Avg. loss: 0.022998\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 2.99, NNZs: 300, Bias: -2.572219, T: 21110, Avg. loss: 0.011794\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 2.73, NNZs: 300, Bias: -2.278857, T: 31665, Avg. loss: 0.010594\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 2.67, NNZs: 300, Bias: -1.972660, T: 42220, Avg. loss: 0.010900\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 2.61, NNZs: 300, Bias: -1.806201, T: 52775, Avg. loss: 0.010231\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 2.54, NNZs: 300, Bias: -1.719969, T: 63330, Avg. loss: 0.010183\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 2.51, NNZs: 300, Bias: -1.603552, T: 73885, Avg. loss: 0.009804\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 8\n",
      "Norm: 2.50, NNZs: 300, Bias: -1.539933, T: 84440, Avg. loss: 0.009791\n",
      "Total training time: 0.02 seconds.\n",
      "Convergence after 8 epochs took 0.02 seconds\n",
      "-- Epoch 1\n",
      "Norm: 2.82, NNZs: 300, Bias: -4.473568, T: 10555, Avg. loss: 0.040390\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 2.34, NNZs: 300, Bias: -3.558436, T: 21110, Avg. loss: 0.020388\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 2.09, NNZs: 300, Bias: -3.081217, T: 31665, Avg. loss: 0.017002\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 2.03, NNZs: 300, Bias: -2.758019, T: 42220, Avg. loss: 0.016759\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 1.96, NNZs: 300, Bias: -2.487894, T: 52775, Avg. loss: 0.015917\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 1.89, NNZs: 300, Bias: -2.331978, T: 63330, Avg. loss: 0.015732\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 1.87, NNZs: 300, Bias: -2.175148, T: 73885, Avg. loss: 0.015200\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 8\n",
      "Norm: 1.79, NNZs: 300, Bias: -2.089984, T: 84440, Avg. loss: 0.014868\n",
      "Total training time: 0.03 seconds.\n",
      "Convergence after 8 epochs took 0.03 seconds\n",
      "-- Epoch 1\n",
      "Norm: 4.88, NNZs: 300, Bias: 0.849906, T: 10555, Avg. loss: 0.224164\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 3.87, NNZs: 300, Bias: 0.640005, T: 21110, Avg. loss: 0.120158\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 3.40, NNZs: 300, Bias: 0.484053, T: 31665, Avg. loss: 0.112407\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 3.34, NNZs: 300, Bias: 0.260540, T: 42220, Avg. loss: 0.109339\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 3.18, NNZs: 300, Bias: 0.152501, T: 52775, Avg. loss: 0.108626\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 3.05, NNZs: 300, Bias: 0.083392, T: 63330, Avg. loss: 0.107252\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 2.96, NNZs: 300, Bias: 0.043669, T: 73885, Avg. loss: 0.106724\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 8\n",
      "Norm: 2.90, NNZs: 300, Bias: 0.090448, T: 84440, Avg. loss: 0.106089\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 9\n",
      "Norm: 2.93, NNZs: 300, Bias: 0.052430, T: 94995, Avg. loss: 0.105710\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 10\n",
      "Norm: 2.87, NNZs: 300, Bias: 0.010585, T: 105550, Avg. loss: 0.105122\n",
      "Total training time: 0.04 seconds.\n",
      "-- Epoch 11\n",
      "Norm: 2.86, NNZs: 300, Bias: -0.025097, T: 116105, Avg. loss: 0.105406\n",
      "Total training time: 0.04 seconds.\n",
      "Convergence after 11 epochs took 0.04 seconds\n",
      "-- Epoch 1\n",
      "Norm: 3.71, NNZs: 300, Bias: -2.502093, T: 10555, Avg. loss: 0.086729\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 3.01, NNZs: 300, Bias: -1.814659, T: 21110, Avg. loss: 0.044723\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 2.81, NNZs: 300, Bias: -1.453660, T: 31665, Avg. loss: 0.040034\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 2.67, NNZs: 300, Bias: -1.205235, T: 42220, Avg. loss: 0.038821\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 2.60, NNZs: 300, Bias: -1.058768, T: 52775, Avg. loss: 0.037849\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 2.56, NNZs: 300, Bias: -0.995205, T: 63330, Avg. loss: 0.036412\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 2.50, NNZs: 300, Bias: -0.934936, T: 73885, Avg. loss: 0.037140\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 8\n",
      "Norm: 2.51, NNZs: 300, Bias: -0.905912, T: 84440, Avg. loss: 0.036562\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 9\n",
      "Norm: 2.51, NNZs: 300, Bias: -0.825499, T: 94995, Avg. loss: 0.036215\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 10\n",
      "Norm: 2.49, NNZs: 300, Bias: -0.768293, T: 105550, Avg. loss: 0.035945\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 11\n",
      "Norm: 2.48, NNZs: 300, Bias: -0.823505, T: 116105, Avg. loss: 0.036032\n",
      "Total training time: 0.03 seconds.\n",
      "Convergence after 11 epochs took 0.03 seconds\n",
      "-- Epoch 1\n",
      "Norm: 4.01, NNZs: 300, Bias: -1.796129, T: 10555, Avg. loss: 0.091073\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 3.33, NNZs: 300, Bias: -0.870142, T: 21110, Avg. loss: 0.051249\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 3.07, NNZs: 300, Bias: -0.597794, T: 31665, Avg. loss: 0.045920\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 3.05, NNZs: 300, Bias: -0.514341, T: 42220, Avg. loss: 0.045525\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 2.96, NNZs: 300, Bias: -0.456316, T: 52775, Avg. loss: 0.044715\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 2.91, NNZs: 300, Bias: -0.484013, T: 63330, Avg. loss: 0.044443\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 2.91, NNZs: 300, Bias: -0.450890, T: 73885, Avg. loss: 0.043446\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 8\n",
      "Norm: 2.87, NNZs: 300, Bias: -0.422507, T: 84440, Avg. loss: 0.043395\n",
      "Total training time: 0.03 seconds.\n",
      "Convergence after 8 epochs took 0.03 seconds\n",
      "-- Epoch 1\n",
      "Norm: 4.12, NNZs: 300, Bias: -1.470679, T: 10555, Avg. loss: 0.055922\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 3.26, NNZs: 300, Bias: -1.023242, T: 21110, Avg. loss: 0.022407\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 2.87, NNZs: 300, Bias: -0.685762, T: 31665, Avg. loss: 0.022130\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 2.77, NNZs: 300, Bias: -0.520065, T: 42220, Avg. loss: 0.020784\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 2.71, NNZs: 300, Bias: -0.396529, T: 52775, Avg. loss: 0.020412\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 2.72, NNZs: 300, Bias: -0.413582, T: 63330, Avg. loss: 0.019495\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 2.69, NNZs: 300, Bias: -0.334706, T: 73885, Avg. loss: 0.019798\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 8\n",
      "Norm: 2.65, NNZs: 300, Bias: -0.255974, T: 84440, Avg. loss: 0.019701\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 9\n",
      "Norm: 2.65, NNZs: 300, Bias: -0.222080, T: 94995, Avg. loss: 0.019742\n",
      "Total training time: 0.03 seconds.\n",
      "Convergence after 9 epochs took 0.03 seconds\n",
      "-- Epoch 1\n",
      "Norm: 4.27, NNZs: 300, Bias: -1.612789, T: 10555, Avg. loss: 0.081560\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 3.63, NNZs: 300, Bias: -0.944079, T: 21110, Avg. loss: 0.038283\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 3.40, NNZs: 300, Bias: -0.533109, T: 31665, Avg. loss: 0.036166\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 3.32, NNZs: 300, Bias: -0.325932, T: 42220, Avg. loss: 0.033853\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 3.24, NNZs: 300, Bias: -0.269956, T: 52775, Avg. loss: 0.033395\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 3.27, NNZs: 300, Bias: -0.299047, T: 63330, Avg. loss: 0.032969\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 3.23, NNZs: 300, Bias: -0.179258, T: 73885, Avg. loss: 0.032472\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 8\n",
      "Norm: 3.23, NNZs: 300, Bias: -0.205237, T: 84440, Avg. loss: 0.032707\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 9\n",
      "Norm: 3.18, NNZs: 300, Bias: -0.158996, T: 94995, Avg. loss: 0.032196\n",
      "Total training time: 0.03 seconds.\n",
      "Convergence after 9 epochs took 0.03 seconds\n",
      "-- Epoch 1\n",
      "Norm: 4.32, NNZs: 300, Bias: 0.653436, T: 10555, Avg. loss: 0.352040\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 3.74, NNZs: 300, Bias: 0.254033, T: 21110, Avg. loss: 0.176410\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 3.21, NNZs: 300, Bias: 0.170520, T: 31665, Avg. loss: 0.164176\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 3.08, NNZs: 300, Bias: 0.016577, T: 42220, Avg. loss: 0.158407\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 2.88, NNZs: 300, Bias: 0.085120, T: 52775, Avg. loss: 0.155780\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 2.87, NNZs: 300, Bias: -0.008876, T: 63330, Avg. loss: 0.154817\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 2.80, NNZs: 300, Bias: -0.051338, T: 73885, Avg. loss: 0.153514\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 8\n",
      "Norm: 2.83, NNZs: 300, Bias: -0.025102, T: 84440, Avg. loss: 0.151905\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 9\n",
      "Norm: 2.73, NNZs: 300, Bias: -0.040316, T: 94995, Avg. loss: 0.152217\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 10\n",
      "Norm: 2.68, NNZs: 300, Bias: -0.052241, T: 105550, Avg. loss: 0.150243\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 11\n",
      "Norm: 2.64, NNZs: 300, Bias: -0.069372, T: 116105, Avg. loss: 0.150822\n",
      "Total training time: 0.04 seconds.\n",
      "-- Epoch 12\n",
      "Norm: 2.68, NNZs: 300, Bias: -0.102312, T: 126660, Avg. loss: 0.150153\n",
      "Total training time: 0.04 seconds.\n",
      "-- Epoch 13\n",
      "Norm: 2.65, NNZs: 300, Bias: -0.124395, T: 137215, Avg. loss: 0.150206\n",
      "Total training time: 0.04 seconds.\n",
      "-- Epoch 14\n",
      "Norm: 2.62, NNZs: 300, Bias: -0.082541, T: 147770, Avg. loss: 0.149813\n",
      "Total training time: 0.05 seconds.\n",
      "-- Epoch 15\n",
      "Norm: 2.61, NNZs: 300, Bias: -0.123462, T: 158325, Avg. loss: 0.149498\n",
      "Total training time: 0.05 seconds.\n",
      "Convergence after 15 epochs took 0.05 seconds\n",
      "-- Epoch 1\n",
      "Norm: 3.01, NNZs: 300, Bias: -3.614565, T: 10555, Avg. loss: 0.023988\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 2.48, NNZs: 300, Bias: -2.996492, T: 21110, Avg. loss: 0.012108\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 2.14, NNZs: 300, Bias: -2.690841, T: 31665, Avg. loss: 0.011242\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 2.09, NNZs: 300, Bias: -2.477397, T: 42220, Avg. loss: 0.010989\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 2.00, NNZs: 300, Bias: -2.348608, T: 52775, Avg. loss: 0.010452\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 1.95, NNZs: 300, Bias: -2.226316, T: 63330, Avg. loss: 0.010478\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 1.93, NNZs: 300, Bias: -2.162866, T: 73885, Avg. loss: 0.010115\n",
      "Total training time: 0.02 seconds.\n",
      "Convergence after 7 epochs took 0.02 seconds\n",
      "-- Epoch 1\n",
      "Norm: 2.79, NNZs: 300, Bias: -4.144908, T: 10555, Avg. loss: 0.027769\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 2.62, NNZs: 300, Bias: -3.637352, T: 21110, Avg. loss: 0.011648\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 2.52, NNZs: 300, Bias: -3.242002, T: 31665, Avg. loss: 0.010481\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 2.38, NNZs: 300, Bias: -3.053634, T: 42220, Avg. loss: 0.010145\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 2.34, NNZs: 300, Bias: -2.927842, T: 52775, Avg. loss: 0.010022\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 2.28, NNZs: 300, Bias: -2.821694, T: 63330, Avg. loss: 0.009614\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 2.25, NNZs: 300, Bias: -2.747217, T: 73885, Avg. loss: 0.009622\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 8\n",
      "Norm: 2.23, NNZs: 300, Bias: -2.659547, T: 84440, Avg. loss: 0.009369\n",
      "Total training time: 0.03 seconds.\n",
      "Convergence after 8 epochs took 0.03 seconds\n",
      "-- Epoch 1\n",
      "Norm: 4.71, NNZs: 300, Bias: 0.441807, T: 10555, Avg. loss: 0.138425\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 3.97, NNZs: 300, Bias: 0.396858, T: 21110, Avg. loss: 0.072523\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 3.75, NNZs: 300, Bias: 0.438587, T: 31665, Avg. loss: 0.069544\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 3.53, NNZs: 300, Bias: 0.393379, T: 42220, Avg. loss: 0.068103\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 3.43, NNZs: 300, Bias: 0.308719, T: 52775, Avg. loss: 0.066710\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 3.34, NNZs: 300, Bias: 0.295244, T: 63330, Avg. loss: 0.066210\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 3.28, NNZs: 300, Bias: 0.298171, T: 73885, Avg. loss: 0.065367\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 8\n",
      "Norm: 3.19, NNZs: 300, Bias: 0.331044, T: 84440, Avg. loss: 0.065473\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 9\n",
      "Norm: 3.21, NNZs: 300, Bias: 0.258731, T: 94995, Avg. loss: 0.065282\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 10\n",
      "Norm: 3.18, NNZs: 300, Bias: 0.180798, T: 105550, Avg. loss: 0.065255\n",
      "Total training time: 0.03 seconds.\n",
      "Convergence after 10 epochs took 0.03 seconds\n",
      "-- Epoch 1\n",
      "Norm: 3.63, NNZs: 300, Bias: -2.996112, T: 10555, Avg. loss: 0.039115\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 2.95, NNZs: 300, Bias: -2.193822, T: 21110, Avg. loss: 0.022469\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 2.80, NNZs: 300, Bias: -1.711850, T: 31665, Avg. loss: 0.020507\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 2.62, NNZs: 300, Bias: -1.509612, T: 42220, Avg. loss: 0.020135\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 2.60, NNZs: 300, Bias: -1.403741, T: 52775, Avg. loss: 0.019400\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 2.55, NNZs: 300, Bias: -1.263194, T: 63330, Avg. loss: 0.019310\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 2.51, NNZs: 300, Bias: -1.216578, T: 73885, Avg. loss: 0.019035\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 8\n",
      "Norm: 2.52, NNZs: 300, Bias: -1.127697, T: 84440, Avg. loss: 0.019192\n",
      "Total training time: 0.03 seconds.\n",
      "Convergence after 8 epochs took 0.03 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  40 out of  40 | elapsed:    1.1s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SGDClassifier(alpha=0.001, random_state=7600, verbose=1)"
      ]
     },
     "execution_count": 446,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_d2v_sgd.fit(X_train_d2v, y_train_nonempty)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 448,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7203801945181255"
      ]
     },
     "execution_count": 448,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_test_nonempty, clf_d2v_sgd.predict(X_test_d2v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 447,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.50      0.22      0.31         9\n",
      "           1       0.67      0.82      0.74       237\n",
      "           2       0.79      0.86      0.82       172\n",
      "           3       0.72      0.69      0.70       251\n",
      "           4       0.88      0.53      0.66        57\n",
      "           5       0.70      0.58      0.64       115\n",
      "           6       0.81      0.80      0.81        71\n",
      "           7       0.72      0.78      0.75        60\n",
      "           8       0.39      0.44      0.41        39\n",
      "           9       0.86      0.77      0.81        47\n",
      "          10       0.75      0.75      0.75       147\n",
      "          11       0.81      0.83      0.82        46\n",
      "          12       0.65      0.92      0.76        76\n",
      "          13       0.87      0.99      0.92        80\n",
      "          14       0.57      0.57      0.57        23\n",
      "          15       0.73      0.84      0.78        75\n",
      "          16       0.61      0.74      0.66       529\n",
      "          17       0.78      0.75      0.77        81\n",
      "          18       0.82      0.88      0.85        95\n",
      "          19       0.61      0.64      0.62       157\n",
      "          20       0.76      0.86      0.80       152\n",
      "          21       0.69      0.93      0.79        43\n",
      "          22       0.76      0.57      0.65        28\n",
      "          23       0.82      0.85      0.83        73\n",
      "          24       0.83      0.77      0.80        26\n",
      "          25       0.81      0.71      0.76       175\n",
      "          26       0.93      0.93      0.93        14\n",
      "          27       0.63      0.92      0.75       127\n",
      "          28       0.86      0.94      0.90        32\n",
      "          29       0.83      0.43      0.57        46\n",
      "          30       0.73      0.65      0.69       298\n",
      "          31       0.82      0.45      0.58        92\n",
      "          32       0.81      0.78      0.79       140\n",
      "          33       0.82      0.77      0.79        70\n",
      "          34       0.75      0.81      0.78        90\n",
      "          35       0.70      0.58      0.63       427\n",
      "          36       0.88      0.62      0.73        48\n",
      "          37       0.84      0.70      0.76        37\n",
      "          38       0.84      0.47      0.60       167\n",
      "          39       0.77      0.75      0.76        72\n",
      "\n",
      "    accuracy                           0.72      4524\n",
      "   macro avg       0.75      0.72      0.73      4524\n",
      "weighted avg       0.73      0.72      0.72      4524\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test_nonempty, clf_d2v_sgd.predict(X_test_d2v)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 449,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 450,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_d2v_lr = LogisticRegression(\n",
    "    max_iter=200,\n",
    "    random_state=7600,\n",
    "    verbose=1,\n",
    "    n_jobs=cpu_count()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 451,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=16)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=16)]: Done   1 out of   1 | elapsed:   10.6s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(max_iter=200, n_jobs=16, random_state=7600, verbose=1)"
      ]
     },
     "execution_count": 451,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_d2v_lr.fit(X_train_d2v, y_train_nonempty)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 452,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.75552608311229"
      ]
     },
     "execution_count": 452,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_test_nonempty, clf_d2v_lr.predict(X_test_d2v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 453,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.67      0.75         9\n",
      "           1       0.77      0.80      0.78       237\n",
      "           2       0.82      0.85      0.83       172\n",
      "           3       0.69      0.74      0.71       251\n",
      "           4       0.76      0.65      0.70        57\n",
      "           5       0.74      0.68      0.71       115\n",
      "           6       0.76      0.83      0.79        71\n",
      "           7       0.80      0.80      0.80        60\n",
      "           8       0.78      0.54      0.64        39\n",
      "           9       0.87      0.72      0.79        47\n",
      "          10       0.80      0.73      0.77       147\n",
      "          11       0.95      0.76      0.84        46\n",
      "          12       0.77      0.92      0.84        76\n",
      "          13       0.93      0.99      0.96        80\n",
      "          14       0.75      0.52      0.62        23\n",
      "          15       0.82      0.79      0.80        75\n",
      "          16       0.69      0.70      0.69       529\n",
      "          17       0.78      0.77      0.77        81\n",
      "          18       0.85      0.87      0.86        95\n",
      "          19       0.61      0.81      0.70       157\n",
      "          20       0.82      0.85      0.83       152\n",
      "          21       0.88      0.88      0.88        43\n",
      "          22       0.92      0.43      0.59        28\n",
      "          23       0.85      0.92      0.88        73\n",
      "          24       0.90      0.73      0.81        26\n",
      "          25       0.82      0.78      0.80       175\n",
      "          26       0.93      0.93      0.93        14\n",
      "          27       0.81      0.85      0.83       127\n",
      "          28       0.97      0.88      0.92        32\n",
      "          29       0.83      0.63      0.72        46\n",
      "          30       0.70      0.73      0.72       298\n",
      "          31       0.72      0.67      0.70        92\n",
      "          32       0.81      0.79      0.80       140\n",
      "          33       0.89      0.83      0.86        70\n",
      "          34       0.73      0.84      0.78        90\n",
      "          35       0.67      0.66      0.66       427\n",
      "          36       0.85      0.71      0.77        48\n",
      "          37       0.87      0.73      0.79        37\n",
      "          38       0.70      0.68      0.69       167\n",
      "          39       0.89      0.71      0.79        72\n",
      "\n",
      "    accuracy                           0.76      4524\n",
      "   macro avg       0.81      0.76      0.78      4524\n",
      "weighted avg       0.76      0.76      0.76      4524\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test_nonempty, clf_d2v_lr.predict(X_test_d2v)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 535,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  6,   0,   0, ...,   0,   0,   0],\n",
       "       [  0, 189,   0, ...,   2,   0,   0],\n",
       "       [  0,   0, 146, ...,   0,   2,   0],\n",
       "       ...,\n",
       "       [  0,   3,   0, ...,  27,   2,   0],\n",
       "       [  0,   1,   2, ...,   0, 113,   0],\n",
       "       [  0,   1,   0, ...,   0,   0,  51]], dtype=int64)"
      ]
     },
     "execution_count": 535,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print confusion matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix(y_test_nonempty, clf_d2v_lr.predict(X_test_d2v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 472,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original job post:\n",
      "Cascade Bank CSJC is looking for a motivated and\n",
      "proactive candidate for the position of Reporting Officer in Lending\n",
      "Department.\n",
      "\n",
      "Ground truth label: 23\n",
      "\n",
      "Predicted label: 23\n"
     ]
    }
   ],
   "source": [
    "# sample test data prediction\n",
    "print(f'Original job post:\\n{X[4737]}\\n')\n",
    "print(f'Ground truth label: {y[4737]}\\n')\n",
    "print(f'Predicted label: {clf_d2v_lr.predict(X_test_d2v)[0]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'clf_d2v_lr' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_37928\\584061500.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mclf_d2v_lr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0md2v\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minfer_vector\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw2v_preprocess\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'I am good with excel and microsoft. I also like writing.'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'clf_d2v_lr' is not defined"
     ]
    }
   ],
   "source": [
    "clf_d2v_lr.predict([d2v.infer_vector(w2v_preprocess('I am good with excel and microsoft. I also like writing.'))])\n",
    "df_job_postings.loc[df_job_postings['cluster']==35]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 536,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['models/clf_d2v_lr_career.joblib']"
      ]
     },
     "execution_count": 536,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# save model\n",
    "from joblib import dump\n",
    "dump(clf_d2v_lr, 'models/clf_d2v_lr_career.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 473,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import scipy\n",
    "\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 474,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1 GPU(s) available.\n",
      "Device name:  NVIDIA GeForce RTX 3080 Ti\n"
     ]
    }
   ],
   "source": [
    "# set up gpu for training\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    print(f'There are {torch.cuda.device_count()} GPU(s) available.')\n",
    "    print('Device name: ', torch.cuda.get_device_name(0))\n",
    "\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    print('No GPU available, using the CPU instead.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 489,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert X d2v vectors to tensor\n",
    "X_train_d2v_tensor = torch.tensor(X_train_d2v).float().to(device)\n",
    "X_test_d2v_tensor = torch.tensor(X_test_d2v).float().to(device)\n",
    "\n",
    "# convert y vectors to tensor\n",
    "y_train_nonempty_tensor = torch.tensor(y_train_nonempty.values).to(device)\n",
    "y_test_nonempty_tensor = torch.tensor(y_test_nonempty.values).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 490,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Linear(in_features=300, out_features=64, bias=True)\n",
       "  (1): ReLU()\n",
       "  (2): Linear(in_features=64, out_features=40, bias=True)\n",
       "  (3): LogSoftmax(dim=1)\n",
       ")"
      ]
     },
     "execution_count": 490,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define cnn model\n",
    "clf_d2v_cnn = nn.Sequential(\n",
    "    nn.Linear(X_train_d2v_tensor.shape[1], 64),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(64, y_nonempty.nunique()),\n",
    "    nn.LogSoftmax(dim=1)\n",
    ")\n",
    "clf_d2v_cnn.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 491,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define loss\n",
    "criterion = nn.NLLLoss()\n",
    "logps = clf_d2v_cnn(X_train_d2v_tensor)\n",
    "loss = criterion(logps, y_train_nonempty_tensor)\n",
    "\n",
    "loss.backward()\n",
    "\n",
    "# define optimizer\n",
    "optimizer = torch.optim.Adam(clf_d2v_cnn.parameters(), lr=2e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 492,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define parameters for model training\n",
    "EPOCHS = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 493,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n",
      "Epoch: 1\n",
      "Epoch: 2\n",
      "Epoch: 3\n",
      "Epoch: 4\n",
      "Epoch: 5\n",
      "Epoch: 6\n",
      "Epoch: 7\n",
      "Epoch: 8\n",
      "Epoch: 9\n",
      "Epoch: 10\n",
      "Epoch: 11\n",
      "Epoch: 12\n",
      "Epoch: 13\n",
      "Epoch: 14\n",
      "Epoch: 15\n",
      "Epoch: 16\n",
      "Epoch: 17\n",
      "Epoch: 18\n",
      "Epoch: 19\n",
      "Epoch: 20\n",
      "Epoch: 21\n",
      "Epoch: 22\n",
      "Epoch: 23\n",
      "Epoch: 24\n",
      "Epoch: 25\n",
      "Epoch: 26\n",
      "Epoch: 27\n",
      "Epoch: 28\n",
      "Epoch: 29\n",
      "Epoch: 30\n",
      "Epoch: 31\n",
      "Epoch: 32\n",
      "Epoch: 33\n",
      "Epoch: 34\n",
      "Epoch: 35\n",
      "Epoch: 36\n",
      "Epoch: 37\n",
      "Epoch: 38\n",
      "Epoch: 39\n",
      "Epoch: 40\n",
      "Epoch: 41\n",
      "Epoch: 42\n",
      "Epoch: 43\n",
      "Epoch: 44\n",
      "Epoch: 45\n",
      "Epoch: 46\n",
      "Epoch: 47\n",
      "Epoch: 48\n",
      "Epoch: 49\n"
     ]
    }
   ],
   "source": [
    "# train the model\n",
    "for epoch in range(EPOCHS):\n",
    "    # zero the paramater gradients\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # forward + backward + optimize\n",
    "    outputs = clf_d2v_cnn.forward(X_train_d2v_tensor)\n",
    "    loss = criterion(outputs, y_train_nonempty_tensor)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # print statistics\n",
    "    print(f'Epoch: {epoch}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 494,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate our model\n",
    "with torch.no_grad():\n",
    "    clf_d2v_cnn.eval()\n",
    "    log_ps = clf_d2v_cnn(X_test_d2v_tensor)\n",
    "    test_loss = criterion(log_ps, y_test_nonempty_tensor)\n",
    "\n",
    "    ps = torch.exp(log_ps)\n",
    "    top_p, top_class = ps.topk(1, dim=1)\n",
    "    equals = top_class == y_test_nonempty_tensor.view(*top_class.shape)\n",
    "    test_accuracy = torch.mean(equals.float())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 495,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.4996, device='cuda:0')"
      ]
     },
     "execution_count": 495,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## After Classification Model - Cluster to Job Title Mapping + Job Description Keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the saved dataset\n",
    "df_job_postings = pd.read_csv('data/job_postings_labeled.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['English Language Instructor', 'English Language Specialist',\n",
       "       'C#.NET Senior Developer/ Architect', 'Team Leader',\n",
       "       'Announcements Moderator'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sample - find the mapping between the cluster no. and the top 5 most frequent job titles associated with that cluster\n",
    "df_job_postings.loc[df_job_postings['cluster'] == 35]['Title'].value_counts().index[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.13 ('DataScience')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "32d60d92059b3e36b6bd9986edfc808c4ae526b74500509951e82855bb1b814d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
